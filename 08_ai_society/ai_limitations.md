# AIの限界と脆弱性

## 要点
- AIは万能ではなく、多くの限界と脆弱性を持つ。データ依存性、説明困難性、敵対的攻撃への脆弱性、バイアス、予測不可能な失敗等が主な課題。
- 敵対的攻撃（Adversarial Attack）は、ニューラルネットワークを意図的に誤作動させる手法で、セキュリティ上の重大なリスク。
- AI導入時はこれらの限界を理解し、人間の監督・検証と組み合わせることが重要。

## 定義
AIの限界とは、現代のAI技術が抱える本質的な制約や弱点のこと。データ品質への依存、解釈可能性の欠如、敵対的攻撃への脆弱性、バイアスの増幅、ドメイン外データへの対応困難等が含まれる。

---

## 敵対的攻撃（Adversarial Attack）★試験頻出

### 要点
ニューラルネットワークモデルを意図的に誤作動させる手法技術。人間には気づかないほど微小な摂動（ノイズ）を入力に加えることで、AIに誤った判断をさせる。画像認識、音声認識、自動運転等で深刻なセキュリティリスク。

### 定義
**敵対的攻撃（Adversarial Attack）**とは、ニューラルネットワークに対して意図的に誤った出力を生成させるために、入力データに人間には知覚できない程度の微小な摂動（perturbation）を加える攻撃手法。生成された入力を**敵対的サンプル（Adversarial Examples）**という。

### 重要キーワード
- **敵対的攻撃（Adversarial Attack）**: NNを意図的に誤作動させる攻撃手法
- **敵対的サンプル（Adversarial Examples）**: 攻撃用に作成された誤分類を誘発する入力データ
- **摂動（Perturbation）**: 入力に加える微小なノイズ
- **FGSM（Fast Gradient Sign Method）**: 勾配の符号を利用した高速な攻撃手法
- **転移可能性（Transferability）**: あるモデルで生成した敵対的サンプルが他モデルでも有効
- **物理的攻撃（Physical Attack）**: 実世界の物体（標識等）への攻撃
- **ホワイトボックス攻撃**: モデルの内部構造・パラメータを知っている場合の攻撃
- **ブラックボックス攻撃**: モデルの出力のみにアクセスできる場合の攻撃
- **敵対的訓練（Adversarial Training）**: 敵対的サンプルで学習して頑健性を向上

### 詳細

#### 背景
深層学習モデルは高精度を達成したが、**微小な摂動に対して脆弱**であることが判明。この脆弱性は：
- **セキュリティリスク**: 自動運転、顔認証、医療診断等で悪用可能
- **理論的興味**: なぜNNは人間と異なる認識をするのか
- **実用上の課題**: 信頼性の高いAIシステム構築が困難

#### 敵対的サンプルの仕組み

**基本原理**:
```
元画像 x（正しく分類）
   ↓ + 微小摂動 δ
敵対的サンプル x' = x + δ
   ↓
誤分類（意図した誤答）

条件：
- ||δ|| が非常に小さい（人間には認識不可能）
- モデルは x と x' を全く異なるクラスに分類
```

**例**:
```
【パンダの画像】
元画像: 「パンダ」と正しく分類（99.3%）
   ↓ + 微小ノイズ（目には見えない）
攻撃後: 「テナガザル」と誤分類（99.3%）

人間の目: 両方ともパンダに見える
AIの判断: 完全に異なる動物
```

#### 攻撃の分類

**1. ホワイトボックス攻撃（White-box Attack）**

**定義**: モデルの内部構造、パラメータ、勾配情報を全て知っている場合の攻撃。

**代表的手法**:

**FGSM（Fast Gradient Sign Method）**:
- 最も基本的な攻撃手法
- 損失関数の勾配の符号を利用

$$x' = x + \epsilon \cdot \text{sign}(\nabla_x L(\theta, x, y))$$

- $x$: 元入力
- $\epsilon$: 摂動の大きさ（小さい値）
- $L$: 損失関数
- $y$: 正解ラベル

**PGD（Projected Gradient Descent）**:
- FGSMの反復版、より強力
- 複数ステップで攻撃を最適化

**C&W攻撃（Carlini & Wagner）**:
- 最も強力な攻撃の1つ
- 防御機構を突破できる

**2. ブラックボックス攻撃（Black-box Attack）**

**定義**: モデルの内部は不明、出力（クエリ結果）のみにアクセス可能。

**手法**:
- **転移ベース攻撃**: 他の代理モデルで生成した敵対的サンプルを流用
- **クエリベース攻撃**: 多数のクエリで出力を観測し、勾配を推定

**転移可能性（Transferability）**:
- あるモデルAで生成した敵対的サンプルが、別のモデルBでも誤分類を引き起こす現象
- ブラックボックス攻撃の基礎

**3. 物理的攻撃（Physical Attack）**

**定義**: 実世界の物体を改変して攻撃。

**例**:
- **道路標識への攻撃**: 「止まれ」標識にステッカーを貼り、「速度制限」と誤認識させる
- **眼鏡フレーム攻撃**: 特殊な模様の眼鏡で顔認証を突破
- **パッチ攻撃**: 衣服に特定パターンを貼り、人物検出を回避

**課題**:
- カメラ角度、照明、距離の変化に対して頑健な摂動が必要
- デジタル攻撃より困難だが、より危険（自動運転車等）

#### 攻撃の目的

**1. Targeted Attack（標的型攻撃）**
- 特定のクラスに誤分類させる
- 例: 「パンダ」を「テナガザル」と認識させる

**2. Untargeted Attack（非標的型攻撃）**
- 正しいクラス以外の何かに誤分類させる（何でも良い）
- 例: 「パンダ」を「パンダ以外」と認識させる

#### 図解（敵対的攻撃の流れ）

```
【ホワイトボックス攻撃（FGSM）】

1. 元画像 x を入力
   ↓
2. モデル: 「パンダ」と正しく分類
   ↓
3. 勾配計算: ∇L (損失を増やす方向)
   ↓
4. 摂動生成: δ = ε・sign(∇L)
   ↓
5. 敵対的サンプル作成: x' = x + δ
   ↓
6. x' を入力
   ↓
7. モデル: 「テナガザル」と誤分類（成功）


【物理的攻撃の例】

標識「止まれ」
   ↓ + ステッカー貼付
改変標識（人間には「止まれ」に見える）
   ↓
カメラ撮影 → CNN
   ↓
誤認識「速度制限 45km/h」
   ↓
自動運転車が停止しない（危険）
```

#### 防御手法

**1. 敵対的訓練（Adversarial Training）**

**概念**: 訓練時に敵対的サンプルも学習データに含める。

**プロセス**:
```
1. 通常データで訓練
2. 敵対的サンプルを生成
3. 通常データ + 敵対的サンプルで再訓練
4. 繰り返し
```

**効果**:
- 既知の攻撃に対する頑健性向上
- ただし、未知の攻撃には脆弱

**2. 入力変換（Input Transformation）**

**手法**:
- **ノイズ除去**: 入力画像にガウスノイズ付加や平滑化
- **JPEG圧縮**: 高周波の摂動を除去
- **ランダムリサイズ**: スケール変換でパターンを崩す

**効果**: 微小な摂動を破壊

**3. 検出ベース防御（Detection-based Defense）**

**手法**:
- **統計的検出**: 敵対的サンプルの分布が通常データと異なることを利用
- **別モデルでの検証**: 複数モデルの予測を比較

**4. 認証防御（Certified Defense）**

**手法**: 数学的に証明可能な頑健性を提供。

**例**: ランダム平滑化（Randomized Smoothing）

**課題**: 精度が大幅低下する場合がある

#### 実例

**例1: 自動運転への攻撃**
- **攻撃**: 道路標識に小さなステッカーを貼る
- **結果**: 「止まれ」を「速度制限」と誤認識
- **リスク**: 交通事故、人命損失

**例2: 顔認証システムへの攻撃**
- **攻撃**: 特殊な模様の眼鏡を装着
- **結果**: 他人として認証される
- **リスク**: セキュリティ突破、なりすまし

**例3: スパムフィルタへの攻撃**
- **攻撃**: メール本文に微小な変更
- **結果**: スパムメールが正常メールと判定
- **リスク**: フィッシング詐欺の拡大

**例4: 医療診断AIへの攻撃**
- **攻撃**: 医療画像に微小ノイズ追加
- **結果**: 悪性腫瘍を良性と誤診断
- **リスク**: 誤診による患者への被害

**例5: 音声認識への攻撃**
- **攻撃**: 人間には聞こえない音声コマンドを隠す
- **結果**: スマートスピーカーが不正コマンド実行
- **リスク**: プライバシー侵害、不正操作

#### 敵対的攻撃の特性

**転移可能性（Transferability）**:
- モデルAで生成した敵対的サンプルが、モデルBでも有効
- 異なるアーキテクチャ間でも転移
- **原因**: NNが共通の脆弱な特徴を学習

**普遍性（Universality）**:
- 1つの摂動パターンが、異なる多くの入力に対して有効
- **Universal Adversarial Perturbations**: 1つのノイズで多数の画像を誤分類

**知覚不可能性（Imperceptibility）**:
- 人間には元画像と区別不可能
- $L_\infty$ ノルムで $\epsilon < 8/255$ 程度

#### なぜ攻撃が成功するのか

**1. 高次元空間の性質**:
- 画像は高次元（例: 224×224×3 = 150,528次元）
- 各次元に微小な変化を加えると、合計で大きな影響

**2. 線形性の影響**:
- NNは局所的に線形近似可能
- 線形モデルは敵対的サンプルに脆弱

**3. 過剰適合（Overfitting）**:
- 訓練データの微細なパターンまで学習
- 本質的でない特徴に依存

**4. 滑らかでない決定境界**:
- クラス間の境界が複雑で、微小な移動で境界越え

### 試験での問われ方

**典型問題**：「ニューラルネットワークモデルを意図的に誤作動させる手法技術を（　）という。」

**解答**: **敵対的攻撃**（Adversarial Attack）

**キーワード認識**:
- 「**意図的に誤作動**」→ 敵対的攻撃
- 「**微小な摂動**」「**人間には気づかないノイズ**」→ 敵対的サンプル
- 「**FGSM**」→ 代表的な攻撃手法
- 「**敵対的訓練**」→ 防御手法

**選択肢問題例**:

**問**: 以下のうち、敵対的攻撃の説明として最も適切なものを選べ。

- A. ニューラルネットワークの学習を高速化する手法
- B. 人間には知覚できない微小な摂動を加えてモデルを誤作動させる手法
- C. データ拡張によりモデルの精度を向上させる手法
- D. 過学習を防ぐ正則化手法

**正解**: B

**判別ポイント**:
- ✅ **「意図的に誤作動」「微小な摂動」「人間には気づかない」** → 敵対的攻撃
- ❌ 「学習高速化」「精度向上」「過学習防止」 → 別の技術

**ひっかけポイント**:
- ❌ 「GANの敵対的学習」と混同 → GANは生成技術、敵対的攻撃はセキュリティ攻撃
- ❌ 「データ拡張」と混同 → データ拡張は良性の技術
- ✅ 「敵対的訓練」は防御手法（攻撃ではない）

**比較表（試験頻出）**:

| 項目 | **敵対的攻撃** | **GAN** | **データ拡張** |
|------|----------------|---------|----------------|
| **目的** | モデルを誤作動させる | 新データ生成 | 訓練データ拡充 |
| **悪意** | あり（攻撃） | なし（技術） | なし（改善） |
| **摂動** | 微小（知覚不可能） | - | 大きい（回転・拡大等） |
| **対象** | 訓練済みモデル | 未訓練ネットワーク | 訓練データ |
| **応用** | セキュリティ分析 | 画像生成、データ合成 | モデル精度向上 |
| **代表手法** | FGSM、PGD | StyleGAN、DCGAN | 回転、クロップ |

### 実務での注意点

**AIシステム開発時**:
1. **敵対的攻撃のリスク評価**: 自動運転、顔認証等の安全重要システムでは必須
2. **敵対的訓練の導入**: 頑健性を高める
3. **複数モデルの併用**: アンサンブルで単一モデルの脆弱性を緩和
4. **入力検証**: 異常な入力を検出・拒否
5. **人間による監督**: 重要な判断には人間の確認を組み込む

**セキュリティ観点**:
- **物理的攻撃**: 道路標識、顔認証等は物理攻撃に注意
- **APIセキュリティ**: クエリ数制限でブラックボックス攻撃を防ぐ
- **モデルの秘匿**: ホワイトボックス攻撃を防ぐため内部構造を公開しない

**倫理的考慮**:
- 攻撃研究は防御目的に限定
- 悪用される可能性を考慮した公開方法

---

## AIの他の限界

### データ依存性

**問題**: AIは訓練データの品質に完全依存。データが偏っていれば、AIも偏る。

**例**:
- 特定人種の顔データが少ない → 顔認証の精度低下
- 過去の採用データに性別偏見 → 採用AIがバイアス継承

### 説明困難性（ブラックボックス問題）

**問題**: 深層学習の判断根拠が不明。

**リスク**:
- 医療診断で「なぜ癌と判断したか」説明不可能
- 融資拒否の理由を顧客に説明できない
- 法的責任の所在が不明確

### ドメイン外データへの対応困難

**問題**: 訓練時に見なかったデータに対して予測不能。

**例**:
- 画像認識AIが未知の物体を誤分類
- 自動運転が想定外の状況で誤判断

### 常識・因果推論の欠如

**問題**: 人間の常識や因果関係の理解が不足。

**例**:
- 「傘をさす → 雨が降る」と誤った因果関係学習
- 文脈を無視した機械的な判断

### バイアスの増幅

**問題**: データ内のバイアスをAIが学習・増幅。

**例**:
- 性別、人種による差別的判断
- 富裕層優遇の融資判断

### 計算資源・環境コスト

**問題**: 大規模モデルの訓練に膨大なエネルギー。

**例**: GPT-3訓練で約552トンのCO₂排出（車5台の生涯排出量相当）

---

## 重要キーワード

- **敵対的攻撃（Adversarial Attack）**: NNを意図的に誤作動させる攻撃
- **敵対的サンプル（Adversarial Examples）**: 誤分類を誘発する入力
- **摂動（Perturbation）**: 入力に加える微小ノイズ
- **FGSM（Fast Gradient Sign Method）**: 基本的な攻撃手法
- **転移可能性（Transferability）**: 攻撃が他モデルでも有効
- **ホワイトボックス攻撃**: モデル内部を知る攻撃
- **ブラックボックス攻撃**: 出力のみで攻撃
- **物理的攻撃（Physical Attack）**: 実世界の物体への攻撃
- **敵対的訓練（Adversarial Training）**: 防御手法
- **データ依存性**: 訓練データの品質に完全依存
- **説明困難性**: 判断根拠の不透明性
- **バイアス増幅**: データの偏りを学習

## 試験での問われ方（総合）

**比較されやすい概念**:
- **敵対的攻撃 vs GAN**: 攻撃（悪意） vs 生成技術（良性）
- **敵対的攻撃 vs データ拡張**: 微小摂動（知覚不可能） vs 大きな変換（明確）
- **敵対的攻撃 vs 敵対的訓練**: 攻撃手法 vs 防御手法

**引っ掛けポイント**:
- ❌ 「GANの敵対的」と「敵対的攻撃」は別概念
- ❌ 「敵対的訓練」は攻撃ではなく防御
- ✅ 敵対的サンプルは「人間には気づかない」が重要

## 補足

**今後の研究方向**:
- **Certified Robustness**: 数学的に証明可能な頑健性
- **Adversarial Robustness Benchmark**: 標準評価基準の確立
- **物理的攻撃への対応**: 実世界での防御技術

**実務での重要性**:
- 自動運転、医療診断、セキュリティシステムでは敵対的攻撃対策が必須
- AIの信頼性・安全性確保のための基礎知識

**関連トピック**:
- [バイアスと公平性](../09_law_ethics/bias_and_fairness.md) - AIのバイアス問題
- [AI倫理原則](../09_law_ethics/ai_ethics_principles.md) - 安全性・頑健性
- [CNN](../06_deep_learning/cnn.md) - 画像認識モデル（攻撃対象）
- [GAN](../06_deep_learning/generative_models.md) - 敵対的学習（別概念）
