# XAI（説明可能なAI：Explainable AI）

## 要点
- **XAI**は、AIモデルの予測根拠や判断理由を人間が理解できる形で説明する技術。ブラックボックス問題の解決が目的。
- **Grad-CAM**：畳み込み層の勾配から注目領域をヒートマップで可視化。画像分類の根拠を示す。
- **SHAP**：協力ゲーム理論（Shapley値）を応用し、各特徴量の貢献度を公平に算出。
- **LIME**：**局所的に線形モデルで近似**し、個別予測を解釈可能にする。複雑な非線形ではない点が重要。

---

## 定義

**XAI（Explainable AI：説明可能なAI）**は、深層学習等のブラックボックス化したモデルの予測根拠を、人間が理解・信頼できる形で説明する技術群。医療診断、金融審査、自動運転等の高リスク領域で重要性が高い。

---

## 重要キーワード

- **XAI（Explainable AI）**：AIの判断根拠を説明する技術全般
- **ブラックボックス問題**：深層学習の内部処理が不透明で理解困難な問題
- **Grad-CAM（Gradient-weighted Class Activation Mapping）**：CNNの勾配情報からヒートマップを生成し、画像のどこに注目したか可視化
- **Guided Grad-CAM**：Grad-CAMの改良版。入力値の勾配情報を追加し、より細かい可視化
- **SHAP（SHapley Additive exPlanations）**：ゲーム理論のShapley値を応用し、各特徴量の貢献度を算出
- **LIME（Local Interpretable Model-agnostic Explanations）**：局所的に線形モデルで近似し、個別予測を説明
- **Attention機構**：Transformerで重要な入力部分に重みを付ける仕組み。自然な説明性を持つ
- **特徴量重要度（Feature Importance）**：各特徴量がモデル予測に与える影響度

---

## 詳細

### XAIの必要性

#### ブラックボックス問題
深層学習は高精度だが、内部の判断過程が不透明：
```
入力（画像）
  ↓
深層ニューラルネットワーク
 [数百万のパラメータ]
 [複雑な非線形変換]
  ↓
出力（猫：99%）

→ なぜ「猫」と判断したか不明（ブラックボックス）
```

#### XAIが必要な理由
1. **信頼性の確保**：医療診断で誤診のリスク低減
2. **法的要求**：GDPR（EU一般データ保護規則）で説明責任が義務化
3. **デバッグ**：モデルの誤動作を発見・修正
4. **バイアス検出**：差別的な判断基準を特定
5. **ユーザー受容性**：説明がないと利用者が不安

---

### 主なXAI手法

#### 1. Grad-CAM（Gradient-weighted Class Activation Mapping）

**定義**：
畳み込みニューラルネットワーク（CNN）の**勾配情報**を利用し、画像のどの領域がクラス分類に重要かを**ヒートマップ**で可視化する手法。

**プロセス**：
```
【入力】画像（例：犬の写真）
  ↓
CNN（畳み込み層）で特徴抽出
  ↓
【ターゲットクラス】「犬」の予測スコアに対する勾配を計算
  ↓
各畳み込みフィルタの勾配を重み付け
  ↓
重み付けした特徴マップを合成
  ↓
【出力】ヒートマップ（赤＝重要、青＝非重要）
```

**特徴**：
- ✅ **可視化が直感的**：ヒートマップで一目瞭然
- ✅ **モデル構造に依存**：CNNに特化
- ✅ **クラス別の注目領域**：各クラスごとに可視化可能
- ❌ **畳み込み層が必要**：全結合層のみのモデルには不適用

**具体例**：
```
入力：犬の画像
  ↓
Grad-CAM実行
  ↓
ヒートマップ：犬の顔周辺が赤く強調
  ↓
解釈：モデルは「犬の顔」を見て「犬」と判断した
```

**応用**：
- 医療画像診断（病変部位の可視化）
- 物体検出の根拠確認
- モデルのデバッグ（誤った領域に注目していないか確認）

---

#### 2. Guided Grad-CAM

**定義**：
Grad-CAMを改良し、**入力値の勾配情報**も組み合わせることで、より細かい注目領域を可視化する手法。

**Grad-CAMとの違い**：
| 項目 | Grad-CAM | Guided Grad-CAM |
|------|----------|-----------------|
| 使用する情報 | 畳み込み層の勾配 | 畳み込み層 + **入力層の勾配** |
| 解像度 | 粗い（畳み込み層の解像度） | **細かい（入力解像度）** |
| 可視化 | クラス全体の注目領域 | **ピクセル単位の詳細** |

**特徴**：
- ✅ 高解像度の可視化
- ✅ Grad-CAMより詳細な根拠提示
- ❌ 計算コストがやや高い

---

#### 3. SHAP（SHapley Additive exPlanations）

**定義**：
**協力ゲーム理論のShapley値**を機械学習に応用し、各特徴量がモデル予測値に与える**貢献度**を公平に算出する手法。

**Shapley値とは**：
協力ゲーム理論の概念で、プレイヤー（特徴量）が全体の利益（予測値）にどれだけ貢献したかを公平に分配する指標。

**計算の考え方**：
```
【例：住宅価格予測】
特徴量：面積、築年数、駅距離

Shapley値の計算（簡略版）：
1. 特徴量の全ての組み合わせで予測値を計算
2. 各特徴量を追加した際の予測値の変化を平均化
3. 各特徴量の平均的な貢献度を算出

結果：
- 面積：+500万円（貢献度大）
- 築年数：-200万円（マイナス貢献）
- 駅距離：+100万円（貢献度小）
```

**特徴**：
- ✅ **理論的に公平**：ゲーム理論の厳密な定義に基づく
- ✅ **モデル非依存**：どのモデルにも適用可能
- ✅ **加法性**：各特徴の貢献度の和が予測値に一致
- ❌ **計算コスト高**：全組み合わせの計算が必要（近似手法で軽減）

**応用**：
- 金融審査（融資判断の根拠説明）
- 医療診断（診断に影響した検査値の特定）
- 不動産評価（価格要因の分解）

**SHAP値の可視化**：
- **Force Plot**：各特徴の貢献を矢印で表示
- **Summary Plot**：全データの特徴量重要度を一覧
- **Dependence Plot**：特徴量と予測値の関係

---

#### 4. LIME（Local Interpretable Model-agnostic Explanations）

**定義（★試験重要）**：
複雑なブラックボックスモデルを、**局所的に線形モデルで近似**することで、個別の予測を解釈可能にする手法。

**重要ポイント**：
- ❌ **複雑な非線形回帰で近似するのではない**
- ✅ **シンプルな線形モデルで局所的に近似する**

**プロセス**：
```
【1. 予測したいサンプル】
元のサンプル x を選択（例：猫の画像）

【2. 周辺サンプル生成】
元のサンプルを少し変化させたサンプルを多数生成
（例：画像の一部をマスク、テキストの単語を削除）

【3. ブラックボックスモデルで予測】
生成した全サンプルを元のモデルで予測

【4. 線形モデルで近似】
元のサンプル x の近傍だけを、シンプルな線形モデルで近似
  y ≈ w₁x₁ + w₂x₂ + ... + b

【5. 解釈】
線形モデルの重み w₁, w₂, ... が各特徴の重要度を示す
```

**具体例（画像分類）**：
```
入力：猫の画像
  ↓
【周辺サンプル生成】
元画像の一部をグレーアウトした画像を多数生成
  ↓
【ブラックボックスモデルで予測】
各サンプルの「猫」らしさを予測
  ↓
【線形モデルで近似】
「顔の領域」の重みが大 → 顔が重要
「背景」の重みが小 → 背景は無関係
  ↓
【解釈】
この画像では「猫の顔」を見て判断している
```

**特徴**：
- ✅ **モデル非依存**：どんなブラックボックスモデルにも適用可能
- ✅ **解釈が簡単**：線形モデルなので重みが直接的に理解できる
- ✅ **局所的な説明**：個別予測ごとに説明を生成
- ❌ **大域的な理解は困難**：モデル全体の挙動は説明できない
- ❌ **サンプリングに依存**：周辺サンプルの選び方で結果が変わる

**LIMEとSHAPの違い**：
| 項目 | LIME | SHAP |
|------|------|------|
| 理論基盤 | 局所線形近似 | ゲーム理論（Shapley値） |
| 公平性 | 保証なし | **理論的に公平** |
| 計算速度 | 速い | やや遅い |
| 安定性 | サンプリングに依存 | 安定 |
| 応用範囲 | 広い | 広い |

---

### XAI手法の比較

| 手法 | 対象 | 手法 | 出力 | 利点 | 欠点 |
|------|------|------|------|------|------|
| **Grad-CAM** | 画像（CNN） | 勾配情報 | ヒートマップ | 直感的、視覚的 | CNN専用 |
| **Guided Grad-CAM** | 画像（CNN） | 勾配+入力情報 | 高解像度マップ | より詳細 | 計算コスト |
| **SHAP** | 任意 | Shapley値 | 特徴量貢献度 | 理論的に公平 | 計算コスト高 |
| **LIME** | 任意 | **局所線形近似** | 線形重み | 解釈簡単、汎用 | 不安定 |
| **Attention** | 系列データ | 重み学習 | 注目度分布 | 組み込み型 | 間接的 |

---

## 試験での問われ方（★重要）

### 典型問題：最も不適切な選択肢を選べ

> 「XAIに関する説明として、最も不適切な選択肢を1つ選べ。」

✅ **適切な選択肢**：
1. **Guided Grad-CAMは、Grad-CAMを改良し、入力値の勾配情報を用いた手法である。** → ✅ 正しい
2. **Grad-CAMはモデル学習の勾配情報に応じて値を重み付けし、注目度を可視化する手法である。** → ✅ 正しい
3. **SHAPは協力ゲーム理論を機械学習に応用し、特徴量がモデル予測値に与える貢献度を算出する手法である。** → ✅ 正しい

❌ **不適切な選択肢**：
4. **LIMEは複雑な非線形回帰で近似することでも高い精度を誇る手法である。** → ❌ **最も不適切**

**誤りの理由**：
LIMEは**局所的にシンプルな線形モデルで近似する**手法であり、「複雑な非線形回帰で近似する」という説明は完全に誤り。LIMEの特徴は、複雑なモデルを局所的に**線形モデル**で説明することで解釈可能性を高める点にある。

---

### ひっかけポイント

| ひっかけ | 正しい理解 |
|----------|------------|
| ❌ LIMEは複雑な非線形回帰を使用 | ✅ LIMEは**局所的に線形モデル**で近似 |
| ❌ Grad-CAMは全結合NNに適用可能 | ✅ Grad-CAMは**CNNの畳み込み層**が必要 |
| ❌ SHAPは特定モデル専用 | ✅ SHAPは**モデル非依存**（どのモデルにも適用可能） |
| ❌ Guided Grad-CAMはGrad-CAMより粗い | ✅ Guided Grad-CAMは**より細かい**可視化 |
| ❌ XAIは精度向上が目的 | ✅ XAIは**説明可能性**が目的（精度向上ではない） |

---

### 類似問題パターン

**パターン1：手法の説明**
- Grad-CAM：勾配情報でヒートマップ
- SHAP：Shapley値で貢献度算出
- LIME：**局所的に線形近似**（非線形ではない）

**パターン2：適用範囲**
- Grad-CAM：CNN専用
- SHAP/LIME：モデル非依存

**パターン3：出力形式**
- Grad-CAM：ヒートマップ
- SHAP：特徴量貢献度
- LIME：線形重み

---

## 実例

### 実例1：医療画像診断でのGrad-CAM

```
【入力】肺のX線画像
  ↓
【CNNで予測】肺炎：95%
  ↓
【Grad-CAM実行】
ヒートマップで肺の右下部分が赤く強調
  ↓
【解釈】
モデルは肺の右下の白い影（浸潤影）を見て肺炎と判断
→ 医師が根拠を確認し、信頼性を向上
```

### 実例2：融資審査でのSHAP

```
【入力】融資申請者のデータ
- 年収：500万円
- 勤続年数：10年
- 借入額：2000万円

【モデル予測】承認確率：75%

【SHAP値】
- 年収：+15%（プラス貢献）
- 勤続年数：+20%（大きなプラス貢献）
- 借入額：-10%（マイナス貢献）

【解釈】
勤続年数の長さが最も承認に貢献
借入額の多さが若干のマイナス要因
→ 申請者に根拠を説明できる
```

### 実例3：テキスト分類でのLIME

```
【入力】商品レビュー「この商品は素晴らしい！絶対おすすめです。」
  ↓
【モデル予測】ポジティブ：98%
  ↓
【LIME実行】
周辺サンプル生成（単語を削除）：
- "この商品は［削除］！絶対おすすめです。" → 80%
- "この商品は素晴らしい！絶対［削除］です。" → 85%
  ↓
【線形近似】
重み：「素晴らしい」= +0.6、「おすすめ」= +0.4
  ↓
【解釈】
「素晴らしい」と「おすすめ」がポジティブ判定の主要因
```

---

## 補足：実務観点

### XAI導入の重要性
1. **規制対応**：GDPR、金融規制で説明責任が義務化
2. **信頼性向上**：医療・金融等でユーザーの信頼獲得
3. **デバッグ効率化**：誤動作の原因特定が容易に
4. **バイアス検出**：不公平な判断基準の発見

### 手法の選び方
- **画像分類**：Grad-CAM / Guided Grad-CAM
- **表形式データ**：SHAP / LIME
- **テキスト分類**：LIME / Attention可視化
- **高精度な貢献度**：SHAP（計算コスト許容なら）
- **高速な近似**：LIME（速度重視なら）

### 今後の展開
- **Integrated Gradients**：勾配の積分で安定した説明
- **Counterfactual Explanations**：「どう変えれば結果が変わるか」を提示
- **Concept-based説明**：高レベルの概念で説明

---

## 関連トピック
- [CNN](../06_deep_learning/cnn.md)：Grad-CAMの対象モデル
- [Transformer](../06_deep_learning/transformer.md)：Attention機構による説明性
- [AI倫理原則](ai_ethics_principles.md)：説明可能性の重要性
- [バイアスと公平性](../09_law_ethics/bias_and_fairness.md)：XAIによるバイアス検出
- [GDPR](../09_law_ethics/gdpr.md)：説明責任の法的要求

---

## 参考文献・出典
- Grad-CAM論文（Selvaraju et al., 2017）
- SHAP論文（Lundberg & Lee, 2017）
- LIME論文（Ribeiro et al., 2016）
- 日本ディープラーニング協会G検定シラバス
