# 探索アルゴリズム（幅優先探索・深さ優先探索）

## 要点（試験用）
- **幅優先探索（BFS）**: 同階層を全て探索後に次階層へ。キュー使用、最短経路保証、メモリ大。
- **深さ優先探索（DFS）**: 深さ方向に進む。スタック使用、最短保証なし、メモリ小。
- A*は**ヒューリスティック探索**で評価関数 $f(n) = g(n) + h(n)$ で効率化。

## 定義
探索アルゴリズムは、問題空間（状態空間）内で目標状態に到達する経路を見つける手法。代表的な手法に**幅優先探索（BFS）**と**深さ優先探索（DFS）**がある。

## 重要キーワード
- **幅優先探索（BFS: Breadth-First Search）**: 同階層のノードを全て探索してから次階層へ進む探索手法。
- **深さ優先探索（DFS: Depth-First Search）**: 一つの分岐を最深部まで探索してからバックトラックする手法。
- **キュー（Queue）**: FIFO（First In First Out）構造。BFSで使用。
- **スタック（Stack）**: LIFO（Last In First Out）構造。DFSで使用。
- **完全性（Completeness）**: 解が存在すれば必ず見つけられること。
- **最適性（Optimality）**: 最短経路・最小コストの解を保証すること。
- **時間計算量**: 探索にかかる時間（ノード展開数）。
- **空間計算量**: メモリ使用量（保持するノード数）。
- **A*探索**: ヒューリスティック関数 $h(n)$ を用いた効率的探索。
- **ダイクストラ法**: 最短経路探索（重み付きグラフ）。
- **反復深化探索（IDS）**: DFSの深さ制限を段階的に増やす手法。

## 詳細（教科書風）

### 背景と直観
探索アルゴリズムは、迷路・パズル・経路探索・ゲーム木探索など幅広い問題に適用される基本技術です。探索の順序とデータ構造の選択が、性能と解の質を決定します。

**基本的な考え方**：
- 初期状態から可能な行動を展開
- 未探索ノードを適切な順序で選択
- 目標状態に到達するまで繰り返し

### 幅優先探索（BFS: Breadth-First Search）

#### アルゴリズム
```
1. 初期ノードをキューに追加
2. キューからノードを取り出す（FIFO）
3. 目標判定：ゴールなら終了
4. 未訪問の隣接ノードを全てキューに追加
5. 2へ戻る
```

#### 図解：探索順序
```
        1
      / | \
     2  3  4
    /|  |  |\
   5 6  7  8 9

探索順序: 1 → 2, 3, 4 → 5, 6, 7, 8, 9
（階層ごとに左から右へ）
```

#### 特徴
**利点**：
- **最短経路保証**（エッジコスト一定時）
- **完全性**：解が存在すれば必ず発見
- 浅い解を優先的に発見

**欠点**：
- **メモリ消費大**：全階層のノードを保持
- 深い解には不向き
- 分岐数 $b$、深さ $d$ のとき空間計算量 $O(b^d)$

#### 適用例
- 迷路の最短経路
- SNSの友達関係（○○さんまで何ステップ？）
- Webクローラー（リンク探索）
- パズルゲーム（最少手数）

### 深さ優先探索（DFS: Depth-First Search）

#### アルゴリズム
```
1. 初期ノードをスタックに追加
2. スタックからノードを取り出す（LIFO）
3. 目標判定：ゴールなら終了
4. 未訪問の隣接ノードをスタックに追加
5. 2へ戻る
```

#### 図解：探索順序
```
        1
      / | \
     2  3  4
    /|  |  |\
   5 6  7  8 9

探索順序: 1 → 2 → 5 → 6 → 3 → 7 → 4 → 8 → 9
（深さ優先で一本道を進む）
```

#### 特徴
**利点**：
- **メモリ消費小**：現在の経路のみ保持
- 深い解の発見が速い場合あり
- 空間計算量 $O(bd)$（$b$:分岐、$d$:深さ）

**欠点**：
- **最短保証なし**：最初に見つけた解が最短とは限らない
- 無限ループの可能性（循環検出が必要）
- 非常に深い経路で時間がかかる

#### 適用例
- トポロジカルソート
- 迷路探索（解の存在確認）
- バックトラック問題（ナップサック、N-Queen）

### BFS vs DFS 比較表

| 項目 | 幅優先探索（BFS） | 深さ優先探索（DFS） |
|------|-------------------|---------------------|
| **探索順序** | 階層ごと（横方向） | 深さ優先（縦方向） |
| **データ構造** | **キュー（FIFO）** | **スタック（LIFO）** |
| **最短経路** | **保証あり** | 保証なし |
| **完全性** | ○（有限空間で保証） | △（無限深さで×） |
| **時間計算量** | $O(b^d)$ | $O(b^m)$（$m$:最大深さ） |
| **空間計算量** | $O(b^d)$（大） | $O(bd)$（小） |
| **適用場面** | 最短経路、浅い解 | メモリ制約、深い解 |

### ヒューリスティック探索（A*）

#### 概要
A*探索は、**コスト関数 $g(n)$（実際のコスト）**と**ヒューリスティック関数 $h(n)$（推定コスト）**を組み合わせて効率化：

$$f(n) = g(n) + h(n)$$

- $g(n)$: 開始ノードから $n$ までの実コスト
- $h(n)$: $n$ からゴールまでの推定コスト（許容的ヒューリスティック）
- $f(n)$: 総推定コスト

#### 特徴
- **最適性**：$h(n)$ が許容的（真のコスト以下）なら最短保証
- **効率性**：BFSより探索ノード数が少ない
- **適用例**：カーナビ、ゲームAI、ロボット経路計画

#### 代表的ヒューリスティック
- **マンハッタン距離**：グリッド上の移動（$|x_1 - x_2| + |y_1 - y_2|$）
- **ユークリッド距離**：直線距離（$\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$）

### その他の探索手法

#### 反復深化探索（IDS: Iterative Deepening Search）
- DFSの深さ制限を段階的に増やす（深さ1→2→3...）
- BFSのメモリ効率とDFSの空間効率を両立
- 空間計算量 $O(d)$、時間計算量 $O(b^d)$

#### ダイクストラ法
- 重み付きグラフの最短経路探索
- A*の $h(n) = 0$ の特殊ケース
- 優先度付きキューで最小コストノードを選択

#### ベルマン・フォード法
- 負の重みを許容する最短経路探索
- 負閉路の検出可能

### 実装例（擬似コード）

#### BFS
```python
def bfs(start, goal):
    queue = [start]
    visited = {start}
    
    while queue:
        node = queue.pop(0)  # FIFO
        if node == goal:
            return "Found"
        
        for neighbor in node.neighbors:
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append(neighbor)
    
    return "Not Found"
```

#### DFS
```python
def dfs(start, goal, visited=None):
    if visited is None:
        visited = set()
    
    visited.add(start)
    if start == goal:
        return "Found"
    
    for neighbor in start.neighbors:
        if neighbor not in visited:
            result = dfs(neighbor, goal, visited)
            if result == "Found":
                return "Found"
    
    return "Not Found"
```

## 探索・推論が適した問題と不適切な問題

### 探索・推論が適した問題（試験頻出）

| 問題タイプ | 理由 | 代表例 |
|----------|------|--------|
| **経路探索** | 状態＝位置、明確なゴール | 迷路、ナビゲーション、配送最適化 |
| **パズル** | 状態遷移が明確、制約条件あり | 8パズル、15パズル、ルービックキューブ |
| **ゲーム** | ルールが明確、勝敗判定可能 | チェス、将棋、囲碁（探索+評価） |
| **プランニング** | 行動の前提条件と効果が定義可能 | STRIPS、ロボット動作計画 |
| **制約充足問題（CSP）** | 制約条件が明示可能 | スケジューリング、資源配分 |
| **論理推論** | 命題・述語論理で記述可能 | エキスパートシステム、定理証明 |

**共通する特徴（適用条件）**：
- ✅ 状態空間が**離散的**に定義できる
- ✅ 遷移ルールが**明確**
- ✅ ゴール状態が**判定可能**
- ✅ 問題が**記号的**に表現できる

### 探索・推論が不適切な問題（試験で最重要）

| 問題タイプ | 理由 | 適切な手法 |
|----------|------|-----------|
| **画像認識** | ピクセル値から特徴抽出が必要、ルール化困難 | **CNN（深層学習）** |
| **音声認識** | 連続的な音響信号、話者・環境に依存 | **RNN/Transformer** |
| **自然言語処理** | 曖昧性、文脈依存、意味理解が必要 | **Transformer/LLM** |
| **手書き文字認識** | パターンの多様性、ルール化不可能 | **CNN/機械学習** |
| **異常検知** | 正常パターンの統計的把握が必要 | **機械学習（教師なし）** |
| **時系列予測** | 連続値、確率的変動 | **RNN/LSTM/統計モデル** |
| **感情分析** | 言語の曖昧性、文脈依存 | **Transformer/NLP** |

**共通する特徴（不適用条件）**：
- ❌ 状態空間が**連続的・高次元**
- ❌ ルールを**明示的に記述できない**
- ❌ **パターン認識**や**統計的推定**が必要
- ❌ **不確実性**が本質的に含まれる

### ハイブリッドアプローチ（境界領域）
現代のAIでは探索と学習を組み合わせる例もある：

- **AlphaGo**: モンテカルロ木探索（MCTS）+ 深層学習（価値・方策ネットワーク）
- **強化学習**: 探索（行動選択）+ 学習（価値関数の更新）
- **ニューロシンボリックAI**: 記号的推論と深層学習の統合

ただし、**主手法は深層学習**であり、探索は補助的。試験では「探索が主手法として適切か」を問う。

---

## 試験での問われ方

### 典型設問（探索手法の選択）
- 「同階層を全て探索後に次階層へ進む手法は？」→ **幅優先探索（BFS）**
- 「深さ方向に進む探索は？」→ **深さ優先探索（DFS）**
- 「最短経路を保証する探索は？」→ **幅優先探索（BFS）**（コスト一定時）
- 「メモリ消費が少ない探索は？」→ **深さ優先探索（DFS）**

### 典型設問（適用問題の判定）
**問：探索・推論の手法が適した問題として、最も不適切な選択肢を1つ選べ**

✅ **適切な選択肢**：
- 「迷路の最短経路を見つける」→ A*探索等が適用可能
- 「8パズルの解を求める」→ 状態空間探索で解決
- 「チェスで最善手を決定する」→ ゲーム木探索（ミニマックス法）
- 「ロボットの動作計画を立てる」→ STRIPSなどのプランニング
- 「スケジュールの最適化」→ 制約充足問題として探索

❌ **不適切な選択肢（試験で狙われる）**：
- **「画像から猫と犬を識別する」** → ❌ **最も不適切**（パターン認識、CNNが適切）
- **「音声から文字列に変換する」** → ❌ 音響モデル+言語モデル（RNN等）が必要
- **「文章の感情を分析する」** → ❌ 自然言語処理、Transformerが適切
- **「手書き数字を認識する」** → ❌ 画像認識、CNNが適切
- **「株価を予測する」** → ❌ 時系列予測、統計/機械学習が適切
- **「異常なデータを検出する」** → ❌ 教師なし学習、統計的手法

### 比較されやすい概念
- **BFS** vs **DFS**: 階層順 vs 深さ優先、キュー vs スタック、最短保証 vs なし
- **BFS** vs **A***: 盲目的探索 vs ヒューリスティック探索
- **DFS** vs **反復深化**: 無制限深さ vs 段階的深さ制限
- **A*** vs **ダイクストラ**: ヒューリスティックあり vs なし
- **記号的AI** vs **サブシンボリックAI**: 探索・推論 vs パターン認識・深層学習
- **離散状態空間** vs **連続状態空間**: 探索適用可 vs 不適

### 引っ掛けポイント
- 「階層ごと」「同じ深さを全て」→ **BFS**（DFSではない）
- 「キュー（FIFO）」→ **BFS**、「スタック（LIFO）」→ **DFS**
- 「最短保証」→ BFS（コスト一定）、A*（許容的ヒューリスティック）
- 「メモリ効率」→ DFS > BFS
- A*の評価関数 $f(n) = g(n) + h(n)$ の意味（実コスト+推定）
- **「画像認識も状態空間として扱えるから探索可能」** → ❌ 理論上可能だが実用的でない（次元の呪い）
- **「音声認識は音素の遷移だから状態探索できる」** → ❌ HMMの時代は使われたが、深層学習が主流
- **「ゲームは深層学習の領域」** → △ AlphaGoは探索+深層学習の組み合わせ（探索も重要）

### 頻出パターン
- データ構造（キュー/スタック）と探索手法の対応
- 探索順序の図からBFS/DFSを判別
- 最短経路問題でBFSまたはA*を選択
- メモリ制約下でDFSを選択
- **画像・音声・自然言語処理は探索手法が不適切**（最頻出）

## 補足
- **実務的観点**：
  - カーナビ・ゲームAIではA*が標準（効率と最適性の両立）
  - Webクローラーでは幅優先が主流（浅い階層を優先）
  - 深い再帰はスタックオーバーフローのリスク（DFS実装時注意）
  - 双方向探索で探索空間を削減（開始・ゴール両方から探索）
- **関連トピック**：
  - [プランニング（STRIPS）](planning.md) - 状態空間探索との組み合わせ
  - [強化学習](../05_machine_learning/reinforcement_learning.md) - 探索と活用のトレードオフ
  - [フレーム問題](frame_problem.md) - 探索空間の爆発
- **発展**：
  - モンテカルロ木探索（MCTS）：ゲームAIで使用
  - ビームサーチ：幅優先の変種（上位k個のみ保持）
  - 分枝限定法：最適化問題での探索
  - ミニマックス法：2人ゲームでの探索（詳細は次セクション）

---

## ゲーム木探索（ミニマックス法・αβ枝刈り）

### 要点（試験用）
- **ミニマックス法**：囲碁・将棋等のゲームで、自分の利益を最大化し相手の利益を最小化する手を選択。
- **Max層**（自分のターン）で利益最大化、**Min層**（相手のターン）で自分の利益を最小化。
- **αβ枝刈り**：不要な枝を探索せず効率化。最善手は変わらず、探索量を大幅削減。

### 定義
**ミニマックス法（Minimax Algorithm）**は、2人の対戦ゲーム（囲碁・将棋・チェス等）で最適な手を選択する探索アルゴリズム。プレイヤーが交互に手番を持つゼロサムゲーム（一方の利益が他方の損失）で使用される。

**基本原理**：
- **自分のターン（Max層）**：利益を**最大化**する手を選ぶ
- **相手のターン（Min層）**：（相手が最善手を選ぶと仮定し）自分の利益を**最小化**する手を想定

これにより、相手が最適に対応しても得られる最良の結果を計算。

### 重要キーワード
- **ミニマックス法（Minimax）**：自分の最大利益と相手の最小利益を追求する探索手法。
- **ゼロサムゲーム（Zero-sum Game）**：一方の利益が他方の損失となるゲーム。
- **Max層（Maxノード）**：自分のターン、利益を最大化。
- **Min層（Minノード）**：相手のターン、自分の利益を最小化（相手の利益を最大化）。
- **ゲーム木（Game Tree）**：可能な手順を木構造で表現。
- **評価関数（Evaluation Function）**：盤面の有利・不利を数値化（+100=自分有利、-100=相手有利等）。
- **終端ノード（Terminal Node）**：ゲーム終了状態（勝敗確定）。
- **αβ枝刈り（Alpha-Beta Pruning）**：ミニマックス法の効率化、最善手に影響しない枝を削除。
- **探索深さ（Depth）**：先読みする手数。深いほど強いが計算コスト大。
- **モンテカルロ木探索（MCTS）**：ランダムシミュレーションで評価する手法（AlphaGoで使用）。

### 詳細

#### ミニマックス法のアルゴリズム

##### 基本プロセス
```
1. 現在の盤面から可能な手を全て展開（子ノード生成）
2. 各子ノードで再帰的にミニマックス法を適用
3. 自分のターン（Max層）: 子ノードの評価値の最大値を選択
4. 相手のターン（Min層）: 子ノードの評価値の最小値を選択
5. 終端ノードまたは深さ制限に達したら評価関数で評価
6. 評価値を親ノードに伝播
```

##### 擬似コード
```python
def minimax(node, depth, is_maximizing):
    # 終端条件
    if depth == 0 or node.is_terminal():
        return evaluate(node)
    
    if is_maximizing:  # Max層（自分のターン）
        max_eval = -∞
        for child in node.children:
            eval = minimax(child, depth-1, False)
            max_eval = max(max_eval, eval)
        return max_eval
    
    else:  # Min層（相手のターン）
        min_eval = +∞
        for child in node.children:
            eval = minimax(child, depth-1, True)
            min_eval = min(min_eval, eval)
        return min_eval
```

#### ゲーム木の例（三目並べ）

```
                   ○の手番（Max層）
                   評価値: 5
                   ↓
        /          |          \
       /           |           \
   ×の手番      ×の手番      ×の手番
   (Min層)      (Min層)      (Min層)
   評価:5       評価:3       評価:6
     ↓            ↓            ↓
   / | \        / | \        / | \
  5  7  6      3  4  8      6  9  7
 (○手番)     (○手番)     (○手番)

Min層の選択: min(5,7,6)=5  min(3,4,8)=3  min(6,9,7)=6
Max層の選択: max(5, 3, 6) = 6 → 右の手を選択
```

**解釈**：
- ○（Max）は評価値6の手を選択（最も有利）
- 各Min層では相手×が最善手（自分にとって最悪）を選ぶと仮定

#### αβ枝刈り（Alpha-Beta Pruning）

##### 目的
ミニマックス法の**探索範囲を削減**し、計算時間を短縮。最善手は変わらず、不要な枝を探索しない。

##### 基本原理
- **α値**：Max層で現在確保している最小保証値（これ以上は確実に得られる）
- **β値**：Min層で現在確保している最大保証値（これ以下に抑えられる）
- α ≥ β のとき、それ以降の子ノードは探索不要（枝刈り）

##### 擬似コード
```python
def alphabeta(node, depth, alpha, beta, is_maximizing):
    if depth == 0 or node.is_terminal():
        return evaluate(node)
    
    if is_maximizing:
        max_eval = -∞
        for child in node.children:
            eval = alphabeta(child, depth-1, alpha, beta, False)
            max_eval = max(max_eval, eval)
            alpha = max(alpha, eval)
            if beta <= alpha:
                break  # β枝刈り
        return max_eval
    
    else:
        min_eval = +∞
        for child in node.children:
            eval = alphabeta(child, depth-1, alpha, beta, True)
            min_eval = min(min_eval, eval)
            beta = min(beta, eval)
            if beta <= alpha:
                break  # α枝刈り
        return min_eval
```

##### 枝刈りの例
```
        Max（α=-∞, β=+∞）
         ↓
    /    |    \
   3    Min   ?（探索不要）
        ↓
      / | \
     3  2  ?（探索不要、すでにMin=2でMax=3より小さい）

説明: 
- 左の子で3を取得（α=3）
- 中央のMinで2を発見→この枝の評価は2以下確定
- Maxは既に3を確保しているため、2以下の枝は不要（枝刈り）
```

##### 効果
- 最悪ケース：$O(b^d)$（枝刈りなし）
- 最良ケース：$O(b^{d/2})$（最適な手順で探索）
- 実用上：探索深さを2倍にできる効果

#### 評価関数の設計

ゲーム未終了時に盤面の有利・不利を評価：

**例：チェス**
```
評価 = (自分の駒の価値) - (相手の駒の価値)
     + (配置ボーナス) - (脅威ペナルティ)

駒の価値:
- ポーン: 1
- ナイト/ビショップ: 3
- ルーク: 5
- クイーン: 9
```

**例：将棋**
```
評価 = (持ち駒の価値) + (盤上の駒の位置評価) + (王の安全度)
```

評価関数の精度が強さに直結。現代では深層学習で学習（AlphaGo等）。

### 実例

#### 例1：三目並べの評価
```
盤面: 
X | O | X
---------
O |   | 
---------
  |   | X

評価関数:
- 勝利: +100
- 敗北: -100
- 2つ揃い（リーチ）: +10 / -10
- 中央: +5

この盤面（○のターン）:
- ×が2箇所リーチ → 評価: -20（不利）
```

#### 例2：αβ枝刈りの効果
```
探索深さ5、分岐数30のチェス：
- 枝刈りなし: 30^5 = 24,300,000 ノード
- αβ枝刈り: 30^2.5 ≈ 5,000 ノード（約0.02%）
→ 大幅な効率化
```

#### 例3：AlphaGoでの応用
AlphaGoは以下を組み合わせ：
- **モンテカルロ木探索（MCTS）**：ランダムプレイアウトで評価
- **価値ネットワーク**：盤面評価を深層学習
- **方策ネットワーク**：次の手の確率を学習
- ミニマックス的な思想（最善手の探索）

### 他手法との違い

| 手法 | 探索方式 | 評価方法 | 適用ゲーム |
|------|----------|----------|-----------|
| **ミニマックス法** | 全探索（深さ制限） | 評価関数 | チェス、オセロ、三目並べ |
| **αβ枝刈り** | ミニマックス+枝刈り | 評価関数 | 同上（効率化） |
| **MCTS** | ランダムサンプリング | シミュレーション | 囲碁（AlphaGo）、複雑ゲーム |
| **Q学習** | 強化学習 | 報酬の累積 | 不完全情報ゲーム、連続行動 |

## 試験での問われ方（ミニマックス法）

### 典型問題
**問**: 「囲碁や将棋といったゲームで、自身の最大利益と相手の最小利益を追求する手を選択する手法」として最も適切なものを選べ。

- ✅ **正解**: ミニマックス法（Minimax Algorithm）
- ❌ 誤答1: モンテカルロ法（ランダムサンプリング、最大・最小の明示的追求ではない）
- ❌ 誤答2: 深さ優先探索（探索順序の手法、Max/Minの概念なし）
- ❌ 誤答3: A*探索（ヒューリスティック探索、ゲーム用ではない）
- ❌ 誤答4: 遺伝的アルゴリズム（最適化手法、ゲーム木探索ではない）

### 比較されやすい概念

| 概念 | 説明 | ゲームでの役割 |
|------|------|----------------|
| **ミニマックス法** | Max/Minを交互に適用 | 最適手の計算 |
| αβ枝刈り | ミニマックスの効率化 | 探索削減 |
| モンテカルロ木探索 | ランダムプレイアウト | 評価精度向上（囲碁等） |
| 評価関数 | 盤面の数値化 | 未終了状態の評価 |
| 深さ優先探索 | 探索順序 | ミニマックスで使用 |

### 引っ掛けポイント
1. **「最大利益と最小利益」の記述**：ミニマックス法の特徴的表現（他の探索手法では使わない）
2. **「相手が最善手を打つ前提」**：Min層で相手の最適応答を想定
3. **「ゲーム木」**：状態空間ではなくゲーム専用の木構造
4. **αβ枝刈りとの関係**：枝刈りはミニマックスの効率化、本質は同じ
5. **モンテカルロ木探索との違い**：
   - ミニマックス：決定論的、全探索（深さ制限）
   - MCTS：確率的、ランダムサンプリング
6. **強化学習との違い**：ミニマックスは事前の探索、強化学習は経験から学習

### キーワードから手法を特定
| 問題文のキーワード | 対応手法 |
|-------------------|----------|
| 「最大利益」「最小利益」 | **ミニマックス法** |
| 「αβ」「枝刈り」 | **αβ枝刈り** |
| 「ランダムプレイアウト」「モンテカルロ」 | **MCTS** |
| 「報酬」「方策」「Q値」 | **強化学習** |
| 「ヒューリスティック」「f(n)=g(n)+h(n)」 | **A*探索** |

## 補足

### 実務上の応用
- **チェスエンジン**（Stockfish等）：ミニマックス+αβ枝刈り+高度な評価関数
- **将棋AI**（Bonanza、やねうら王等）：ミニマックス+機械学習評価関数
- **AlphaGo**：MCTS + 深層学習（ミニマックスは直接使わないが、思想は継承）
- **ゲームAI全般**：リアルタイムゲームでは探索深さを浅く制限

### 限界と対策
- **探索空間の爆発**：囲碁等で分岐数が膨大（19×19=361手）
  - 対策: αβ枝刈り、MCTS、深層学習で評価精度向上
- **評価関数の精度**：終端以外の状態評価が困難
  - 対策: 深層学習で学習（AlphaZero等）
- **計算時間**：リアルタイム性が必要な場合、深さ制限
  - 対策: 反復深化、時間管理

### 関連トピック
- [探索アルゴリズム（BFS/DFS）](#探索アルゴリズム幅優先探索深さ優先探索)：ミニマックスの基礎となる探索
- [プランニング](planning.md)：状態空間探索との違い
- [強化学習](../05_machine_learning/reinforcement_learning.md)：方策学習との対比
