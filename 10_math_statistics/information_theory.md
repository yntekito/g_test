# 情報理論（Information Theory）

## 要点（試験用）
- **エントロピー**：情報の不確実性・乱雑さを表す指標。$H(X) = -\sum p(x) \log p(x)$。
- **相互情報量**：2つの確率変数間の依存性を表す。**I(X, Y) = 0 のとき、XとYは統計的に独立**。
- **KLダイバージェンス**：2つの確率分布の違いを測る非対称な尺度。機械学習の損失関数に使用。

## 定義
**情報理論（Information Theory）**は、情報の量・伝達・圧縮を数学的に扱う理論。クロード・シャノンが1948年に創始。通信理論、機械学習、データ圧縮などに広く応用される。

基本概念：
- **情報量（自己情報量）**：事象の起こりにくさを定量化
- **エントロピー**：確率分布の不確実性・情報量の期待値
- **相互情報量**：2変数間の依存性
- **KLダイバージェンス**：分布間の距離

## 重要キーワード
- **情報量（Self-information）**：事象 $x$ の情報量 $I(x) = -\log p(x)$。確率が低いほど情報量大。
- **エントロピー（Entropy）**：情報量の期待値。$H(X) = -\sum_x p(x) \log p(x)$。
- **シャノンエントロピー**：エントロピーの別名。
- **条件付きエントロピー（Conditional Entropy）**：$H(Y|X) = -\sum_{x,y} p(x,y) \log p(y|x)$。$X$を知った後の$Y$の不確実性。
- **結合エントロピー（Joint Entropy）**：$H(X, Y) = -\sum_{x,y} p(x,y) \log p(x,y)$。2変数の同時分布のエントロピー。
- **相互情報量（Mutual Information）**：$I(X; Y) = H(X) + H(Y) - H(X, Y)$。$X$と$Y$の依存性。
- **KLダイバージェンス（Kullback-Leibler Divergence）**：$D_{KL}(P||Q) = \sum_x p(x) \log \frac{p(x)}{q(x)}$。分布$P$と$Q$の違い。
- **クロスエントロピー（Cross Entropy）**：$H(P, Q) = -\sum_x p(x) \log q(x)$。分類問題の損失関数。
- **情報利得（Information Gain）**：決定木で分岐時のエントロピー減少量。
- **ビット（bit）**：情報量の単位。対数の底を2とする。
- **ナット（nat）**：自然対数を用いた情報量の単位。

## 詳細

### 情報量（Self-information）

**定義**：事象 $x$ が起こったときの情報量

$$
I(x) = -\log p(x) = \log \frac{1}{p(x)}
$$

**直観**：
- 確率が低い事象ほど、起こったときの情報量が大きい
- 例：「太陽が昇った」（確率≈1）→情報量小
- 例：「隕石が落ちた」（確率≪1）→情報量大

**性質**：
- $p(x) = 1$ のとき $I(x) = 0$（確実な事象は情報なし）
- $p(x) \to 0$ のとき $I(x) \to \infty$（稀な事象は情報量大）
- $I(x) \geq 0$（情報量は非負）

### エントロピー（Entropy）

**定義**：情報量の期待値、確率分布の不確実性

$$
H(X) = -\sum_{x} p(x) \log p(x) = \mathbb{E}[-\log p(X)]
$$

連続分布の場合：
$$
H(X) = -\int p(x) \log p(x) dx
$$

**直観**：
- 分布が一様（平坦）なほどエントロピーが大きい（不確実性大）
- 分布が偏る（一点に集中）とエントロピーが小さい（確実性大）

**性質**：
1. **非負性**：$H(X) \geq 0$
2. **最大値**：離散分布では一様分布でエントロピー最大
   - $n$個の等確率事象：$H(X) = \log n$
3. **最小値**：確定的分布（1点に集中）で $H(X) = 0$

**例**：コイン投げ
```
公正なコイン: p(表)=0.5, p(裏)=0.5
H = -0.5 log 0.5 - 0.5 log 0.5 = 1 bit

偏ったコイン: p(表)=0.9, p(裏)=0.1
H = -0.9 log 0.9 - 0.1 log 0.1 ≈ 0.47 bit
→ 結果が予測しやすい（不確実性低い）
```

### 条件付きエントロピー（Conditional Entropy）

**定義**：$X$を知った後の$Y$の不確実性

$$
H(Y|X) = -\sum_{x,y} p(x,y) \log p(y|x) = \mathbb{E}_X[H(Y|X=x)]
$$

**意味**：
- $X$の情報を得た後、$Y$にどれだけ不確実性が残るか
- $H(Y|X) = H(Y)$ なら$X$は$Y$に情報を与えない（独立）
- $H(Y|X) = 0$ なら$X$で$Y$が完全に決まる（決定的）

### 結合エントロピー（Joint Entropy）

**定義**：2変数の同時分布のエントロピー

$$
H(X, Y) = -\sum_{x,y} p(x,y) \log p(x,y)
$$

**連鎖律（Chain Rule）**：
$$
H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
$$

### 相互情報量（Mutual Information）

**定義**：2つの確率変数間の依存性・共有情報量

$$
I(X; Y) = H(X) + H(Y) - H(X, Y)
$$

別の表現：
$$
I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
$$

$$
I(X; Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
$$

**意味**：
- $X$を知ることで$Y$の不確実性がどれだけ減少するか
- $X$と$Y$がどれだけ「情報を共有」しているか

**性質**：
1. **非負性**：$I(X; Y) \geq 0$
2. **対称性**：$I(X; Y) = I(Y; X)$
3. **独立性**：**$I(X; Y) = 0 \iff X$と$Y$は統計的に独立**（試験頻出）
4. **最大値**：$I(X; Y) \leq \min(H(X), H(Y))$

**ベン図表現**：
```
   H(X)          H(Y)
    ┌─────┐  ┌─────┐
    │     │  │     │
    │  ┌──┼──┼──┐  │
    │  │  │  │  │  │
    └──┼──┘  └──┼──┘
       │  I(X;Y) │
  H(X|Y)│        │H(Y|X)
       └─────────┘

H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
I(X; Y) = H(X) + H(Y) - H(X, Y)
```

#### 相互情報量 = 0 の意味（試験最重要）

**I(X; Y) = 0 のとき、XとYは統計的に独立**

証明：
$$
I(X; Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)} = 0
$$

これは $p(x,y) = p(x)p(y)$ のとき（独立）のみ成立。

**直観**：
- $X$を知っても$Y$の不確実性が全く減らない
- $X$と$Y$は互いに情報を与えない
- $H(Y|X) = H(Y)$（$X$を知っても$Y$のエントロピー不変）

### KLダイバージェンス（Kullback-Leibler Divergence）

**定義**：2つの確率分布 $P$ と $Q$ の違いを測る

$$
D_{KL}(P||Q) = \sum_{x} p(x) \log \frac{p(x)}{q(x)} = \mathbb{E}_P\left[\log \frac{P(X)}{Q(X)}\right]
$$

**性質**：
1. **非負性**：$D_{KL}(P||Q) \geq 0$
2. **非対称**：$D_{KL}(P||Q) \neq D_{KL}(Q||P)$（距離ではない）
3. **$P=Q$のとき0**：$D_{KL}(P||Q) = 0 \iff P = Q$

**意味**：
- $P$を$Q$で近似したときの情報の損失
- $P$（真の分布）と$Q$（近似分布）の「乖離度」

**機械学習での応用**：
- VAE（変分オートエンコーダ）の損失関数
- 分布の近似・学習の評価

### クロスエントロピー（Cross Entropy）

**定義**：分布$P$のデータを分布$Q$で符号化したときの平均符号長

$$
H(P, Q) = -\sum_{x} p(x) \log q(x)
$$

**関係式**：
$$
H(P, Q) = H(P) + D_{KL}(P||Q)
$$

**機械学習での応用**：
- **分類問題の損失関数**（最重要）
- $P$：正解ラベル（one-hot）
- $Q$：モデルの予測確率
- クロスエントロピー最小化 = KLダイバージェンス最小化

**例**：2クラス分類
```
正解: y=1（猫）
予測: p(猫)=0.8, p(犬)=0.2

クロスエントロピー損失:
L = -[1×log(0.8) + 0×log(0.2)] ≈ 0.22

予測が正解に近いほど損失小
```

## 実例

### 例1：エントロピーの計算
```
天気の確率分布:
p(晴) = 0.5, p(曇) = 0.3, p(雨) = 0.2

エントロピー:
H = -0.5 log₂(0.5) - 0.3 log₂(0.3) - 0.2 log₂(0.2)
  = 0.5 + 0.521 + 0.464
  ≈ 1.485 bit

→ 一様分布（log₂ 3 ≈ 1.585 bit）より小さい
```

### 例2：相互情報量
```
X: コイン1の結果（表/裏）
Y: コイン2の結果（表/裏）

【ケース1】独立なコイン:
p(X=表, Y=表) = 0.25（= 0.5 × 0.5）
→ I(X; Y) = 0（独立）

【ケース2】同じコイン（Y=X）:
p(X=表, Y=表) = 0.5, p(X=裏, Y=裏) = 0.5
H(X) = 1 bit, H(Y) = 1 bit, H(X,Y) = 1 bit
→ I(X; Y) = 1 + 1 - 1 = 1 bit（完全依存）
```

### 例3：条件付きエントロピー
```
X: 天気（晴/雨）
Y: 傘を持つか（持つ/持たない）

雨なら必ず傘、晴なら50%で傘

H(Y) = 1 bit（一様でない場合を仮定）
H(Y|X) ≈ 0.5 bit（天気を知れば不確実性減少）
I(X; Y) = H(Y) - H(Y|X) = 0.5 bit
→ 天気は傘に0.5 bitの情報を提供
```

### 例4：機械学習でのクロスエントロピー
```
3クラス分類（猫/犬/鳥）
正解: 猫
正解ラベル（one-hot）: [1, 0, 0]
モデル予測: [0.7, 0.2, 0.1]

クロスエントロピー損失:
L = -[1×log(0.7) + 0×log(0.2) + 0×log(0.1)]
  = -log(0.7)
  ≈ 0.357

完全な予測[1, 0, 0]なら L = 0
```

## 試験での問われ方

### 典型問題
**問**: 確率分布XとYの相互情報量をI(X, Y)とする。I(X, Y) = 0 であるときは、（　）である。

- ✅ **正解**: XとYは統計的に独立である
- ✅ **正解**: XとYは互いに情報を与えない
- ✅ **正解**: p(x, y) = p(x)p(y) が成り立つ
- ❌ 誤答: XとYは完全に相関している（逆、I(X;Y)最大のとき）
- ❌ 誤答: XとYは同じ分布である（独立性と同一性は別概念）
- ❌ 誤答: XとYの分散が等しい（分散とは無関係）

### 関連する典型問題

#### エントロピーの性質
**問**: エントロピーが最大となる分布は？
- ✅ 正解: 一様分布（全ての事象が等確率）

**問**: エントロピーが0となるのは？
- ✅ 正解: 確定的分布（1つの事象の確率が1）

#### クロスエントロピー
**問**: 分類問題の損失関数として使われるのは？
- ✅ 正解: クロスエントロピー

**問**: クロスエントロピーとKLダイバージェンスの関係は？
- ✅ 正解: H(P, Q) = H(P) + D_{KL}(P||Q)

### 比較されやすい概念

| 概念 | 記号 | 意味 | 特徴 |
|------|------|------|------|
| **情報量** | $I(x)$ | 事象の起こりにくさ | 個別事象 |
| **エントロピー** | $H(X)$ | 分布の不確実性 | 期待値 |
| **条件付きエントロピー** | $H(Y\|X)$ | $X$を知った後の$Y$の不確実性 | 依存性 |
| **相互情報量** | $I(X;Y)$ | 2変数の依存度 | **I=0で独立** |
| **KLダイバージェンス** | $D_{KL}(P\|\|Q)$ | 分布間の乖離 | 非対称 |
| **クロスエントロピー** | $H(P, Q)$ | 符号化の効率 | 損失関数 |

### 引っ掛けポイント
1. **「相互情報量=0」→独立**（最頻出、完全暗記）
2. **「相互情報量=最大」→完全依存**（Y=f(X)等）
3. **KLダイバージェンスは非対称**：$D_{KL}(P||Q) \neq D_{KL}(Q||P)$
4. **エントロピー最大=一様分布**（等確率）
5. **クロスエントロピーは対称ではない**：$H(P, Q) \neq H(Q, P)$
6. **条件付きエントロピーの連鎖律**：$H(X, Y) = H(X) + H(Y|X)$

### 数式の変形（試験で役立つ）
```
相互情報量の表現:
I(X; Y) = H(X) + H(Y) - H(X, Y)
        = H(X) - H(X|Y)
        = H(Y) - H(Y|X)
        = Σ p(x,y) log[p(x,y)/(p(x)p(y))]

独立条件:
I(X; Y) = 0 ⟺ p(x,y) = p(x)p(y) ⟺ H(Y|X) = H(Y)
```

## 補足

### 情報理論と機械学習の関係

| 情報理論 | 機械学習での応用 |
|----------|------------------|
| エントロピー | 決定木の情報利得、不純度 |
| クロスエントロピー | 分類問題の損失関数 |
| KLダイバージェンス | VAE、GAN、強化学習の損失 |
| 相互情報量 | 特徴選択、依存性解析 |

### 実務的な応用
1. **データ圧縮**：エントロピーが圧縮の理論的限界（シャノン限界）
2. **通信理論**：チャネル容量、符号化
3. **決定木**：情報利得で分岐選択（ID3、C4.5アルゴリズム）
4. **ニューラルネット**：クロスエントロピー損失で学習
5. **特徴選択**：相互情報量で有用な特徴を選択
6. **異常検知**：KLダイバージェンスで分布の変化検出

### 対数の底の慣習
- **bit（ビット）**：$\log_2$（情報理論の標準）
- **nat（ナット）**：$\ln$（自然対数、微積分で便利）
- **dit（ディット）**：$\log_{10}$（稀）

機械学習では自然対数（nat）を使うことが多い。

### 重要な不等式
- **ジェンセンの不等式から**：$D_{KL}(P||Q) \geq 0$
- **データ処理不等式**：$I(X; Y) \geq I(X; f(Y))$（情報は処理で減少）
- **Fanoの不等式**：推定誤差の下界

### 歴史的背景
- **1948年**：クロード・シャノン「通信の数学的理論」発表
- デジタル通信・データ圧縮の理論的基礎
- AIや機械学習にも広く応用される普遍的理論

### 関連トピック
- [確率論](probability.md)：情報理論の基礎
- [統計学](statistics.md)：統計的推定・検定
- [評価指標](../05_machine_learning/evaluation_metrics.md)：クロスエントロピー損失
- [深層学習](../06_deep_learning/neural_network_basics.md)：損失関数での応用
