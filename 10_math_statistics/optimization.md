# 最適化（Optimization）

## 要点
- 最適化は損失関数を最小化（または目的関数を最大化）するパラメータを見つける手法。深層学習では勾配降下法が基本。
- **局所最適解（Local Optimum）**：探索範囲内で最適だが全体では最適でない可能性がある解。勾配降下法の課題。
- **大域最適解（Global Optimum）**：全体で最適な解。学習率の調整、モメンタム、Adam等の工夫で改善。

## 定義
**最適化（Optimization）**は、目的関数 $f(x)$ を最小化（または最大化）する $x$ を見つける問題。深層学習では、損失関数 $L(\theta)$ を最小化するパラメータ $\theta$ を勾配に基づいて更新する。

$$\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)$$

- $\theta$: パラメータ（重み）
- $\eta$: 学習率（Learning Rate）
- $\nabla L$: 損失関数の勾配

## 重要キーワード
- **勾配降下法（Gradient Descent）**: 勾配の逆方向にパラメータを更新する最適化手法
- **学習率（Learning Rate）**: パラメータ更新の幅を制御するハイパーパラメータ
- **局所最適解（Local Optimum / Local Minimum）**: 探索範囲内で最適だが全体では最適でない可能性がある解
- **大域最適解（Global Optimum / Global Minimum）**: 全体で最も良い解
- **鞍点（Saddle Point）**: 一部の方向では極小、別の方向では極大となる点
- **モメンタム（Momentum）**: 過去の勾配を慣性として利用し、局所解を脱出しやすくする
- **学習率減衰（Learning Rate Decay）**: 学習の進行に応じて学習率を小さくする手法
- **Adam**: モメンタムと適応的学習率を組み合わせた最適化手法

---

## 局所最適解と大域最適解（★試験重要）

### 定義

**局所最適解（Local Optimum）**:
- その近傍では最も良い解だが、全体では最適でない可能性がある
- 勾配が0（$\nabla L = 0$）になるため、勾配降下法が停止してしまう
- 探索範囲の中で最適な解であって、その範囲外では必ずしも最適ではない

**大域最適解（Global Optimum）**:
- 全体（全パラメータ空間）で最も良い解
- すべての点の中で損失関数が最小（または目的関数が最大）

### 図解

```
損失関数 L(θ)
  ↑
  │    ╱╲                    ╱╲
  │   ╱  ╲        ╱╲        ╱  ╲
  │  ╱    ╲      ╱  ╲      ╱    ╲
  │ ╱   局所 ╲   ╱ 局所 ╲   ╱      ╲
  │╱    最適解 ╲╱   最適解 ╲╱  大域最適解
  └─────────────────────────────────→ θ
  
勾配降下法の問題:
- 初期値によっては局所最適解に陥る
- 局所解では勾配=0なので更新が止まる
```

### 典型的な試験問題（穴埋め）

> 「勾配降下法では（A）に収束してしまう可能性が高くなります。これは探索範囲の中で最適な解であって、その範囲外では必ずしも最適ではない可能性がある結果のことです。
> 
> これを避けて全体最適解である（B）を得るためには、学習率を大きく設定し、適切なタイミングで学習率を小さくしていく必要があります。」

**正解**:
- **(A): 局所最適解（局所解、Local Optimum、Local Minimum）**
- **(B): 大域最適解（大域解、Global Optimum、Global Minimum）**

### 局所解に陥る原因

1. **勾配が0になる**: 局所解では $\nabla L = 0$ なので更新が停止
2. **学習率が小さすぎる**: 小さな谷から抜け出せない
3. **初期値が不適切**: 悪い局所解の近くから開始してしまう
4. **単純な勾配降下法**: モメンタムや適応的手法を使わない

### 局所解を避ける手法

#### 1. 学習率の調整

**学習率減衰（Learning Rate Decay）**:
- **初期**: 学習率を大きく設定 → 局所解を飛び越える
- **後期**: 学習率を徐々に小さく → 大域解の近くで精密に収束

**スケジュール例**:
```
ステップ減衰: η_t = η_0 / (1 + decay_rate × epoch)
指数減衰:     η_t = η_0 × 0.95^epoch
コサイン減衰:  η_t = η_min + (η_0 - η_min) × (1 + cos(πt/T)) / 2
```

#### 2. モメンタム（Momentum）

**原理**:
- 過去の勾配情報を慣性として利用
- 局所解の小さな谷を勢いで飛び越える

**更新式**:
$$v_t = \gamma v_{t-1} + \eta \nabla L(\theta_t)$$
$$\theta_{t+1} = \theta_t - v_t$$

- $v$: 速度（過去の勾配の移動平均）
- $\gamma$: モメンタム係数（通常0.9）

**効果**:
- ✅ 勾配の振動を抑制
- ✅ 局所解から脱出しやすい
- ✅ 収束が高速化

#### 3. 適応的学習率手法

**Adam（Adaptive Moment Estimation）**:
- 各パラメータごとに学習率を自動調整
- モメンタムと適応的学習率を組み合わせ
- 現在最も広く使われる最適化手法

**RMSprop**:
- 勾配の2乗の移動平均で学習率を調整
- RNNの学習で有効

**AdaGrad**:
- 頻繁に更新されるパラメータの学習率を小さくする
- スパースなデータで有効

#### 4. その他の工夫

**ランダム初期化**:
- 異なる初期値から複数回学習し、最良の結果を選択
- アンサンブル学習で複数モデルを組み合わせ

**Batch Normalization**:
- 各層の入力を正規化し、最適化を安定化

**Skip Connection（ResNet）**:
- 勾配消失を防ぎ、深いネットワークでも最適化可能

### 実際の深層学習での状況

**凸最適化 vs 非凸最適化**:

| 項目 | 凸最適化 | 非凸最適化（深層学習） |
|------|---------|---------------------|
| **局所解** | 1つ（= 大域解） | **多数存在** |
| **保証** | 大域解に収束 | **保証なし** |
| **例** | 線形回帰、SVM | **ニューラルネットワーク** |

**深層学習の現実**:
- 理論的には局所解の問題が存在
- **実務では**：良い局所解に到達すれば十分性能が出ることが多い
- 現代の工夫（Adam、Batch Norm、ResNet等）で実用的な解が得られる

### 鞍点（Saddle Point）問題★試験頻出

**定義**:
- 一部の方向では極小、別の方向では極大となる点
- 勾配が0（$\nabla L = 0$）だが局所解ではない

**図解**:
```
    ╱│╲
   ╱ │ ╲     ← この方向では極大
  ╱  │  ╲
 ╱   ●   ╲   ● = 鞍点（勾配=0）
╱    │    ╲
─────┼─────  ← この方向では極小
     │
```

**深層学習での影響**:
- 高次元空間では**鞍点が局所解より多い**（理論的に示されている）
- 勾配が0なので通常の勾配降下法では停滞してしまう
- 局所解よりも鞍点での停滞が実際の問題となることが多い

**鞍点問題への対処法（1990年代に提唱）**:

#### モメンタム（Momentum）★重要

**提唱**: 1990年代初頭に提唱された最適化技術

**原理**:
- 過去の勾配情報を**慣性（モーメント）**として利用
- **最適化の進行方向に学習を加速**させる
- 鞍点や平坦な領域での**学習の停滞を防ぐ**

**更新式**:
$$v_t = \gamma v_{t-1} + \eta \nabla L(\theta_t)$$
$$\theta_{t+1} = \theta_t - v_t$$

- $v_t$: 速度ベクトル（過去の勾配の指数移動平均）
- $\gamma$: モメンタム係数（通常0.9、範囲0～1）
- $\eta$: 学習率

**鞍点での効果**:
```
鞍点で勾配=0になっても、過去の勢い（v）で突破
        ↓
通常のSGD: 鞍点で停止
         ●─────（勾配0で動けない）
         
Momentum: 鞍点を突破
    ─────●───→ （過去の勢いで前進）
```

**Adamなどのモデルへの採用**:
- **Adam（2014年）**: モメンタムの概念を組み込んだ最適化手法
  - 1次モーメント（勾配の移動平均）を利用
  - 2次モーメント（勾配の2乗の移動平均）も追加
- **RMSprop**: モメンタムの考えを拡張
- **NAdam**: Nesterov Momentum + Adam

**効果**:
- ✅ **鞍点を突破**：勾配が0でも過去の勢いで前進
- ✅ **勾配の振動を抑制**：ノイズの多いSGDを滑らかに
- ✅ **局所解から脱出**：慣性で小さな谷を飛び越える
- ✅ **収束の高速化**：進行方向を一貫させる

**典型的な試験問題（穴埋め）**:

> 「鞍点問題への対処法としては、1990年代に提唱された（**モメンタム**）がある。Adamなどのモデルに採用され、最適化の進行方向に学習を加速させ、学習の停滞を防ぐ効果があります。」

**他の2次最適化手法**:
- **Newton法**: ヘッセ行列（2次微分）を使い鞍点の方向を判定
- **共役勾配法**: 前回の探索方向を考慮
- ただし計算コスト大、実用はモメンタム系が主流

---

## 勾配降下法の種類

### 1. バッチ勾配降下法（Batch Gradient Descent）

**特徴**:
- 全訓練データで勾配を計算
- 1回の更新で大量の計算

**更新式**:
$$\theta_{t+1} = \theta_t - \eta \frac{1}{N} \sum_{i=1}^N \nabla L(\theta_t; x_i, y_i)$$

**利点**:
- 安定した収束
- 真の勾配方向に更新

**欠点**:
- 計算コストが大きい
- メモリ使用量が多い
- オンライン学習不可

### 2. 確率的勾配降下法（SGD: Stochastic Gradient Descent）

**特徴**:
- **1サンプルごとに勾配を計算して更新**
- 高速だがノイズが大きい

**更新式**:
$$\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t; x_i, y_i)$$

**利点**:
- 計算が高速
- オンライン学習可能
- **局所解から脱出しやすい**（ノイズが助けになる）

**欠点**:
- 勾配が不安定（ノイズが大きい）
- 収束が遅い
- 最適解の周りで振動

### 3. ミニバッチ勾配降下法（Mini-batch Gradient Descent）

**特徴**:
- **小さなバッチ（例：32, 64, 128）で勾配を計算**
- バッチGDとSGDの中間
- **現在の標準手法**

**更新式**:
$$\theta_{t+1} = \theta_t - \eta \frac{1}{B} \sum_{i \in \text{batch}} \nabla L(\theta_t; x_i, y_i)$$

**利点**:
- 計算効率が高い（GPU並列化）
- 勾配が比較的安定
- 局所解脱出とノイズのバランスが良い

**欠点**:
- バッチサイズの調整が必要

---

## 主要な最適化アルゴリズム

### 比較表

| アルゴリズム | 特徴 | 学習率調整 | 用途 |
|------------|------|-----------|------|
| **SGD** | シンプル、モメンタムと併用 | 手動 | 基本的な学習 |
| **SGD + Momentum** | 慣性で収束高速化 | 手動 | 画像認識（ResNet） |
| **AdaGrad** | 各パラメータごとに適応 | 自動 | スパースデータ |
| **RMSprop** | AdaGradの改良 | 自動 | RNN学習 |
| **Adam** | **最も広く使われる** | 自動 | **汎用的（デフォルト推奨）** |
| **AdamW** | Adamに重み減衰を追加 | 自動 | Transformer学習 |

### Adam（Adaptive Moment Estimation）

**特徴**:
- **モメンタム**（1次モーメント）と**適応的学習率**（2次モーメント）を組み合わせ
- 各パラメータごとに学習率を自動調整
- **現在最も広く使われる最適化手法**

**更新式**:
$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla L(\theta_t)$$
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla L(\theta_t))^2$$
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$
$$\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

**ハイパーパラメータ**:
- $\beta_1 = 0.9$（1次モーメントの減衰率）
- $\beta_2 = 0.999$（2次モーメントの減衰率）
- $\epsilon = 10^{-8}$（数値安定性のための小さな値）
- $\eta = 0.001$（学習率、デフォルト）

**利点**:
- ✅ ハイパーパラメータの調整が容易
- ✅ 各パラメータごとに最適な学習率
- ✅ 幅広いタスクで高性能
- ✅ モメンタムと適応的学習率の両方の利点

**欠点**:
- 一部のタスクでSGD+Momentumより汎化性能が劣る場合がある

---

## 試験での問われ方

### 典型設問

**穴埋め問題**:
> 「勾配降下法では**（局所最適解）**に収束してしまう可能性が高くなります。これを避けて**（大域最適解）**を得るためには、学習率を大きく設定し、適切なタイミングで学習率を小さくしていく必要があります。」

**選択問題**:
- 「局所最適解を避ける手法として適切なものは？」
  - ✅ 学習率減衰、モメンタム、Adam
  - ❌ 学習率を固定、全結合層を増やす

- 「現在最も広く使われる最適化手法は？」
  - ✅ **Adam**
  - ❌ SGD（基本だが古い）、AdaGrad（特定用途）

### 引っ掛けポイント

| ひっかけ | 正しい理解 |
|----------|------------|
| ❌ 局所解 = 悪い解 | ✅ 深層学習では良い局所解で十分な場合が多い |
| ❌ 大域解は必ず見つかる | ✅ 理論的保証はない（非凸最適化） |
| ❌ 学習率は大きいほど良い | ✅ 大きすぎると発散、小さすぎると収束遅い |
| ❌ Adamは常に最良 | ✅ タスクによってはSGD+Momentumが優位 |
| ❌ 鞍点 = 局所解 | ✅ 鞍点は勾配0だが極値ではない |

### 比較されやすい概念

**局所解 vs 大域解**:
- 局所解：近傍で最適、範囲外では最適でない可能性
- 大域解：全体で最適

**凸最適化 vs 非凸最適化**:
- 凸：局所解=大域解、収束保証あり
- 非凸：局所解が多数、収束保証なし（深層学習）

**SGD vs Adam**:
- SGD：シンプル、モメンタムと併用、画像認識で強い
- Adam：適応的学習率、汎用的、デフォルト推奨

**モメンタム vs 適応的学習率**:
- モメンタム：過去の勾配を利用（慣性）
- 適応的学習率：各パラメータごとに学習率調整

---

## 補足

### 実務的観点

**最適化手法の選択**:
1. **デフォルト**: Adam（学習率0.001）を試す
2. **画像認識**: SGD + Momentum（学習率0.1）+ 学習率減衰
3. **NLP/Transformer**: AdamW（重み減衰付き）
4. **RNN**: Adam または RMSprop

**学習率の設定**:
- **Learning Rate Finder**: 小さい学習率から徐々に増やし、損失が急減する点を探す
- **Warm-up**: 最初の数エポックで学習率を徐々に上げる（Transformerで有効）
- **Cosine Annealing**: コサイン関数で学習率を周期的に変化

**収束の確認**:
- 訓練損失が減少しているか
- 検証損失が減少しているか（過学習の確認）
- 勾配のノルムが適切か（勾配消失・爆発の確認）

### 発展

**2次の最適化手法**:
- **Newton法**: ヘッセ行列（2次微分）を利用、高次元では計算困難
- **L-BFGS**: 準ニュートン法、メモリ効率的だが深層学習には不向き

**最新の最適化手法**:
- **Lookahead**: 複数ステップ先を見て更新
- **RAdam**: Adamの学習初期の不安定性を改善
- **Adafactor**: メモリ効率的なAdam改良版（大規模言語モデル）

### 関連トピック
- [誤差逆伝播法](../06_deep_learning/backpropagation.md) - 勾配計算の基礎
- [ニューラルネットワーク基礎](../06_deep_learning/neural_network_basics.md) - 学習の全体像
- [過学習対策](../05_machine_learning/overfitting_underfitting.md) - 正則化手法
- [線形代数](linear_algebra.md) - 勾配、ヘッセ行列
- [確率・統計](statistics.md) - 確率的勾配降下法の理論
