# 強化学習（Reinforcement Learning）

## 要点
- 試行錯誤を通じて報酬を最大化する行動（方策）を学習する機械学習パラダイム。教師あり学習と異なり、正解は与えられず報酬シグナルから学習。
- 主要要素：エージェント、環境、状態、行動、報酬、方策。マルコフ決定過程（MDP）でモデル化。
- 代表手法：Q学習（価値ベース）、Policy Gradient（方策ベース）、Actor-Critic（両者の組み合わせ）、DQN（深層強化学習）。

## 定義
強化学習とは、エージェントが環境と相互作用しながら、試行錯誤を通じて累積報酬を最大化する行動方策を学習する機械学習の枠組み。正解ラベルではなく、報酬（スカラー値）というフィードバック信号を使って学習する。

## 重要キーワード
- **エージェント（Agent）**: 学習・行動する主体
- **環境（Environment）**: エージェントが相互作用する対象
- **状態（State）**: 環境の現在の状況
- **行動（Action）**: エージェントが選択する動作
- **報酬（Reward）**: 行動の結果得られるスカラー値のフィードバック
- **方策（Policy）**: 状態から行動への写像 $\pi: S \rightarrow A$
- **価値関数（Value Function）**: 状態や行動の長期的な良さ
- **Q関数**: 状態-行動ペアの価値 $Q(s,a)$
- **マルコフ決定過程（MDP）**: 強化学習の数学的枠組み

---

## Actor-Critic手法

### 要点
Actor-Criticは方策ベースと価値ベースを組み合わせた強化学習手法。Actorが方策（行動選択）を学習し、Criticが価値関数（行動評価）を学習。両者が協調して学習効率と安定性を向上。A3C、PPO等の発展手法がある。

### 定義
Actor-Critic手法とは、Actor（方策を学習するネットワーク）とCritic（価値関数を学習するネットワーク）の2つのモジュールを持つ強化学習アルゴリズム。Actorが行動を選択し、Criticがその行動の良さ（価値）を評価してActorにフィードバックすることで学習を進める。

### 重要キーワード
- **Actor（行動者）**: 方策 $\pi(a|s)$ を学習し、状態に応じて行動を選択
- **Critic（評価者）**: 価値関数 $V(s)$ または $Q(s,a)$ を学習し、行動を評価
- **方策ベース**: 方策を直接学習（例：Policy Gradient）
- **価値ベース**: 価値関数を学習して行動選択（例：Q学習、DQN）
- **TD誤差（Temporal Difference Error）**: Criticが計算する予測誤差、Actorの学習信号
- **Advantage関数**: $A(s,a) = Q(s,a) - V(s)$、基準からの優位性
- **A3C（Asynchronous Advantage Actor-Critic）**: 並列学習版
- **PPO（Proximal Policy Optimization）**: 安定性向上版

### 詳細

#### 背景
強化学習には大きく2つのアプローチがある：
- **価値ベース**（Q学習、DQN）: 価値関数を学習し、最大価値の行動を選択。決定的だが探索が難しい
- **方策ベース**（Policy Gradient）: 方策を直接学習。確率的だが分散が大きく学習が不安定

Actor-Criticは両者の利点を組み合わせ、安定性と効率を両立。

#### 基本構造

**Actor（行動者）**:
- **役割**: 方策 $\pi_\theta(a|s)$ を学習し、行動を選択
- **入力**: 状態 $s$
- **出力**: 行動の確率分布または行動そのもの
- **学習**: Criticからの評価を使って方策を改善

**Critic（評価者）**:
- **役割**: 価値関数 $V_\phi(s)$ を学習し、状態・行動を評価
- **入力**: 状態 $s$（または状態-行動ペア $(s,a)$）
- **出力**: 価値の推定値
- **学習**: TD誤差を最小化

#### アルゴリズムの流れ
```
1. 初期状態 s を観測
2. Actor: 方策 π(a|s) に従って行動 a を選択
3. 環境: 行動を実行し、報酬 r と次状態 s' を返す
4. Critic: TD誤差を計算
   δ = r + γV(s') - V(s)
5. Critic: 価値関数を更新（TD誤差を減らす）
   V(s) ← V(s) + α_c・δ
6. Actor: 方策を更新（TD誤差を利用）
   θ ← θ + α_a・δ・∇log π(a|s)
7. s ← s' として繰り返し
```

#### 図解（構造）
```
        環境
         ↓ 状態 s
    ┌────────────┐
    │   Actor    │ ← 方策パラメータ θ
    │ π(a|s;θ)   │ (行動を選択)
    └────────────┘
         ↓ 行動 a
        環境
         ↓ 報酬 r, 次状態 s'
    ┌────────────┐
    │   Critic   │ ← 価値パラメータ φ
    │  V(s;φ)    │ (行動を評価)
    └────────────┘
         ↓ TD誤差 δ
    Actor更新 ← δ
```

#### 主要な変種

**Advantage Actor-Critic（A2C）**:
- **Advantage関数**: $A(s,a) = Q(s,a) - V(s)$
- **利点**: 基準値からの相対的な良さを評価、分散削減
- **更新**: $\theta \leftarrow \theta + \alpha \cdot A(s,a) \cdot \nabla \log \pi(a|s)$

**A3C（Asynchronous Advantage Actor-Critic）**:
- **特徴**: 複数の並列エージェントで非同期学習
- **利点**: 学習の多様性、高速化
- **DeepMind**: Atariゲームで高性能

**PPO（Proximal Policy Optimization）**:
- **特徴**: 方策の大幅な変更を制限（Clipping）
- **利点**: 学習の安定性向上
- **用途**: ロボット制御、OpenAI Five等

### 試験での問われ方

#### 典型設問
- **「Actorが（行動を選択）し、Criticが（行動を評価）する」**
- **「Actorが（方策を学習）し、Criticが（価値関数を学習）する」**
- Actor-Criticの構成要素と役割
- 価値ベース・方策ベース・Actor-Criticの違い
- TD誤差の役割
- Advantage関数の意味

#### ひっかけポイントと違いの整理

| 項目 | Actor-Critic | Q学習/DQN | Policy Gradient |
|------|--------------|-----------|-----------------|
| **分類** | 方策+価値ベース | 価値ベース | 方策ベース |
| **学習対象** | 方策と価値関数 | 価値関数のみ | 方策のみ |
| **行動選択** | 方策から確率的 | 価値最大を決定的 | 方策から確率的 |
| **評価方法** | Criticが評価 | Q値で評価 | モンテカルロ報酬 |
| **安定性** | 中〜高 | 高 | 低（分散大） |
| **探索** | 方策に組み込み | ε-greedy等必要 | 自然に探索 |

**混同注意**:
- **Actor-Critic vs Q学習**: Actor-Criticは方策を直接学習、Q学習は価値から行動を導出
- **Actor-Critic vs Policy Gradient**: Actor-CriticはCriticで評価、Policy Gradientはモンテカルロ報酬
- **A2C vs A3C**: A2Cは同期学習、A3Cは非同期並列学習

**出題パターン**:
- 「Actorの役割」→**行動選択、方策学習**
- 「Criticの役割」→**行動評価、価値関数学習**
- 「TD誤差の使用目的」→**CriticとActorの両方を更新**
- 「方策ベースと価値ベースの組み合わせ」→**Actor-Critic**

### 補足

#### 実務観点
**用途**: 
- ロボット制御（連続動作空間）
- ゲームAI（Atari、囲碁、DOTA2）
- 自動運転（経路計画）
- 資源最適化

**利点**:
- 連続動作空間に対応
- 確率的方策で探索が自然
- 価値ベースより柔軟

**課題**:
- ハイパーパラメータ調整が難しい
- 学習の安定性（PPOで改善）
- 計算コスト（2つのネットワーク）

#### 実装のポイント
- **ネットワーク構成**: ActorとCriticで重みを共有することも可能
- **学習率**: Actor $\alpha_a$ とCritic $\alpha_c$ を別々に設定
- **報酬の正規化**: 学習の安定化に重要
- **ベースライン**: Criticの価値推定を基準に使用

#### 関連トピック
- [教師あり学習](supervised_learning.md) - 学習パラダイムの違い
- [ニューラルネットワーク](../06_deep_learning/neural_network_basics.md) - Deep Actor-Criticの基礎

---

## 強化学習の基礎概念

### マルコフ決定過程（MDP）
- **構成要素**: 状態集合 $S$、行動集合 $A$、遷移確率 $P(s'|s,a)$、報酬関数 $R(s,a)$、割引率 $\gamma$
- **マルコフ性**: 次状態は現在の状態と行動のみに依存
- **目的**: 累積報酬の期待値 $\mathbb{E}[\sum_{t=0}^\infty \gamma^t R_t]$ を最大化

### 主要な学習手法

#### Q学習
- **価値ベース**: Q関数 $Q(s,a)$ を学習
- **更新式**: $Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$
- **特徴**: オフポリシー学習、離散行動空間

---

## ε-greedy方策

### 要点
ε-greedy（イプシロン・グリーディ）方策は、探索（Exploration）と活用（Exploitation）のバランスを取る最も基本的な行動選択戦略。確率 $\epsilon$ でランダムな行動（探索）、確率 $1-\epsilon$ で現在最良の行動（活用）を選択する。Q学習やDQNで標準的に使用される。

### 定義
ε-greedy方策とは、強化学習における行動選択戦略の1つで、以下のルールに従う：
- **確率 $\epsilon$（通常0.1〜0.3）**: すべての行動からランダムに選択（探索）
- **確率 $1-\epsilon$**: 現在のQ値が最大の行動を選択（活用）

数式表現：
$$
a = \begin{cases}
\arg\max_{a'} Q(s,a') & \text{確率 } 1-\epsilon \text{ （活用）} \\
\text{random}(A) & \text{確率 } \epsilon \text{ （探索）}
\end{cases}
$$

### 重要キーワード
- **探索（Exploration）**: 未知の行動を試して新しい情報を得る
- **活用（Exploitation）**: 現在の知識で最良の行動を選ぶ
- **ε（イプシロン）**: ランダム行動の確率（0≤ε≤1）
- **グリーディ（Greedy）**: 常に最良の行動を選ぶ戦略
- **ε-減衰（ε-decay）**: 学習進行に応じてεを減少させる手法
- **探索と活用のトレードオフ**: 探索しすぎると報酬が低く、活用しすぎると局所解に陥る

### 詳細

#### 背景
強化学習では、未知の環境で最適な方策を見つける必要がある。この過程で2つの相反する要求が生じる：

- **探索（Exploration）**: 未経験の行動を試して環境の情報を収集。長期的には必要だが短期的には報酬が低い
- **活用（Exploitation）**: 既知の最良行動を選んで報酬を最大化。短期的には有効だが新しい発見がない

**探索と活用のジレンマ**:
- 探索しすぎ → 報酬が低い、学習が非効率
- 活用しすぎ → 局所最適解に陥る、より良い行動を見逃す

ε-greedy方策は、このトレードオフを単純かつ効果的にバランスさせる。

#### 基本アルゴリズム
```
現在の状態: s
Q値テーブル: Q(s,a) (すべての状態-行動ペア)
パラメータ: ε (例: 0.1)

1. 確率εでランダム行動を選択するか判定:
   - ランダム値 r ∈ [0,1) を生成
   - if r < ε:
       a = ランダムに選択(すべての行動)  # 探索
   - else:
       a = argmax_a' Q(s,a')  # 活用

2. 行動 a を実行
3. 報酬 r と次状態 s' を観測
4. Q値を更新
5. s ← s' として繰り返し
```

#### 図解（行動選択の流れ）
```
    状態 s
      ↓
┌─────────────────┐
│ ランダム値生成  │
│   r ∈ [0,1)     │
└─────────────────┘
      ↓
   r < ε ?
   /     \
 Yes      No
  ↓        ↓
探索      活用
ランダム   最大Q値
行動選択   行動選択
  ↓        ↓
    行動 a
```

#### εの設定パターン

**固定ε**:
- **例**: ε = 0.1（常に10%探索）
- **利点**: シンプル、実装が容易
- **欠点**: 学習後期も探索し続ける、最適方策への収束が遅い

**ε-減衰（ε-decay）**:
- **例**: $\epsilon_t = \epsilon_0 \cdot \text{decay}^t$ または $\epsilon_t = \max(\epsilon_{min}, \epsilon_0 - \Delta\epsilon \cdot t)$
- **典型値**: $\epsilon_0 = 1.0 \rightarrow \epsilon_{min} = 0.01$
- **利点**: 初期に十分探索、後期に活用を重視
- **用途**: DQNの標準的設定

**例（線形減衰）**:
```
初期: ε = 1.0（完全ランダム）
↓ エピソード進行
中期: ε = 0.3（30%探索）
↓ さらに進行
後期: ε = 0.01（1%探索、ほぼ活用）
```

#### 実例（数値シミュレーション）

**設定**:
- 4つの行動: A, B, C, D
- 現在のQ値: Q(s,A)=10, Q(s,B)=15, Q(s,C)=5, Q(s,D)=8
- 最良行動: B（Q値15）
- ε = 0.2（20%探索）

**行動選択の確率**:
- **活用（確率80%）**: 行動Bを選択
- **探索（確率20%）**: A, B, C, Dから均等にランダム選択（各5%）

**結果**:
- P(A) = 0.05（探索のみ）
- P(B) = 0.80 + 0.05 = 0.85（活用+探索）
- P(C) = 0.05（探索のみ）
- P(D) = 0.05（探索のみ）

→ 最良行動Bが高確率で選ばれるが、他の行動も試される

#### 他の探索戦略との比較

| 戦略 | 特徴 | 利点 | 欠点 |
|------|------|------|------|
| **ε-greedy** | 確率εでランダム、1-εで最良 | シンプル、実装容易 | 一様ランダムで非効率 |
| **Softmax（Boltzmann）** | Q値に応じた確率分布 | Q値を考慮した探索 | 温度パラメータ調整が難しい |
| **UCB（Upper Confidence Bound）** | 不確実性を考慮 | 理論的保証あり | Q学習では適用困難 |
| **Thompson Sampling** | ベイズ的サンプリング | 効率的探索 | 実装が複雑 |

### 試験での問われ方

#### 典型設問
- **「ε-greedy方策で、εは（探索の確率）を表す」**
- **「確率εで（ランダムな行動）、確率1-εで（最良の行動）を選択」**
- ε-greedy方策の定義
- 探索と活用のバランスの取り方
- εの値が学習に与える影響
- ε-減衰の目的

#### ひっかけポイントと違いの整理

**混同注意**:
- **ε-greedy vs Softmax**: ε-greedyは一様ランダム、SoftmaxはQ値に応じた確率
- **探索（Exploration） vs 活用（Exploitation）**: 探索は情報収集、活用は報酬最大化
- **εの大小**: ε大→探索重視、ε小→活用重視

**出題パターン**:
- 「εの値が大きいとき」→**探索が増える、ランダム性が高い**
- 「ε=0のとき」→**完全なグリーディ方策（常に最良行動）**
- 「ε=1のとき」→**完全なランダム方策（常に探索）**
- 「ε-減衰の目的」→**初期に探索、後期に活用を重視**
- 「ε-greedyの欠点」→**一様ランダムで非効率、劣悪な行動も同確率で試す**

**選択肢で出やすい対比**:
| 項目 | ε-greedy | Softmax | 純粋グリーディ |
|------|----------|---------|----------------|
| **探索方法** | 確率εでランダム | Q値に応じた確率 | 探索なし |
| **活用方法** | 最大Q値を選択 | 高Q値が高確率 | 常に最大Q値 |
| **パラメータ** | ε（探索確率） | 温度T | なし |
| **適用場面** | Q学習、DQN | 行動選択の洗練 | デプロイ時 |

### 補足

#### 実務観点

**用途**:
- Q学習の標準的探索戦略
- DQNでのエピソード開始時
- マルチアームバンディット問題
- A/Bテストの初期段階

**設定の目安**:
- **初期ε**: 0.5〜1.0（十分に探索）
- **最終ε**: 0.01〜0.05（少し探索を残す）
- **減衰期間**: 全エピソードの50〜80%

**実装のポイント**:
- ε-減衰の実装はエピソード数またはステップ数ベース
- テスト時はε=0（純粋な活用）
- ランダムシードの管理で再現性確保

**課題**:
- **一様ランダムの非効率性**: 明らかに悪い行動も試す
- **εの調整**: タスクに依存、試行錯誤が必要
- **連続動作空間**: 離散化が必要または他手法を使用

#### より高度な探索戦略

**ε-減衰の改良**:
- 指数減衰: $\epsilon_t = \epsilon_0 \cdot \gamma^t$
- 線形減衰: $\epsilon_t = \epsilon_0 - \frac{\epsilon_0 - \epsilon_{min}}{N} \cdot t$
- ステップ関数: 一定期間ごとに減少

**適応的探索**:
- Q値の分散が大きい状態で探索を増やす
- 訪問回数が少ない状態-行動を優先

#### 関連トピック
- [強化学習基礎](reinforcement_learning.md#強化学習の基礎概念) - MDP、Q学習
- [DQN](../06_deep_learning/neural_network_basics.md) - Deep Q-Network での使用例
- [Actor-Critic](reinforcement_learning.md#actor-critic手法) - 方策ベースでの探索

#### Policy Gradient
- **方策ベース**: 方策 $\pi_\theta(a|s)$ を直接学習
- **更新**: 勾配上昇法で期待報酬を最大化
- **特徴**: 確率的方策、連続動作対応

#### DQN（Deep Q-Network）
- **深層強化学習**: Q学習にニューラルネットワーク適用
- **工夫**: Experience Replay、Target Network
- **成果**: Atariゲームで人間超え

---

## 試験での問われ方（強化学習全般）

### 典型設問
- 強化学習の定義（報酬から学習）
- 教師あり学習・教師なし学習との違い
- エージェント・環境・状態・行動・報酬の役割
- 方策・価値関数・Q関数の意味
- 探索と活用のトレードオフ

### 引っ掛けポイント
- **強化学習 vs 教師あり学習**: 正解ラベルなし、報酬のみ
- **強化学習 vs 教師なし学習**: 目的（報酬最大化）あり、教師なしは構造発見
- **方策ベース vs 価値ベース**: 学習対象の違い

## 補足（強化学習全般）

### 実務課題
- **報酬設計**: 適切な報酬関数の設定が困難
- **サンプル効率**: 多数の試行が必要
- **安全性**: 探索中の危険な行動
- **シミュレーションギャップ**: 実環境での性能低下

---

## Sim-to-Real転移（シミュレーションから現実への転移）

### 要点
シミュレータで学習した方策を実世界に転移する手法。代表的手法は**ドメインランダマイゼーション**（シミュレータのパラメータをランダム化して多様な環境で訓練）。他に**ドメイン適応**（実データで微調整）、**システム同定**（実環境パラメータ推定）がある。

### 定義
Sim-to-Real転移とは、シミュレーション環境で学習した強化学習の方策を、実世界（現実環境）で動作させる技術。シミュレータは安全・高速・低コストで学習できるが、物理法則の近似や計測誤差により実環境との差（シミュレーションギャップ）が生じるため、この差を埋める工夫が必要。

### 重要キーワード
- **Sim-to-Real（シム・トゥ・リアル）**: シミュレーションから現実への転移
- **シミュレーションギャップ（Reality Gap）**: シミュレータと実環境の差
- **ドメインランダマイゼーション（Domain Randomization）**: シミュレータのパラメータをランダム化
- **ドメイン適応（Domain Adaptation）**: 実データで方策を微調整
- **システム同定（System Identification）**: 実環境の物理パラメータを推定
- **転移学習（Transfer Learning）**: 学習済み知識を新環境に適用
- **ロバスト性（Robustness）**: 環境変動への頑健性

### 詳細

#### 背景
**シミュレータ学習の利点**:
- **安全性**: 失敗しても実機が壊れない
- **高速性**: 並列実行・時間加速が可能
- **低コスト**: 実機実験より圧倒的に安価
- **データ量**: 大量のエピソードを効率的に生成

**シミュレーションギャップの原因**:
- **物理モデルの簡略化**: 摩擦・空気抵抗・弾性などの近似
- **センサノイズの違い**: 理想的センサ vs 実センサの誤差
- **アクチュエータ特性**: モータの遅延・非線形性
- **外乱**: 風・振動などシミュレータで再現困難な要因

→ シミュレータで完璧に動いても、実環境では失敗する可能性

#### 主要な転移手法

**1. ドメインランダマイゼーション（最も代表的）**

**概念**:
シミュレータの物理パラメータ（摩擦係数、質量、重力、カメラ位置、照明等）を訓練中にランダムに変動させる。多様な環境で学習することで、実環境の条件もその変動範囲内に含まれ、ロバストな方策を獲得。

**アルゴリズム**:
```
各エピソード開始時:
1. パラメータをランダムにサンプリング:
   - 摩擦係数: μ ∈ [0.3, 0.7]
   - 物体質量: m ∈ [0.8, 1.2] kg
   - カメラ視点: θ ∈ [-15°, 15°]
   - 照明強度: I ∈ [50, 200] lux
2. サンプルしたパラメータでシミュレータを設定
3. そのシミュレータで1エピソード実行・学習
4. 繰り返し
```

**利点**:
- 実データ不要（シミュレータのみで完結）
- 実装が比較的容易
- OpenAI、Google等で成功事例多数

**欠点**:
- ランダム化範囲の設定が難しい
- 過度なランダム化で学習が困難化
- すべての差を吸収できない場合もある

**実例**:
- **OpenAI Dactyl**: ロボットハンドでルービックキューブ操作（視覚・物理パラメータをランダム化）
- **Google Grasp**: 物体把持タスク（物体形状・テクスチャをランダム化）

**2. ドメイン適応（Domain Adaptation）**

**概念**:
シミュレータで事前学習した方策を、少量の実データでファインチューニング。シミュレータで大まかな挙動を学び、実環境で最終調整。

**手順**:
```
1. シミュレータで方策 π_sim を学習
2. 実環境で少数エピソード（10〜100回程度）実行
3. 実データで π_sim を微調整 → π_real
4. π_real を実環境にデプロイ
```

**利点**:
- シミュレータで大部分を学習（効率的）
- 実データで最終調整（精度向上）

**欠点**:
- 実環境でのデータ収集が必要
- 初期方策が悪いと危険な場合も

**3. システム同定（System Identification）**

**概念**:
実環境から物理パラメータを推定し、シミュレータをより現実に近づける。実測データとシミュレーション結果を比較してパラメータを最適化。

**手順**:
```
1. 実環境で動作を記録（軌跡・力・速度等）
2. シミュレータで同じ動作を再現
3. 実測とシミュレーション結果の誤差を最小化:
   θ* = argmin_θ ||y_real - y_sim(θ)||²
4. 推定パラメータ θ* でシミュレータを更新
5. 更新されたシミュレータで方策を学習
```

**利点**:
- シミュレータの精度向上
- 理論的に明確

**欠点**:
- すべてのパラメータを同定できない
- 計測コストが高い

**4. その他の手法**

- **プログレッシブネット**: 複数タスクの知識を段階的に転移
- **メタ学習**: 環境適応能力自体を学習
- **実環境教師あり学習**: 実環境の状態-行動ペアを教師データとして使用

#### 図解（ドメインランダマイゼーション）
```
シミュレータ（パラメータランダム化）
┌─────────────────────────────────┐
│ エピソード1: 摩擦0.3, 質量0.9kg │ → 学習
│ エピソード2: 摩擦0.6, 質量1.1kg │ → 学習
│ エピソード3: 摩擦0.4, 質量1.0kg │ → 学習
│         ...多様な条件...        │
└─────────────────────────────────┘
           ↓
    ロバストな方策 π
           ↓
      実環境にデプロイ
   （条件は固定だが、訓練済み
    範囲内なので動作可能）
```

#### 実例（数値・具体例）

**OpenAI Dactyl（ロボットハンドのルービックキューブ操作）**:
- **シミュレータ**: MuJoCo物理エンジン
- **ランダム化項目**: 
  - 物体サイズ: ±10%
  - 摩擦係数: 0.7〜1.3
  - 質量: ±20%
  - カメラ位置・角度: ±5度
  - 照明・テクスチャ
- **結果**: 50回連続成功（実環境で追加学習なし）

**比較（手法別の実データ必要量）**:
| 手法 | シミュレータデータ | 実データ | 転移成功率 |
|------|-------------------|---------|-----------|
| **ドメインランダマイゼーション** | 大量（100万〜） | 不要 | 高 |
| **ドメイン適応** | 大量 | 少量（数百） | 非常に高 |
| **システム同定** | 中量 | 中量（数千） | 中〜高 |
| **実環境のみ学習** | なし | 大量（数万〜） | 最高 |

### 試験での問われ方

#### 典型設問
- **「シミュレータで学習した方策を実世界に転移する手法」→ ドメインランダマイゼーション**
- **「シミュレータのパラメータをランダム化して訓練」→ ドメインランダマイゼーション**
- **「シミュレーションギャップを埋める手法」→ ドメインランダマイゼーション、ドメイン適応、システム同定**
- Sim-to-Real転移の目的
- ドメインランダマイゼーションの利点・欠点
- 実データ不要な手法の特定

#### ひっかけポイントと違いの整理

**混同注意**:
- **ドメインランダマイゼーション vs ドメイン適応**: 前者は実データ不要、後者は実データで微調整
- **転移学習 vs Sim-to-Real**: 転移学習は広い概念、Sim-to-Realはその一種（特にシミュ→実環境）
- **システム同定 vs ドメインランダマイゼーション**: システム同定はシミュレータを実環境に合わせる、ランダマイゼーションは逆に多様化

**出題パターン**:
- 「実データなしで転移」→**ドメインランダマイゼーション**
- 「シミュレータのパラメータを推定」→**システム同定**
- 「少量の実データで微調整」→**ドメイン適応**
- 「シミュレーションギャップの原因」→**物理モデルの簡略化、センサノイズ、外乱**
- 「ロバスト性向上」→**ドメインランダマイゼーション**

**選択肢で出やすい対比**:
| 項目 | ドメインランダマイゼーション | ドメイン適応 | システム同定 |
|------|---------------------------|-------------|-------------|
| **実データ** | 不要 | 少量必要 | 中量必要 |
| **シミュレータ設定** | ランダム化 | 固定 | 推定値で更新 |
| **実装難易度** | 低〜中 | 低 | 高 |
| **代表例** | OpenAI Dactyl | ファインチューニング | 物理パラメータ推定 |
| **適用場面** | 実データ収集困難 | 初期方策あり | 高精度要求 |

### 補足

#### 実務観点

**用途**:
- **ロボット制御**: 把持・歩行・マニピュレーション
- **自動運転**: シミュレータで事前学習
- **ドローン制御**: 安全な訓練環境
- **製造業**: 組立・検査ロボット

**成功の鍵**:
- **ランダム化範囲の調整**: 広すぎず狭すぎず
- **重要パラメータの特定**: 影響大きい要素を優先
- **段階的転移**: シミュレータ → 簡易実環境 → 本番環境
- **安全機構**: 実環境での予期せぬ動作への対策

**課題**:
- **視覚情報の転移困難**: CGと実画像の差（Photorealistic Rendering等で対応）
- **接触・摩擦の再現困難**: 物理シミュレーションの限界
- **長期的挙動の予測誤差**: 微小な誤差の蓄積
- **計算コスト**: 大量のランダム化シミュレーション

#### 実装のポイント

**ドメインランダマイゼーションの設定例**:
```python
# 各エピソードでランダムにパラメータ設定
sim.set_friction(uniform(0.3, 0.7))
sim.set_mass(uniform(0.8, 1.2))
sim.set_gravity(uniform(9.6, 10.0))
sim.set_camera_pos(uniform(-0.1, 0.1), uniform(-0.1, 0.1))
sim.set_lighting(uniform(0.5, 1.5))
```

**段階的な転移戦略**:
1. **Phase 1**: シミュレータでベース方策学習
2. **Phase 2**: ドメインランダマイゼーションでロバスト化
3. **Phase 3**: 簡易実環境（安全な場所）でテスト
4. **Phase 4**: 必要に応じてドメイン適応
5. **Phase 5**: 本番環境デプロイ

#### 関連トピック
- [強化学習基礎](reinforcement_learning.md#強化学習の基礎概念) - 方策学習の基本
- [転移学習](../05_machine_learning/supervised_learning.md) - より広い転移学習の文脈
- [ロボット制御](../07_ai_applications/) - 実応用例

---

### 最新動向
- **モデルベース強化学習**: 環境モデルを学習
- **マルチエージェント**: 複数エージェントの協調・競争
- **逆強化学習**: 専門家の行動から報酬関数を推定
- **オフライン強化学習**: 事前収集データのみで学習
- **Sim-to-Real転移**: シミュレータから実環境への方策転移（ドメインランダマイゼーション等）
