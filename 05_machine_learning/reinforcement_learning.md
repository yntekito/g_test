# 強化学習（Reinforcement Learning）

## 要点（試験用）
- 試行錯誤を通じて報酬を最大化する行動（方策）を学習する機械学習パラダイム。教師あり学習と異なり、正解は与えられず報酬シグナルから学習。
- 主要要素：エージェント、環境、状態、行動、報酬、方策。マルコフ決定過程（MDP）でモデル化。
- 代表手法：Q学習（価値ベース）、Policy Gradient（方策ベース）、Actor-Critic（両者の組み合わせ）、DQN（深層強化学習）。

## 定義
強化学習とは、エージェントが環境と相互作用しながら、試行錯誤を通じて累積報酬を最大化する行動方策を学習する機械学習の枠組み。正解ラベルではなく、報酬（スカラー値）というフィードバック信号を使って学習する。

## 重要キーワード
- **エージェント（Agent）**: 学習・行動する主体
- **環境（Environment）**: エージェントが相互作用する対象
- **状態（State）**: 環境の現在の状況
- **行動（Action）**: エージェントが選択する動作
- **報酬（Reward）**: 行動の結果得られるスカラー値のフィードバック
- **方策（Policy）**: 状態から行動への写像 $\pi: S \rightarrow A$
- **価値関数（Value Function）**: 状態や行動の長期的な良さ
- **Q関数**: 状態-行動ペアの価値 $Q(s,a)$
- **マルコフ決定過程（MDP）**: 強化学習の数学的枠組み

---

## Actor-Critic手法

### 要点（試験用）
Actor-Criticは方策ベースと価値ベースを組み合わせた強化学習手法。Actorが方策（行動選択）を学習し、Criticが価値関数（行動評価）を学習。両者が協調して学習効率と安定性を向上。A3C、PPO等の発展手法がある。

### 定義
Actor-Critic手法とは、Actor（方策を学習するネットワーク）とCritic（価値関数を学習するネットワーク）の2つのモジュールを持つ強化学習アルゴリズム。Actorが行動を選択し、Criticがその行動の良さ（価値）を評価してActorにフィードバックすることで学習を進める。

### 重要キーワード
- **Actor（行動者）**: 方策 $\pi(a|s)$ を学習し、状態に応じて行動を選択
- **Critic（評価者）**: 価値関数 $V(s)$ または $Q(s,a)$ を学習し、行動を評価
- **方策ベース**: 方策を直接学習（例：Policy Gradient）
- **価値ベース**: 価値関数を学習して行動選択（例：Q学習、DQN）
- **TD誤差（Temporal Difference Error）**: Criticが計算する予測誤差、Actorの学習信号
- **Advantage関数**: $A(s,a) = Q(s,a) - V(s)$、基準からの優位性
- **A3C（Asynchronous Advantage Actor-Critic）**: 並列学習版
- **PPO（Proximal Policy Optimization）**: 安定性向上版

### 詳細（教科書風）

#### 背景
強化学習には大きく2つのアプローチがある：
- **価値ベース**（Q学習、DQN）: 価値関数を学習し、最大価値の行動を選択。決定的だが探索が難しい
- **方策ベース**（Policy Gradient）: 方策を直接学習。確率的だが分散が大きく学習が不安定

Actor-Criticは両者の利点を組み合わせ、安定性と効率を両立。

#### 基本構造

**Actor（行動者）**:
- **役割**: 方策 $\pi_\theta(a|s)$ を学習し、行動を選択
- **入力**: 状態 $s$
- **出力**: 行動の確率分布または行動そのもの
- **学習**: Criticからの評価を使って方策を改善

**Critic（評価者）**:
- **役割**: 価値関数 $V_\phi(s)$ を学習し、状態・行動を評価
- **入力**: 状態 $s$（または状態-行動ペア $(s,a)$）
- **出力**: 価値の推定値
- **学習**: TD誤差を最小化

#### アルゴリズムの流れ
```
1. 初期状態 s を観測
2. Actor: 方策 π(a|s) に従って行動 a を選択
3. 環境: 行動を実行し、報酬 r と次状態 s' を返す
4. Critic: TD誤差を計算
   δ = r + γV(s') - V(s)
5. Critic: 価値関数を更新（TD誤差を減らす）
   V(s) ← V(s) + α_c・δ
6. Actor: 方策を更新（TD誤差を利用）
   θ ← θ + α_a・δ・∇log π(a|s)
7. s ← s' として繰り返し
```

#### 図解（構造）
```
        環境
         ↓ 状態 s
    ┌────────────┐
    │   Actor    │ ← 方策パラメータ θ
    │ π(a|s;θ)   │ (行動を選択)
    └────────────┘
         ↓ 行動 a
        環境
         ↓ 報酬 r, 次状態 s'
    ┌────────────┐
    │   Critic   │ ← 価値パラメータ φ
    │  V(s;φ)    │ (行動を評価)
    └────────────┘
         ↓ TD誤差 δ
    Actor更新 ← δ
```

#### 主要な変種

**Advantage Actor-Critic（A2C）**:
- **Advantage関数**: $A(s,a) = Q(s,a) - V(s)$
- **利点**: 基準値からの相対的な良さを評価、分散削減
- **更新**: $\theta \leftarrow \theta + \alpha \cdot A(s,a) \cdot \nabla \log \pi(a|s)$

**A3C（Asynchronous Advantage Actor-Critic）**:
- **特徴**: 複数の並列エージェントで非同期学習
- **利点**: 学習の多様性、高速化
- **DeepMind**: Atariゲームで高性能

**PPO（Proximal Policy Optimization）**:
- **特徴**: 方策の大幅な変更を制限（Clipping）
- **利点**: 学習の安定性向上
- **用途**: ロボット制御、OpenAI Five等

### 試験での問われ方

#### 典型設問
- **「Actorが（行動を選択）し、Criticが（行動を評価）する」**
- **「Actorが（方策を学習）し、Criticが（価値関数を学習）する」**
- Actor-Criticの構成要素と役割
- 価値ベース・方策ベース・Actor-Criticの違い
- TD誤差の役割
- Advantage関数の意味

#### ひっかけポイントと違いの整理

| 項目 | Actor-Critic | Q学習/DQN | Policy Gradient |
|------|--------------|-----------|-----------------|
| **分類** | 方策+価値ベース | 価値ベース | 方策ベース |
| **学習対象** | 方策と価値関数 | 価値関数のみ | 方策のみ |
| **行動選択** | 方策から確率的 | 価値最大を決定的 | 方策から確率的 |
| **評価方法** | Criticが評価 | Q値で評価 | モンテカルロ報酬 |
| **安定性** | 中〜高 | 高 | 低（分散大） |
| **探索** | 方策に組み込み | ε-greedy等必要 | 自然に探索 |

**混同注意**:
- **Actor-Critic vs Q学習**: Actor-Criticは方策を直接学習、Q学習は価値から行動を導出
- **Actor-Critic vs Policy Gradient**: Actor-CriticはCriticで評価、Policy Gradientはモンテカルロ報酬
- **A2C vs A3C**: A2Cは同期学習、A3Cは非同期並列学習

**出題パターン**:
- 「Actorの役割」→**行動選択、方策学習**
- 「Criticの役割」→**行動評価、価値関数学習**
- 「TD誤差の使用目的」→**CriticとActorの両方を更新**
- 「方策ベースと価値ベースの組み合わせ」→**Actor-Critic**

### 補足

#### 実務観点
**用途**: 
- ロボット制御（連続動作空間）
- ゲームAI（Atari、囲碁、DOTA2）
- 自動運転（経路計画）
- 資源最適化

**利点**:
- 連続動作空間に対応
- 確率的方策で探索が自然
- 価値ベースより柔軟

**課題**:
- ハイパーパラメータ調整が難しい
- 学習の安定性（PPOで改善）
- 計算コスト（2つのネットワーク）

#### 実装のポイント
- **ネットワーク構成**: ActorとCriticで重みを共有することも可能
- **学習率**: Actor $\alpha_a$ とCritic $\alpha_c$ を別々に設定
- **報酬の正規化**: 学習の安定化に重要
- **ベースライン**: Criticの価値推定を基準に使用

#### 関連トピック
- [教師あり学習](supervised_learning.md) - 学習パラダイムの違い
- [ニューラルネットワーク](../06_deep_learning/neural_network_basics.md) - Deep Actor-Criticの基礎

---

## 強化学習の基礎概念

### マルコフ決定過程（MDP）
- **構成要素**: 状態集合 $S$、行動集合 $A$、遷移確率 $P(s'|s,a)$、報酬関数 $R(s,a)$、割引率 $\gamma$
- **マルコフ性**: 次状態は現在の状態と行動のみに依存
- **目的**: 累積報酬の期待値 $\mathbb{E}[\sum_{t=0}^\infty \gamma^t R_t]$ を最大化

### 主要な学習手法

#### Q学習
- **価値ベース**: Q関数 $Q(s,a)$ を学習
- **更新式**: $Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$
- **特徴**: オフポリシー学習、離散行動空間

#### Policy Gradient
- **方策ベース**: 方策 $\pi_\theta(a|s)$ を直接学習
- **更新**: 勾配上昇法で期待報酬を最大化
- **特徴**: 確率的方策、連続動作対応

#### DQN（Deep Q-Network）
- **深層強化学習**: Q学習にニューラルネットワーク適用
- **工夫**: Experience Replay、Target Network
- **成果**: Atariゲームで人間超え

---

## 試験での問われ方（強化学習全般）

### 典型設問
- 強化学習の定義（報酬から学習）
- 教師あり学習・教師なし学習との違い
- エージェント・環境・状態・行動・報酬の役割
- 方策・価値関数・Q関数の意味
- 探索と活用のトレードオフ

### 引っ掛けポイント
- **強化学習 vs 教師あり学習**: 正解ラベルなし、報酬のみ
- **強化学習 vs 教師なし学習**: 目的（報酬最大化）あり、教師なしは構造発見
- **方策ベース vs 価値ベース**: 学習対象の違い

## 補足（強化学習全般）

### 実務課題
- **報酬設計**: 適切な報酬関数の設定が困難
- **サンプル効率**: 多数の試行が必要
- **安全性**: 探索中の危険な行動
- **シミュレーションギャップ**: 実環境での性能低下

### 最新動向
- **モデルベース強化学習**: 環境モデルを学習
- **マルチエージェント**: 複数エージェントの協調・競争
- **逆強化学習**: 専門家の行動から報酬関数を推定
- **オフライン強化学習**: 事前収集データのみで学習
