# 強化学習（Reinforcement Learning）

## 要点
- 試行錯誤を通じて報酬を最大化する行動（方策）を学習する機械学習パラダイム。教師あり学習と異なり、正解は与えられず報酬シグナルから学習。
- 主要要素：エージェント、環境、状態、行動、報酬、方策。マルコフ決定過程（MDP）でモデル化。
- 代表手法：Q学習（価値ベース）、Policy Gradient（方策ベース）、Actor-Critic（両者の組み合わせ）、DQN（深層強化学習）。

## 定義
強化学習とは、エージェントが環境と相互作用しながら、試行錯誤を通じて累積報酬を最大化する行動方策を学習する機械学習の枠組み。正解ラベルではなく、報酬（スカラー値）というフィードバック信号を使って学習する。

## 重要キーワード
- **エージェント（Agent）**: 学習・行動する主体
- **環境（Environment）**: エージェントが相互作用する対象
- **状態（State）**: 環境の現在の状況
- **行動（Action）**: エージェントが選択する動作
- **報酬（Reward）**: 行動の結果得られるスカラー値のフィードバック
- **方策（Policy）**: 状態から行動への写像 $\pi: S \rightarrow A$
- **価値関数（Value Function）**: 状態や行動の長期的な良さ
- **Q関数**: 状態-行動ペアの価値 $Q(s,a)$
- **マルコフ決定過程（MDP）**: 強化学習の数学的枠組み

---

## Actor-Critic手法

### 要点
Actor-Criticは方策ベースと価値ベースを組み合わせた強化学習手法。Actorが方策（行動選択）を学習し、Criticが価値関数（行動評価）を学習。両者が協調して学習効率と安定性を向上。A3C、PPO等の発展手法がある。

### 定義
Actor-Critic手法とは、Actor（方策を学習するネットワーク）とCritic（価値関数を学習するネットワーク）の2つのモジュールを持つ強化学習アルゴリズム。Actorが行動を選択し、Criticがその行動の良さ（価値）を評価してActorにフィードバックすることで学習を進める。

### 重要キーワード
- **Actor（行動者）**: 方策 $\pi(a|s)$ を学習し、状態に応じて行動を選択
- **Critic（評価者）**: 価値関数 $V(s)$ または $Q(s,a)$ を学習し、行動を評価
- **方策ベース**: 方策を直接学習（例：Policy Gradient）
- **価値ベース**: 価値関数を学習して行動選択（例：Q学習、DQN）
- **TD誤差（Temporal Difference Error）**: Criticが計算する予測誤差、Actorの学習信号
- **Advantage関数**: $A(s,a) = Q(s,a) - V(s)$、基準からの優位性
- **A3C（Asynchronous Advantage Actor-Critic）**: 並列学習版
- **PPO（Proximal Policy Optimization）**: 安定性向上版

### 詳細

#### 背景
強化学習には大きく2つのアプローチがある：
- **価値ベース**（Q学習、DQN）: 価値関数を学習し、最大価値の行動を選択。決定的だが探索が難しい
- **方策ベース**（Policy Gradient）: 方策を直接学習。確率的だが分散が大きく学習が不安定

Actor-Criticは両者の利点を組み合わせ、安定性と効率を両立。

#### 基本構造

**Actor（行動者）**:
- **役割**: 方策 $\pi_\theta(a|s)$ を学習し、行動を選択
- **入力**: 状態 $s$
- **出力**: 行動の確率分布または行動そのもの
- **学習**: Criticからの評価を使って方策を改善

**Critic（評価者）**:
- **役割**: 価値関数 $V_\phi(s)$ を学習し、状態・行動を評価
- **入力**: 状態 $s$（または状態-行動ペア $(s,a)$）
- **出力**: 価値の推定値
- **学習**: TD誤差を最小化

#### アルゴリズムの流れ
```
1. 初期状態 s を観測
2. Actor: 方策 π(a|s) に従って行動 a を選択
3. 環境: 行動を実行し、報酬 r と次状態 s' を返す
4. Critic: TD誤差を計算
   δ = r + γV(s') - V(s)
5. Critic: 価値関数を更新（TD誤差を減らす）
   V(s) ← V(s) + α_c・δ
6. Actor: 方策を更新（TD誤差を利用）
   θ ← θ + α_a・δ・∇log π(a|s)
7. s ← s' として繰り返し
```

#### 図解（構造）
```
        環境
         ↓ 状態 s
    ┌────────────┐
    │   Actor    │ ← 方策パラメータ θ
    │ π(a|s;θ)   │ (行動を選択)
    └────────────┘
         ↓ 行動 a
        環境
         ↓ 報酬 r, 次状態 s'
    ┌────────────┐
    │   Critic   │ ← 価値パラメータ φ
    │  V(s;φ)    │ (行動を評価)
    └────────────┘
         ↓ TD誤差 δ
    Actor更新 ← δ
```

#### 主要な変種

**Advantage Actor-Critic（A2C）**:
- **Advantage関数**: $A(s,a) = Q(s,a) - V(s)$
- **利点**: 基準値からの相対的な良さを評価、分散削減
- **更新**: $\theta \leftarrow \theta + \alpha \cdot A(s,a) \cdot \nabla \log \pi(a|s)$

**A3C（Asynchronous Advantage Actor-Critic）**:
- **特徴**: 複数の並列エージェントで非同期学習
- **利点**: 学習の多様性、高速化
- **DeepMind**: Atariゲームで高性能

**PPO（Proximal Policy Optimization）**:
- **特徴**: 方策の大幅な変更を制限（Clipping）
- **利点**: 学習の安定性向上
- **用途**: ロボット制御、OpenAI Five等

### 試験での問われ方

#### 典型設問
- **「Actorが（行動を選択）し、Criticが（行動を評価）する」**
- **「Actorが（方策を学習）し、Criticが（価値関数を学習）する」**
- Actor-Criticの構成要素と役割
- 価値ベース・方策ベース・Actor-Criticの違い
- TD誤差の役割
- Advantage関数の意味

#### ひっかけポイントと違いの整理

| 項目 | Actor-Critic | Q学習/DQN | Policy Gradient |
|------|--------------|-----------|-----------------|
| **分類** | 方策+価値ベース | 価値ベース | 方策ベース |
| **学習対象** | 方策と価値関数 | 価値関数のみ | 方策のみ |
| **行動選択** | 方策から確率的 | 価値最大を決定的 | 方策から確率的 |
| **評価方法** | Criticが評価 | Q値で評価 | モンテカルロ報酬 |
| **安定性** | 中〜高 | 高 | 低（分散大） |
| **探索** | 方策に組み込み | ε-greedy等必要 | 自然に探索 |

**混同注意**:
- **Actor-Critic vs Q学習**: Actor-Criticは方策を直接学習、Q学習は価値から行動を導出
- **Actor-Critic vs Policy Gradient**: Actor-CriticはCriticで評価、Policy Gradientはモンテカルロ報酬
- **A2C vs A3C**: A2Cは同期学習、A3Cは非同期並列学習

**出題パターン**:
- 「Actorの役割」→**行動選択、方策学習**
- 「Criticの役割」→**行動評価、価値関数学習**
- 「TD誤差の使用目的」→**CriticとActorの両方を更新**
- 「方策ベースと価値ベースの組み合わせ」→**Actor-Critic**

### 補足

#### 実務観点
**用途**: 
- ロボット制御（連続動作空間）
- ゲームAI（Atari、囲碁、DOTA2）
- 自動運転（経路計画）
- 資源最適化

**利点**:
- 連続動作空間に対応
- 確率的方策で探索が自然
- 価値ベースより柔軟

**課題**:
- ハイパーパラメータ調整が難しい
- 学習の安定性（PPOで改善）
- 計算コスト（2つのネットワーク）

#### 実装のポイント
- **ネットワーク構成**: ActorとCriticで重みを共有することも可能
- **学習率**: Actor $\alpha_a$ とCritic $\alpha_c$ を別々に設定
- **報酬の正規化**: 学習の安定化に重要
- **ベースライン**: Criticの価値推定を基準に使用

#### 関連トピック
- [教師あり学習](supervised_learning.md) - 学習パラダイムの違い
- [ニューラルネットワーク](../06_deep_learning/neural_network_basics.md) - Deep Actor-Criticの基礎

---

## 強化学習の基礎概念

### マルコフ決定過程（MDP）
- **構成要素**: 状態集合 $S$、行動集合 $A$、遷移確率 $P(s'|s,a)$、報酬関数 $R(s,a)$、割引率 $\gamma$
- **マルコフ性**: 次状態は現在の状態と行動のみに依存
- **目的**: 累積報酬の期待値 $\mathbb{E}[\sum_{t=0}^\infty \gamma^t R_t]$ を最大化

### 主要な学習手法

#### Q学習
- **価値ベース**: Q関数 $Q(s,a)$ を学習
- **更新式**: $Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$
- **特徴**: オフポリシー学習、離散行動空間

---

## ε-greedy方策

### 要点
ε-greedy（イプシロン・グリーディ）方策は、探索（Exploration）と活用（Exploitation）のバランスを取る最も基本的な行動選択戦略。確率 $\epsilon$ でランダムな行動（探索）、確率 $1-\epsilon$ で現在最良の行動（活用）を選択する。Q学習やDQNで標準的に使用される。

### 定義
ε-greedy方策とは、強化学習における行動選択戦略の1つで、以下のルールに従う：
- **確率 $\epsilon$（通常0.1〜0.3）**: すべての行動からランダムに選択（探索）
- **確率 $1-\epsilon$**: 現在のQ値が最大の行動を選択（活用）

数式表現：
$$
a = \begin{cases}
\arg\max_{a'} Q(s,a') & \text{確率 } 1-\epsilon \text{ （活用）} \\
\text{random}(A) & \text{確率 } \epsilon \text{ （探索）}
\end{cases}
$$

### 重要キーワード
- **探索（Exploration）**: 未知の行動を試して新しい情報を得る
- **活用（Exploitation）**: 現在の知識で最良の行動を選ぶ
- **ε（イプシロン）**: ランダム行動の確率（0≤ε≤1）
- **グリーディ（Greedy）**: 常に最良の行動を選ぶ戦略
- **ε-減衰（ε-decay）**: 学習進行に応じてεを減少させる手法
- **探索と活用のトレードオフ**: 探索しすぎると報酬が低く、活用しすぎると局所解に陥る

### 詳細

#### 背景
強化学習では、未知の環境で最適な方策を見つける必要がある。この過程で2つの相反する要求が生じる：

- **探索（Exploration）**: 未経験の行動を試して環境の情報を収集。長期的には必要だが短期的には報酬が低い
- **活用（Exploitation）**: 既知の最良行動を選んで報酬を最大化。短期的には有効だが新しい発見がない

**探索と活用のジレンマ**:
- 探索しすぎ → 報酬が低い、学習が非効率
- 活用しすぎ → 局所最適解に陥る、より良い行動を見逃す

ε-greedy方策は、このトレードオフを単純かつ効果的にバランスさせる。

#### 基本アルゴリズム
```
現在の状態: s
Q値テーブル: Q(s,a) (すべての状態-行動ペア)
パラメータ: ε (例: 0.1)

1. 確率εでランダム行動を選択するか判定:
   - ランダム値 r ∈ [0,1) を生成
   - if r < ε:
       a = ランダムに選択(すべての行動)  # 探索
   - else:
       a = argmax_a' Q(s,a')  # 活用

2. 行動 a を実行
3. 報酬 r と次状態 s' を観測
4. Q値を更新
5. s ← s' として繰り返し
```

#### 図解（行動選択の流れ）
```
    状態 s
      ↓
┌─────────────────┐
│ ランダム値生成  │
│   r ∈ [0,1)     │
└─────────────────┘
      ↓
   r < ε ?
   /     \
 Yes      No
  ↓        ↓
探索      活用
ランダム   最大Q値
行動選択   行動選択
  ↓        ↓
    行動 a
```

#### εの設定パターン

**固定ε**:
- **例**: ε = 0.1（常に10%探索）
- **利点**: シンプル、実装が容易
- **欠点**: 学習後期も探索し続ける、最適方策への収束が遅い

**ε-減衰（ε-decay）**:
- **例**: $\epsilon_t = \epsilon_0 \cdot \text{decay}^t$ または $\epsilon_t = \max(\epsilon_{min}, \epsilon_0 - \Delta\epsilon \cdot t)$
- **典型値**: $\epsilon_0 = 1.0 \rightarrow \epsilon_{min} = 0.01$
- **利点**: 初期に十分探索、後期に活用を重視
- **用途**: DQNの標準的設定

**例（線形減衰）**:
```
初期: ε = 1.0（完全ランダム）
↓ エピソード進行
中期: ε = 0.3（30%探索）
↓ さらに進行
後期: ε = 0.01（1%探索、ほぼ活用）
```

#### 実例（数値シミュレーション）

**設定**:
- 4つの行動: A, B, C, D
- 現在のQ値: Q(s,A)=10, Q(s,B)=15, Q(s,C)=5, Q(s,D)=8
- 最良行動: B（Q値15）
- ε = 0.2（20%探索）

**行動選択の確率**:
- **活用（確率80%）**: 行動Bを選択
- **探索（確率20%）**: A, B, C, Dから均等にランダム選択（各5%）

**結果**:
- P(A) = 0.05（探索のみ）
- P(B) = 0.80 + 0.05 = 0.85（活用+探索）
- P(C) = 0.05（探索のみ）
- P(D) = 0.05（探索のみ）

→ 最良行動Bが高確率で選ばれるが、他の行動も試される

#### 他の探索戦略との比較

| 戦略 | 特徴 | 利点 | 欠点 |
|------|------|------|------|
| **ε-greedy** | 確率εでランダム、1-εで最良 | シンプル、実装容易 | 一様ランダムで非効率 |
| **Softmax（Boltzmann）** | Q値に応じた確率分布 | Q値を考慮した探索 | 温度パラメータ調整が難しい |
| **UCB（Upper Confidence Bound）** | 不確実性を考慮 | 理論的保証あり | Q学習では適用困難 |
| **Thompson Sampling** | ベイズ的サンプリング | 効率的探索 | 実装が複雑 |

### 試験での問われ方

#### 典型設問
- **「ε-greedy方策で、εは（探索の確率）を表す」**
- **「確率εで（ランダムな行動）、確率1-εで（最良の行動）を選択」**
- ε-greedy方策の定義
- 探索と活用のバランスの取り方
- εの値が学習に与える影響
- ε-減衰の目的

#### ひっかけポイントと違いの整理

**混同注意**:
- **ε-greedy vs Softmax**: ε-greedyは一様ランダム、SoftmaxはQ値に応じた確率
- **探索（Exploration） vs 活用（Exploitation）**: 探索は情報収集、活用は報酬最大化
- **εの大小**: ε大→探索重視、ε小→活用重視

**出題パターン**:
- 「εの値が大きいとき」→**探索が増える、ランダム性が高い**
- 「ε=0のとき」→**完全なグリーディ方策（常に最良行動）**
- 「ε=1のとき」→**完全なランダム方策（常に探索）**
- 「ε-減衰の目的」→**初期に探索、後期に活用を重視**
- 「ε-greedyの欠点」→**一様ランダムで非効率、劣悪な行動も同確率で試す**

**選択肢で出やすい対比**:
| 項目 | ε-greedy | Softmax | 純粋グリーディ |
|------|----------|---------|----------------|
| **探索方法** | 確率εでランダム | Q値に応じた確率 | 探索なし |
| **活用方法** | 最大Q値を選択 | 高Q値が高確率 | 常に最大Q値 |
| **パラメータ** | ε（探索確率） | 温度T | なし |
| **適用場面** | Q学習、DQN | 行動選択の洗練 | デプロイ時 |

### 補足

#### 実務観点

**用途**:
- Q学習の標準的探索戦略
- DQNでのエピソード開始時
- マルチアームバンディット問題
- A/Bテストの初期段階

**設定の目安**:
- **初期ε**: 0.5〜1.0（十分に探索）
- **最終ε**: 0.01〜0.05（少し探索を残す）
- **減衰期間**: 全エピソードの50〜80%

**実装のポイント**:
- ε-減衰の実装はエピソード数またはステップ数ベース
- テスト時はε=0（純粋な活用）
- ランダムシードの管理で再現性確保

**課題**:
- **一様ランダムの非効率性**: 明らかに悪い行動も試す
- **εの調整**: タスクに依存、試行錯誤が必要
- **連続動作空間**: 離散化が必要または他手法を使用

#### より高度な探索戦略

**ε-減衰の改良**:
- 指数減衰: $\epsilon_t = \epsilon_0 \cdot \gamma^t$
- 線形減衰: $\epsilon_t = \epsilon_0 - \frac{\epsilon_0 - \epsilon_{min}}{N} \cdot t$
- ステップ関数: 一定期間ごとに減少

**適応的探索**:
- Q値の分散が大きい状態で探索を増やす
- 訪問回数が少ない状態-行動を優先

#### 関連トピック
- [強化学習基礎](reinforcement_learning.md#強化学習の基礎概念) - MDP、Q学習
- [DQN](../06_deep_learning/neural_network_basics.md) - Deep Q-Network での使用例
- [Actor-Critic](reinforcement_learning.md#actor-critic手法) - 方策ベースでの探索

#### Policy Gradient
- **方策ベース**: 方策 $\pi_\theta(a|s)$ を直接学習
- **更新**: 勾配上昇法で期待報酬を最大化
- **特徴**: 確率的方策、連続動作対応

#### DQN（Deep Q-Network）
- **深層強化学習**: Q学習にニューラルネットワーク適用
- **工夫**: Experience Replay、Target Network
- **成果**: Atariゲームで人間超え

---

## 試験での問われ方（強化学習全般）

### 典型設問
- 強化学習の定義（報酬から学習）
- 教師あり学習・教師なし学習との違い
- エージェント・環境・状態・行動・報酬の役割
- 方策・価値関数・Q関数の意味
- 探索と活用のトレードオフ

### 引っ掛けポイント
- **強化学習 vs 教師あり学習**: 正解ラベルなし、報酬のみ
- **強化学習 vs 教師なし学習**: 目的（報酬最大化）あり、教師なしは構造発見
- **方策ベース vs 価値ベース**: 学習対象の違い

## 補足（強化学習全般）

### 実務課題
- **報酬設計**: 適切な報酬関数の設定が困難
- **サンプル効率**: 多数の試行が必要
- **安全性**: 探索中の危険な行動
- **シミュレーションギャップ**: 実環境での性能低下

### 最新動向
- **モデルベース強化学習**: 環境モデルを学習
- **マルチエージェント**: 複数エージェントの協調・競争
- **逆強化学習**: 専門家の行動から報酬関数を推定
- **オフライン強化学習**: 事前収集データのみで学習
