# 強化学習（Reinforcement Learning）

## 要点
- 試行錯誤を通じて報酬を最大化する行動（方策）を学習する機械学習パラダイム。教師あり学習と異なり、正解は与えられず報酬シグナルから学習。
- 主要要素：エージェント、環境、状態、行動、報酬、方策。マルコフ決定過程（MDP）でモデル化。
- 代表手法：Q学習（価値ベース）、Policy Gradient（方策ベース）、Actor-Critic（両者の組み合わせ）、DQN（深層強化学習）。

## 定義
強化学習とは、エージェントが環境と相互作用しながら、試行錯誤を通じて累積報酬を最大化する行動方策を学習する機械学習の枠組み。正解ラベルではなく、報酬（スカラー値）というフィードバック信号を使って学習する。

## 重要キーワード
- **エージェント（Agent）**: 学習・行動する主体
- **環境（Environment）**: エージェントが相互作用する対象
- **状態（State）**: 環境の現在の状況
- **行動（Action）**: エージェントが選択する動作
- **報酬（Reward）**: 行動の結果得られるスカラー値のフィードバック
- **方策（Policy）**: 状態から行動への写像 $\pi: S \rightarrow A$
- **価値関数（Value Function）**: 状態や行動の長期的な良さ
- **Q関数**: 状態-行動ペアの価値 $Q(s,a)$
- **マルコフ決定過程（MDP）**: 強化学習の数学的枠組み

---

## Actor-Critic手法

### 要点
Actor-Criticは方策ベースと価値ベースを組み合わせた強化学習手法。Actorが方策（行動選択）を学習し、Criticが価値関数（行動評価）を学習。両者が協調して学習効率と安定性を向上。A3C、PPO等の発展手法がある。

### 定義
Actor-Critic手法とは、Actor（方策を学習するネットワーク）とCritic（価値関数を学習するネットワーク）の2つのモジュールを持つ強化学習アルゴリズム。Actorが行動を選択し、Criticがその行動の良さ（価値）を評価してActorにフィードバックすることで学習を進める。

### 重要キーワード
- **Actor（行動者）**: 方策 $\pi(a|s)$ を学習し、状態に応じて行動を選択
- **Critic（評価者）**: 価値関数 $V(s)$ または $Q(s,a)$ を学習し、行動を評価
- **方策ベース**: 方策を直接学習（例：Policy Gradient）
- **価値ベース**: 価値関数を学習して行動選択（例：Q学習、DQN）
- **TD誤差（Temporal Difference Error）**: Criticが計算する予測誤差、Actorの学習信号
- **Advantage関数**: $A(s,a) = Q(s,a) - V(s)$、基準からの優位性
- **A3C（Asynchronous Advantage Actor-Critic）**: 並列学習版
- **PPO（Proximal Policy Optimization）**: 安定性向上版

### 詳細

#### 背景
強化学習には大きく2つのアプローチがある：
- **価値ベース**（Q学習、DQN）: 価値関数を学習し、最大価値の行動を選択。決定的だが探索が難しい
- **方策ベース**（Policy Gradient）: 方策を直接学習。確率的だが分散が大きく学習が不安定

Actor-Criticは両者の利点を組み合わせ、安定性と効率を両立。

#### 基本構造

**Actor（行動者）**:
- **役割**: 方策 $\pi_\theta(a|s)$ を学習し、行動を選択
- **入力**: 状態 $s$
- **出力**: 行動の確率分布または行動そのもの
- **学習**: Criticからの評価を使って方策を改善

**Critic（評価者）**:
- **役割**: 価値関数 $V_\phi(s)$ を学習し、状態・行動を評価
- **入力**: 状態 $s$（または状態-行動ペア $(s,a)$）
- **出力**: 価値の推定値
- **学習**: TD誤差を最小化

#### アルゴリズムの流れ
```
1. 初期状態 s を観測
2. Actor: 方策 π(a|s) に従って行動 a を選択
3. 環境: 行動を実行し、報酬 r と次状態 s' を返す
4. Critic: TD誤差を計算
   δ = r + γV(s') - V(s)
5. Critic: 価値関数を更新（TD誤差を減らす）
   V(s) ← V(s) + α_c・δ
6. Actor: 方策を更新（TD誤差を利用）
   θ ← θ + α_a・δ・∇log π(a|s)
7. s ← s' として繰り返し
```

#### 図解（構造）
```
        環境
         ↓ 状態 s
    ┌────────────┐
    │   Actor    │ ← 方策パラメータ θ
    │ π(a|s;θ)   │ (行動を選択)
    └────────────┘
         ↓ 行動 a
        環境
         ↓ 報酬 r, 次状態 s'
    ┌────────────┐
    │   Critic   │ ← 価値パラメータ φ
    │  V(s;φ)    │ (行動を評価)
    └────────────┘
         ↓ TD誤差 δ
    Actor更新 ← δ
```

#### 主要な変種

**Advantage Actor-Critic（A2C）**:
- **Advantage関数**: $A(s,a) = Q(s,a) - V(s)$
- **利点**: 基準値からの相対的な良さを評価、分散削減
- **更新**: $\theta \leftarrow \theta + \alpha \cdot A(s,a) \cdot \nabla \log \pi(a|s)$

**A3C（Asynchronous Advantage Actor-Critic）**:
- **特徴**: 複数の並列エージェントで非同期学習
- **利点**: 学習の多様性、高速化
- **DeepMind**: Atariゲームで高性能

**PPO（Proximal Policy Optimization）**:
- **特徴**: 方策の大幅な変更を制限（Clipping）
- **利点**: 学習の安定性向上
- **用途**: ロボット制御、OpenAI Five等

### 試験での問われ方

#### 典型設問
- **「Actorが（行動を選択）し、Criticが（行動を評価）する」**
- **「Actorが（方策を学習）し、Criticが（価値関数を学習）する」**
- Actor-Criticの構成要素と役割
- 価値ベース・方策ベース・Actor-Criticの違い
- TD誤差の役割
- Advantage関数の意味

#### ひっかけポイントと違いの整理

| 項目 | Actor-Critic | Q学習/DQN | Policy Gradient |
|------|--------------|-----------|-----------------|
| **分類** | 方策+価値ベース | 価値ベース | 方策ベース |
| **学習対象** | 方策と価値関数 | 価値関数のみ | 方策のみ |
| **行動選択** | 方策から確率的 | 価値最大を決定的 | 方策から確率的 |
| **評価方法** | Criticが評価 | Q値で評価 | モンテカルロ報酬 |
| **安定性** | 中〜高 | 高 | 低（分散大） |
| **探索** | 方策に組み込み | ε-greedy等必要 | 自然に探索 |

**混同注意**:
- **Actor-Critic vs Q学習**: Actor-Criticは方策を直接学習、Q学習は価値から行動を導出
- **Actor-Critic vs Policy Gradient**: Actor-CriticはCriticで評価、Policy Gradientはモンテカルロ報酬
- **A2C vs A3C**: A2Cは同期学習、A3Cは非同期並列学習

**出題パターン**:
- 「Actorの役割」→**行動選択、方策学習**
- 「Criticの役割」→**行動評価、価値関数学習**
- 「TD誤差の使用目的」→**CriticとActorの両方を更新**
- 「方策ベースと価値ベースの組み合わせ」→**Actor-Critic**

### 補足

#### 実務観点
**用途**: 
- ロボット制御（連続動作空間）
- ゲームAI（Atari、囲碁、DOTA2）
- 自動運転（経路計画）
- 資源最適化

**利点**:
- 連続動作空間に対応
- 確率的方策で探索が自然
- 価値ベースより柔軟

**課題**:
- ハイパーパラメータ調整が難しい
- 学習の安定性（PPOで改善）
- 計算コスト（2つのネットワーク）

#### 実装のポイント
- **ネットワーク構成**: ActorとCriticで重みを共有することも可能
- **学習率**: Actor $\alpha_a$ とCritic $\alpha_c$ を別々に設定
- **報酬の正規化**: 学習の安定化に重要
- **ベースライン**: Criticの価値推定を基準に使用

#### 関連トピック
- [教師あり学習](supervised_learning.md) - 学習パラダイムの違い
- [ニューラルネットワーク](../06_deep_learning/neural_network_basics.md) - Deep Actor-Criticの基礎

---

## 強化学習の基礎概念

### マルコフ決定過程（MDP）
- **構成要素**: 状態集合 $S$、行動集合 $A$、遷移確率 $P(s'|s,a)$、報酬関数 $R(s,a)$、割引率 $\gamma$
- **マルコフ性**: 次状態は現在の状態と行動のみに依存
- **目的**: 累積報酬の期待値 $\mathbb{E}[\sum_{t=0}^\infty \gamma^t R_t]$ を最大化

### 主要な学習手法

#### Q学習
- **価値ベース**: Q関数 $Q(s,a)$ を学習
- **更新式**: $Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$
- **特徴**: オフポリシー学習、離散行動空間

---

## ε-greedy方策

### 要点
ε-greedy（イプシロン・グリーディ）方策は、探索（Exploration）と活用（Exploitation）のバランスを取る最も基本的な行動選択戦略。確率 $\epsilon$ でランダムな行動（探索）、確率 $1-\epsilon$ で現在最良の行動（活用）を選択する。Q学習やDQNで標準的に使用される。

### 定義
ε-greedy方策とは、強化学習における行動選択戦略の1つで、以下のルールに従う：
- **確率 $\epsilon$（通常0.1〜0.3）**: すべての行動からランダムに選択（探索）
- **確率 $1-\epsilon$**: 現在のQ値が最大の行動を選択（活用）

数式表現：
$$
a = \begin{cases}
\arg\max_{a'} Q(s,a') & \text{確率 } 1-\epsilon \text{ （活用）} \\
\text{random}(A) & \text{確率 } \epsilon \text{ （探索）}
\end{cases}
$$

### 重要キーワード
- **探索（Exploration）**: 未知の行動を試して新しい情報を得る
- **活用（Exploitation）**: 現在の知識で最良の行動を選ぶ
- **ε（イプシロン）**: ランダム行動の確率（0≤ε≤1）
- **グリーディ（Greedy）**: 常に最良の行動を選ぶ戦略
- **ε-減衰（ε-decay）**: 学習進行に応じてεを減少させる手法
- **探索と活用のトレードオフ**: 探索しすぎると報酬が低く、活用しすぎると局所解に陥る

### 詳細

#### 背景
強化学習では、未知の環境で最適な方策を見つける必要がある。この過程で2つの相反する要求が生じる：

- **探索（Exploration）**: 未経験の行動を試して環境の情報を収集。長期的には必要だが短期的には報酬が低い
- **活用（Exploitation）**: 既知の最良行動を選んで報酬を最大化。短期的には有効だが新しい発見がない

**探索と活用のジレンマ**:
- 探索しすぎ → 報酬が低い、学習が非効率
- 活用しすぎ → 局所最適解に陥る、より良い行動を見逃す

ε-greedy方策は、このトレードオフを単純かつ効果的にバランスさせる。

#### 基本アルゴリズム
```
現在の状態: s
Q値テーブル: Q(s,a) (すべての状態-行動ペア)
パラメータ: ε (例: 0.1)

1. 確率εでランダム行動を選択するか判定:
   - ランダム値 r ∈ [0,1) を生成
   - if r < ε:
       a = ランダムに選択(すべての行動)  # 探索
   - else:
       a = argmax_a' Q(s,a')  # 活用

2. 行動 a を実行
3. 報酬 r と次状態 s' を観測
4. Q値を更新
5. s ← s' として繰り返し
```

#### 図解（行動選択の流れ）
```
    状態 s
      ↓
┌─────────────────┐
│ ランダム値生成  │
│   r ∈ [0,1)     │
└─────────────────┘
      ↓
   r < ε ?
   /     \
 Yes      No
  ↓        ↓
探索      活用
ランダム   最大Q値
行動選択   行動選択
  ↓        ↓
    行動 a
```

#### εの設定パターン

**固定ε**:
- **例**: ε = 0.1（常に10%探索）
- **利点**: シンプル、実装が容易
- **欠点**: 学習後期も探索し続ける、最適方策への収束が遅い

**ε-減衰（ε-decay）**:
- **例**: $\epsilon_t = \epsilon_0 \cdot \text{decay}^t$ または $\epsilon_t = \max(\epsilon_{min}, \epsilon_0 - \Delta\epsilon \cdot t)$
- **典型値**: $\epsilon_0 = 1.0 \rightarrow \epsilon_{min} = 0.01$
- **利点**: 初期に十分探索、後期に活用を重視
- **用途**: DQNの標準的設定

**例（線形減衰）**:
```
初期: ε = 1.0（完全ランダム）
↓ エピソード進行
中期: ε = 0.3（30%探索）
↓ さらに進行
後期: ε = 0.01（1%探索、ほぼ活用）
```

#### 実例（数値シミュレーション）

**設定**:
- 4つの行動: A, B, C, D
- 現在のQ値: Q(s,A)=10, Q(s,B)=15, Q(s,C)=5, Q(s,D)=8
- 最良行動: B（Q値15）
- ε = 0.2（20%探索）

**行動選択の確率**:
- **活用（確率80%）**: 行動Bを選択
- **探索（確率20%）**: A, B, C, Dから均等にランダム選択（各5%）

**結果**:
- P(A) = 0.05（探索のみ）
- P(B) = 0.80 + 0.05 = 0.85（活用+探索）
- P(C) = 0.05（探索のみ）
- P(D) = 0.05（探索のみ）

→ 最良行動Bが高確率で選ばれるが、他の行動も試される

#### 他の探索戦略との比較

| 戦略 | 特徴 | 利点 | 欠点 |
|------|------|------|------|
| **ε-greedy** | 確率εでランダム、1-εで最良 | シンプル、実装容易 | 一様ランダムで非効率 |
| **Softmax（Boltzmann）** | Q値に応じた確率分布 | Q値を考慮した探索 | 温度パラメータ調整が難しい |
| **UCB（Upper Confidence Bound）** | 不確実性を考慮 | 理論的保証あり | Q学習では適用困難 |
| **Thompson Sampling** | ベイズ的サンプリング | 効率的探索 | 実装が複雑 |

### 試験での問われ方

#### 典型設問
- **「ε-greedy方策で、εは（探索の確率）を表す」**
- **「確率εで（ランダムな行動）、確率1-εで（最良の行動）を選択」**
- ε-greedy方策の定義
- 探索と活用のバランスの取り方
- εの値が学習に与える影響
- ε-減衰の目的

#### ひっかけポイントと違いの整理

**混同注意**:
- **ε-greedy vs Softmax**: ε-greedyは一様ランダム、SoftmaxはQ値に応じた確率
- **探索（Exploration） vs 活用（Exploitation）**: 探索は情報収集、活用は報酬最大化
- **εの大小**: ε大→探索重視、ε小→活用重視

**出題パターン**:
- 「εの値が大きいとき」→**探索が増える、ランダム性が高い**
- 「ε=0のとき」→**完全なグリーディ方策（常に最良行動）**
- 「ε=1のとき」→**完全なランダム方策（常に探索）**
- 「ε-減衰の目的」→**初期に探索、後期に活用を重視**
- 「ε-greedyの欠点」→**一様ランダムで非効率、劣悪な行動も同確率で試す**

**選択肢で出やすい対比**:
| 項目 | ε-greedy | Softmax | 純粋グリーディ |
|------|----------|---------|----------------|
| **探索方法** | 確率εでランダム | Q値に応じた確率 | 探索なし |
| **活用方法** | 最大Q値を選択 | 高Q値が高確率 | 常に最大Q値 |
| **パラメータ** | ε（探索確率） | 温度T | なし |
| **適用場面** | Q学習、DQN | 行動選択の洗練 | デプロイ時 |

### 補足

#### 実務観点

**用途**:
- Q学習の標準的探索戦略
- DQNでのエピソード開始時
- マルチアームバンディット問題
- A/Bテストの初期段階

**設定の目安**:
- **初期ε**: 0.5〜1.0（十分に探索）
- **最終ε**: 0.01〜0.05（少し探索を残す）
- **減衰期間**: 全エピソードの50〜80%

**実装のポイント**:
- ε-減衰の実装はエピソード数またはステップ数ベース
- テスト時はε=0（純粋な活用）
- ランダムシードの管理で再現性確保

**課題**:
- **一様ランダムの非効率性**: 明らかに悪い行動も試す
- **εの調整**: タスクに依存、試行錯誤が必要
- **連続動作空間**: 離散化が必要または他手法を使用

#### より高度な探索戦略

**ε-減衰の改良**:
- 指数減衰: $\epsilon_t = \epsilon_0 \cdot \gamma^t$
- 線形減衰: $\epsilon_t = \epsilon_0 - \frac{\epsilon_0 - \epsilon_{min}}{N} \cdot t$
- ステップ関数: 一定期間ごとに減少

**適応的探索**:
- Q値の分散が大きい状態で探索を増やす
- 訪問回数が少ない状態-行動を優先

#### 関連トピック
- [強化学習基礎](reinforcement_learning.md#強化学習の基礎概念) - MDP、Q学習
- [DQN](../06_deep_learning/neural_network_basics.md) - Deep Q-Network での使用例
- [Actor-Critic](reinforcement_learning.md#actor-critic手法) - 方策ベースでの探索

#### Policy Gradient
- **方策ベース**: 方策 $\pi_\theta(a|s)$ を直接学習
- **更新**: 勾配上昇法で期待報酬を最大化
- **特徴**: 確率的方策、連続動作対応

#### DQN（Deep Q-Network）
- **深層強化学習**: Q学習にニューラルネットワーク適用
- **工夫**: Experience Replay、Target Network
- **成果**: Atariゲームで人間超え

---

## DQNとデュエリングネットワーク（Dueling Network）★試験頻出

### DQNの基礎

#### 定義
**DQN（Deep Q-Network）**は、Q学習にニューラルネットワーク（深層学習）を適用した強化学習手法。従来のQ学習がテーブルでQ値を保持するのに対し、DQNはニューラルネットワークでQ関数を近似する。

#### 主要技術
- **Experience Replay**: 過去の経験を保存・再利用して学習
- **Target Network**: 目標Q値を計算する固定ネットワーク
- **成果**: Atariゲームで人間を超える性能（2013年、DeepMind）

---

### デュエリングネットワーク（Dueling Network）★試験頻出

#### 要点
デュエリングネットワークは、DQNの改良版で、Q値を**状態価値V(s)**と**アドバンテージA(s,a)**の2つに分解して学習する。単一ストリームでQ値を直接学習するより、状態の価値と行動の相対的優位性を分離することで学習効率と安定性が向上。

#### 定義
デュエリングネットワークとは、Q関数を以下のように分解するニューラルネットワークアーキテクチャ：

$$
Q(s,a) = V(s) + A(s,a)
$$

または、より正確には（平均減算版）：

$$
Q(s,a) = V(s) + \left(A(s,a) - \frac{1}{|A|}\sum_{a'} A(s,a')\right)
$$

- **V(s)**: 状態価値関数（その状態にいることの価値）
- **A(s,a)**: アドバンテージ関数（その状態で行動aを選ぶ相対的優位性）

#### 重要キーワード
- **デュエリングネットワーク（Dueling Network）**: Q値をV(s)とA(s,a)に分解するDQN改良版
- **状態価値V(s)**: その状態にいることの価値（行動に依存しない）
- **アドバンテージA(s,a)**: 特定の行動の相対的な良さ（平均からの差）
- **デュアルストリーム（Dual Stream）**: 共有層から2つの別々のストリームに分岐
- **DQN（Deep Q-Network）**: Q学習にニューラルネットを適用した基本形
- **Double DQN**: 過大評価を防ぐDQNの改良版
- **Prioritized Experience Replay**: 重要な経験を優先的に学習

#### 詳細

##### 背景：なぜ分解するのか

**従来のDQNの問題点**:
- すべての行動のQ値を独立に学習
- 多くの状態では、どの行動を選んでも価値がほぼ同じ（例：ゲームの待機状態）
- 無駄な学習が多く、効率が悪い

**デュエリングネットワークの利点**:
- **状態価値V(s)**：その状態自体の価値を学習（全行動共通）
- **アドバンテージA(s,a)**：行動間の相対的な優劣のみを学習
- 行動を選ばない状態でもV(s)を学習できる → サンプル効率向上
- 行動の相対的重要性に注目 → 学習安定化

##### アーキテクチャ

**構造**:
```
入力: 状態 s （例：ゲーム画面のピクセル）
    ↓
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
共有層（Conv層等）: 特徴抽出
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ↓
  分岐点
   / \
  /   \
 ↓     ↓
価値ストリーム    アドバンテージストリーム
（全結合層）       （全結合層）
 ↓              ↓
V(s)           A(s,a) for all a
[スカラー]      [行動数次元ベクトル]
 \              /
  \            /
   ↓          ↓
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
統合層: Q(s,a) = V(s) + A(s,a)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ↓
出力: Q(s,a) for all a
[行動数次元ベクトル]
```

**従来のDQNとの比較**:
```
【従来のDQN】
入力 → 共有層 → 全結合層 → Q(s,a) for all a
                          [直接予測]

【デュエリングネットワーク】
入力 → 共有層 → ┬→ 価値ストリーム → V(s)
              └→ アドバンテージストリーム → A(s,a)
              統合: Q(s,a) = V(s) + A(s,a)
```

##### 数式の詳細

**基本分解**:
$$
Q(s,a) = V(s) + A(s,a)
$$

**問題点**: V(s)とA(s,a)が一意に定まらない
- 例：V(s) + 10、A(s,a) - 10 でも同じQ値

**解決策（平均減算）**:
$$
Q(s,a) = V(s) + \left(A(s,a) - \frac{1}{|A|}\sum_{a'} A(s,a')\right)
$$

- アドバンテージの平均を0にする制約
- V(s)とA(s,a)が一意に決まる
- 最良行動のアドバンテージがV(s)からの増分を表す

**別の制約（最大値減算）**:
$$
Q(s,a) = V(s) + (A(s,a) - \max_{a'} A(s,a'))
$$

##### デュエリングネットワークが予測（算出）するもの

✅ **デュエリングネットワークが予測するもの（適切）**:

1. **状態価値V(s)** → ✅ 適切
   - 価値ストリームが出力
   - その状態にいることの価値

2. **アドバンテージA(s,a)** → ✅ 適切
   - アドバンテージストリームが出力
   - 各行動の相対的優位性

3. **Q値Q(s,a)** → ✅ 適切
   - V(s)とA(s,a)を統合して算出
   - 最終的な出力（行動選択に使用）

❌ **デュエリングネットワークが予測しないもの（不適切）**:

1. **方策π(a|s)（行動の確率分布）** → ❌ **最も不適切**
   - デュエリングネットワークは**価値ベース**手法
   - 方策を直接予測するのは**方策ベース**手法（Policy Gradient、Actor-Critic等）
   - Q値から間接的に行動選択するが、確率分布は出力しない

2. **報酬r** → ❌ 不適切
   - 報酬は環境から与えられるもの
   - モデルフリー手法では予測しない

3. **次状態s'** → ❌ 不適切
   - 状態遷移は環境が決定
   - モデルベース手法ではないため予測しない

4. **状態遷移確率P(s'|s,a)** → ❌ 不適切
   - モデルフリー手法では学習しない

##### 試験での問われ方

**典型問題**：「デュエリングネットワークが予測（算出）するものとして、最も不適切な選択肢を1つ選べ。」

**選択肢例**：
- A. 状態価値V(s) → ✅ 適切（価値ストリームが出力）
- B. アドバンテージA(s,a) → ✅ 適切（アドバンテージストリームが出力）
- C. Q値Q(s,a) → ✅ 適切（V(s)とA(s,a)から算出）
- D. **方策π(a|s)** → ❌ **最も不適切**（価値ベースなので方策は予測しない）

**正解**: **D（最も不適切）**

**理由**：
- デュエリングネットワークはDQNの改良版で**価値ベース**手法
- **Q値（V(s) + A(s,a)）**を予測し、最大Q値の行動を選択
- **方策π(a|s)**を直接予測するのは**方策ベース**手法（Actor-Critic、PPO等）

##### 引っ掛けポイント

| ひっかけ | 正しい理解 | 混同されやすい手法 |
|----------|------------|-------------------|
| ❌ 方策π(a|s)を予測 | ✅ Q値を予測、方策は予測しない | Actor-Critic（方策を予測） |
| ❌ 報酬rを予測 | ✅ 環境から与えられる | モデルベース手法 |
| ❌ 次状態s'を予測 | ✅ 環境が決定、予測しない | モデルベース手法 |
| ✅ V(s)を予測 | ✅ 価値ストリームが出力 | デュエリングの特徴 |
| ✅ A(s,a)を予測 | ✅ アドバンテージストリームが出力 | デュエリングの特徴 |
| ✅ Q(s,a)を算出 | ✅ V(s) + A(s,a)で統合 | DQN共通 |

**キーポイント**：
- **価値ベース手法**：Q値・V(s)・A(s,a)を予測（DQN、Dueling DQN）
- **方策ベース手法**：方策π(a|s)を直接予測（Policy Gradient）
- **Actor-Critic**：方策と価値の両方を予測（Actor=方策、Critic=価値）

##### DQN系手法の比較

| 手法 | 予測するもの | 主な改良点 |
|------|-------------|-----------|
| **DQN** | Q(s,a) | Experience Replay、Target Network |
| **Double DQN** | Q(s,a) | 行動選択と評価を分離（過大評価防止） |
| **Dueling DQN** | V(s) + A(s,a) | 価値とアドバンテージに分解 |
| **Rainbow DQN** | 上記すべて | 複数改良の組み合わせ |

**すべて価値ベース手法**：方策π(a|s)は予測せず、Q値から行動選択

#### 実例

##### 例1：ゲーム状態での学習

**状態**: 敵が遠くにいる待機状態

**従来のDQN**:
```
Q(s, 上) = 5.2
Q(s, 下) = 5.1
Q(s, 左) = 5.0
Q(s, 右) = 5.3
```
- すべてのQ値を個別に学習（非効率）

**デュエリングネットワーク**:
```
V(s) = 5.0  （この状態自体の価値）

A(s, 上) = 0.2
A(s, 下) = 0.1
A(s, 左) = 0.0
A(s, 右) = 0.3

Q(s, 上) = 5.0 + 0.2 = 5.2
Q(s, 下) = 5.0 + 0.1 = 5.1
Q(s, 左) = 5.0 + 0.0 = 5.0
Q(s, 右) = 5.0 + 0.3 = 5.3
```
- 状態の基本価値V(s)=5.0を学習
- 行動間の差（アドバンテージ）のみを学習
- 行動を選ばなくてもV(s)を更新可能

##### 例2：出力の対比

**従来のDQN**:
```
入力: ゲーム画面（84×84×4）
出力: [Q(s,a1), Q(s,a2), Q(s,a3), Q(s,a4)]
     例: [2.3, 5.1, 3.7, 4.2]
```

**デュエリングネットワーク**:
```
入力: ゲーム画面（84×84×4）
中間出力1: V(s) = 4.0（価値ストリーム）
中間出力2: A(s,·) = [-1.7, 1.1, -0.3, 0.2]（アドバンテージストリーム）
最終出力: Q(s,·) = V(s) + A(s,·) = [2.3, 5.1, 3.7, 4.2]
```

#### 補足

##### 実務上の利点
1. **サンプル効率向上**: 行動を選ばなくてもV(s)を学習
2. **学習安定化**: 行動間の相対的優劣に注目
3. **汎化性能向上**: 状態価値の共有学習
4. **実装の容易さ**: DQNコードを少し修正するだけ

##### 実装のポイント
- 共有層の設計（特徴抽出）
- 価値ストリームは1次元出力
- アドバンテージストリームは行動数次元出力
- 平均減算の実装（アドバンテージを正規化）

##### 応用事例
- Atariゲーム：多くのゲームでDQNより高性能
- ロボット制御：複雑な行動空間での学習
- 推薦システム：ユーザー状態と推薦行動の価値分解

---

## 試験での問われ方（強化学習全般）

### 典型設問
- 強化学習の定義（報酬から学習）
- 教師あり学習・教師なし学習との違い
- エージェント・環境・状態・行動・報酬の役割
- 方策・価値関数・Q関数の意味
- 探索と活用のトレードオフ

### 引っ掛けポイント
- **強化学習 vs 教師あり学習**: 正解ラベルなし、報酬のみ
- **強化学習 vs 教師なし学習**: 目的（報酬最大化）あり、教師なしは構造発見
- **方策ベース vs 価値ベース**: 学習対象の違い

## 補足（強化学習全般）

### 実務課題
- **報酬設計**: 適切な報酬関数の設定が困難
- **サンプル効率**: 多数の試行が必要
- **安全性**: 探索中の危険な行動
- **シミュレーションギャップ**: 実環境での性能低下

---

## Sim-to-Real転移（シミュレーションから現実への転移）

### 要点
シミュレータで学習した方策を実世界に転移する手法。代表的手法は**ドメインランダマイゼーション**（シミュレータのパラメータをランダム化して多様な環境で訓練）。他に**ドメイン適応**（実データで微調整）、**システム同定**（実環境パラメータ推定）がある。

### 定義
Sim-to-Real転移とは、シミュレーション環境で学習した強化学習の方策を、実世界（現実環境）で動作させる技術。シミュレータは安全・高速・低コストで学習できるが、物理法則の近似や計測誤差により実環境との差（シミュレーションギャップ）が生じるため、この差を埋める工夫が必要。

### 重要キーワード
- **Sim-to-Real（シム・トゥ・リアル）**: シミュレーションから現実への転移
- **シミュレーションギャップ（Reality Gap）**: シミュレータと実環境の差
- **ドメインランダマイゼーション（Domain Randomization）**: シミュレータのパラメータをランダム化
- **ドメイン適応（Domain Adaptation）**: 実データで方策を微調整
- **システム同定（System Identification）**: 実環境の物理パラメータを推定
- **転移学習（Transfer Learning）**: 学習済み知識を新環境に適用
- **ロバスト性（Robustness）**: 環境変動への頑健性

### 詳細

#### 背景
**シミュレータ学習の利点**:
- **安全性**: 失敗しても実機が壊れない
- **高速性**: 並列実行・時間加速が可能
- **低コスト**: 実機実験より圧倒的に安価
- **データ量**: 大量のエピソードを効率的に生成

**シミュレーションギャップの原因**:
- **物理モデルの簡略化**: 摩擦・空気抵抗・弾性などの近似
- **センサノイズの違い**: 理想的センサ vs 実センサの誤差
- **アクチュエータ特性**: モータの遅延・非線形性
- **外乱**: 風・振動などシミュレータで再現困難な要因

→ シミュレータで完璧に動いても、実環境では失敗する可能性

#### 主要な転移手法

**1. ドメインランダマイゼーション（最も代表的）**

**概念**:
シミュレータの物理パラメータ（摩擦係数、質量、重力、カメラ位置、照明等）を訓練中にランダムに変動させる。多様な環境で学習することで、実環境の条件もその変動範囲内に含まれ、ロバストな方策を獲得。

**アルゴリズム**:
```
各エピソード開始時:
1. パラメータをランダムにサンプリング:
   - 摩擦係数: μ ∈ [0.3, 0.7]
   - 物体質量: m ∈ [0.8, 1.2] kg
   - カメラ視点: θ ∈ [-15°, 15°]
   - 照明強度: I ∈ [50, 200] lux
2. サンプルしたパラメータでシミュレータを設定
3. そのシミュレータで1エピソード実行・学習
4. 繰り返し
```

**利点**:
- 実データ不要（シミュレータのみで完結）
- 実装が比較的容易
- OpenAI、Google等で成功事例多数

**欠点**:
- ランダム化範囲の設定が難しい
- 過度なランダム化で学習が困難化
- すべての差を吸収できない場合もある

**実例**:
- **OpenAI Dactyl**: ロボットハンドでルービックキューブ操作（視覚・物理パラメータをランダム化）
- **Google Grasp**: 物体把持タスク（物体形状・テクスチャをランダム化）

**2. ドメイン適応（Domain Adaptation）**

**概念**:
シミュレータで事前学習した方策を、少量の実データでファインチューニング。シミュレータで大まかな挙動を学び、実環境で最終調整。

**手順**:
```
1. シミュレータで方策 π_sim を学習
2. 実環境で少数エピソード（10〜100回程度）実行
3. 実データで π_sim を微調整 → π_real
4. π_real を実環境にデプロイ
```

**利点**:
- シミュレータで大部分を学習（効率的）
- 実データで最終調整（精度向上）

**欠点**:
- 実環境でのデータ収集が必要
- 初期方策が悪いと危険な場合も

**3. システム同定（System Identification）**

**概念**:
実環境から物理パラメータを推定し、シミュレータをより現実に近づける。実測データとシミュレーション結果を比較してパラメータを最適化。

**手順**:
```
1. 実環境で動作を記録（軌跡・力・速度等）
2. シミュレータで同じ動作を再現
3. 実測とシミュレーション結果の誤差を最小化:
   θ* = argmin_θ ||y_real - y_sim(θ)||²
4. 推定パラメータ θ* でシミュレータを更新
5. 更新されたシミュレータで方策を学習
```

**利点**:
- シミュレータの精度向上
- 理論的に明確

**欠点**:
- すべてのパラメータを同定できない
- 計測コストが高い

**4. その他の手法**

- **プログレッシブネット**: 複数タスクの知識を段階的に転移
- **メタ学習**: 環境適応能力自体を学習
- **実環境教師あり学習**: 実環境の状態-行動ペアを教師データとして使用

#### 図解（ドメインランダマイゼーション）
```
シミュレータ（パラメータランダム化）
┌─────────────────────────────────┐
│ エピソード1: 摩擦0.3, 質量0.9kg │ → 学習
│ エピソード2: 摩擦0.6, 質量1.1kg │ → 学習
│ エピソード3: 摩擦0.4, 質量1.0kg │ → 学習
│         ...多様な条件...        │
└─────────────────────────────────┘
           ↓
    ロバストな方策 π
           ↓
      実環境にデプロイ
   （条件は固定だが、訓練済み
    範囲内なので動作可能）
```

#### 実例（数値・具体例）

**OpenAI Dactyl（ロボットハンドのルービックキューブ操作）**:
- **シミュレータ**: MuJoCo物理エンジン
- **ランダム化項目**: 
  - 物体サイズ: ±10%
  - 摩擦係数: 0.7〜1.3
  - 質量: ±20%
  - カメラ位置・角度: ±5度
  - 照明・テクスチャ
- **結果**: 50回連続成功（実環境で追加学習なし）

**比較（手法別の実データ必要量）**:
| 手法 | シミュレータデータ | 実データ | 転移成功率 |
|------|-------------------|---------|-----------|
| **ドメインランダマイゼーション** | 大量（100万〜） | 不要 | 高 |
| **ドメイン適応** | 大量 | 少量（数百） | 非常に高 |
| **システム同定** | 中量 | 中量（数千） | 中〜高 |
| **実環境のみ学習** | なし | 大量（数万〜） | 最高 |

### 試験での問われ方

#### 典型設問
- **「シミュレータで学習した方策を実世界に転移する手法」→ ドメインランダマイゼーション**
- **「シミュレータのパラメータをランダム化して訓練」→ ドメインランダマイゼーション**
- **「シミュレーションギャップを埋める手法」→ ドメインランダマイゼーション、ドメイン適応、システム同定**
- Sim-to-Real転移の目的
- ドメインランダマイゼーションの利点・欠点
- 実データ不要な手法の特定

#### ひっかけポイントと違いの整理

**混同注意**:
- **ドメインランダマイゼーション vs ドメイン適応**: 前者は実データ不要、後者は実データで微調整
- **転移学習 vs Sim-to-Real**: 転移学習は広い概念、Sim-to-Realはその一種（特にシミュ→実環境）
- **システム同定 vs ドメインランダマイゼーション**: システム同定はシミュレータを実環境に合わせる、ランダマイゼーションは逆に多様化

**出題パターン**:
- 「実データなしで転移」→**ドメインランダマイゼーション**
- 「シミュレータのパラメータを推定」→**システム同定**
- 「少量の実データで微調整」→**ドメイン適応**
- 「シミュレーションギャップの原因」→**物理モデルの簡略化、センサノイズ、外乱**
- 「ロバスト性向上」→**ドメインランダマイゼーション**

**選択肢で出やすい対比**:
| 項目 | ドメインランダマイゼーション | ドメイン適応 | システム同定 |
|------|---------------------------|-------------|-------------|
| **実データ** | 不要 | 少量必要 | 中量必要 |
| **シミュレータ設定** | ランダム化 | 固定 | 推定値で更新 |
| **実装難易度** | 低〜中 | 低 | 高 |
| **代表例** | OpenAI Dactyl | ファインチューニング | 物理パラメータ推定 |
| **適用場面** | 実データ収集困難 | 初期方策あり | 高精度要求 |

### 補足

#### 実務観点

**用途**:
- **ロボット制御**: 把持・歩行・マニピュレーション
- **自動運転**: シミュレータで事前学習
- **ドローン制御**: 安全な訓練環境
- **製造業**: 組立・検査ロボット

**成功の鍵**:
- **ランダム化範囲の調整**: 広すぎず狭すぎず
- **重要パラメータの特定**: 影響大きい要素を優先
- **段階的転移**: シミュレータ → 簡易実環境 → 本番環境
- **安全機構**: 実環境での予期せぬ動作への対策

**課題**:
- **視覚情報の転移困難**: CGと実画像の差（Photorealistic Rendering等で対応）
- **接触・摩擦の再現困難**: 物理シミュレーションの限界
- **長期的挙動の予測誤差**: 微小な誤差の蓄積
- **計算コスト**: 大量のランダム化シミュレーション

#### 実装のポイント

**ドメインランダマイゼーションの設定例**:
```python
# 各エピソードでランダムにパラメータ設定
sim.set_friction(uniform(0.3, 0.7))
sim.set_mass(uniform(0.8, 1.2))
sim.set_gravity(uniform(9.6, 10.0))
sim.set_camera_pos(uniform(-0.1, 0.1), uniform(-0.1, 0.1))
sim.set_lighting(uniform(0.5, 1.5))
```

**段階的な転移戦略**:
1. **Phase 1**: シミュレータでベース方策学習
2. **Phase 2**: ドメインランダマイゼーションでロバスト化
3. **Phase 3**: 簡易実環境（安全な場所）でテスト
4. **Phase 4**: 必要に応じてドメイン適応
5. **Phase 5**: 本番環境デプロイ

#### 関連トピック
- [強化学習基礎](reinforcement_learning.md#強化学習の基礎概念) - 方策学習の基本
- [転移学習](../05_machine_learning/supervised_learning.md) - より広い転移学習の文脈
- [ロボット制御](../07_ai_applications/) - 実応用例

---

---

## マルチエージェント強化学習（Multi-Agent Reinforcement Learning, MARL）★試験頻出

### 要点
複数のエージェントが存在し、協調的・競争的な関係を考慮しながら学習を行う強化学習。各エージェントが独立に学習するのではなく、他エージェントの行動を考慮する。AlphaGo、AlphaZero、OpenAI Five等の代表的プログラムで実用化。

### 定義
**マルチエージェント強化学習（MARL）**とは、複数のエージェントが同一環境内で相互作用しながら、各自の報酬を最大化する方策を学習する強化学習の枠組み。エージェント間に協調関係（cooperative）・競争関係（competitive）・混合関係（mixed）が存在し、他エージェントの行動を考慮した戦略学習が必要。

### 重要キーワード
- **マルチエージェント（Multi-Agent）**: 複数のエージェントが環境内に存在
- **協調的（Cooperative）**: エージェント間で目標を共有し協力
- **競争的（Competitive）**: エージェント間で利益が対立（ゼロサムゲーム等）
- **混合環境（Mixed）**: 協調と競争が混在
- **ナッシュ均衡（Nash Equilibrium）**: どのエージェントも戦略を変更する動機がない状態
- **自己対戦（Self-Play）**: 自分自身のコピーと対戦して学習
- **AlphaGo**: Googleの囲碁AI、MARLの代表例
- **AlphaZero**: 汎用ゲームAI（囲碁、将棋、チェス）
- **OpenAI Five**: Dota 2のマルチエージェントAI
- **Communication**: エージェント間の通信・情報共有

### 詳細

#### 背景
通常の強化学習は単一エージェントと静的環境を前提とするが、現実世界では：
- **複数の意思決定主体**が存在（ロボットチーム、トレーディング、自動運転車群）
- **他エージェントの行動が環境を変化**させる
- **協調と競争が混在**する（味方と敵、協力者と競合他社）

これらを扱うのがマルチエージェント強化学習。

#### 基本構造

**単一エージェント vs マルチエージェント**:

```
【単一エージェント強化学習】
エージェント ←→ 環境（静的）
       ↓ 行動
     報酬 + 次状態

【マルチエージェント強化学習】
エージェント1 ←┐
エージェント2 ←┼→ 環境（動的）
   ...        ←┤
エージェントN ←┘
       ↓ 行動の組
   報酬群 + 次状態
```

**MARLの特徴**：
- **非定常性（Non-Stationarity）**: 他エージェントの学習により環境が動的に変化
- **スケーラビリティ**: エージェント数に応じて状態・行動空間が指数的増加
- **クレジット割当問題**: チーム報酬を個別エージェントに割り当てる困難

#### MARLの分類

**1. 協調的MARL（Cooperative MARL）**

**定義**: 全エージェントが共通の目標を持ち、協力して報酬を最大化。

**特徴**:
- 共通報酬関数（チーム全体の報酬を共有）
- 通信・情報共有が重要
- クレジット割当が課題

**例**:
- **ロボットサッカー**: 複数ロボットが協力してゴール
- **倉庫管理**: 複数ロボットが協調して荷物を運搬
- **ドローン群制御**: 編隊飛行、捜索救助

**代表手法**:
- **QMIX**: 個別Q関数を単調結合して共同行動価値を表現
- **COMA（Counterfactual Multi-Agent Policy Gradients）**: 他エージェントの行動を固定して個別貢献を評価
- **CommNet**: エージェント間の通信を学習

**2. 競争的MARL（Competitive MARL）**

**定義**: エージェント間で利益が対立、一方の利得が他方の損失（ゼロサムゲーム等）。

**特徴**:
- 対立する報酬関数
- 相手の戦略予測が鍵
- ナッシュ均衡の概念

**例**:
- **囲碁・将棋・チェス**: 二人対戦ゲーム
- **ポーカー**: 不完全情報ゲーム
- **ゲーム対戦AI**: Dota 2、StarCraft II

**代表手法**:
- **自己対戦（Self-Play）**: 自分のコピーと対戦して強化
- **Fictitious Self-Play**: 過去の方策とも対戦
- **PSRO（Policy-Space Response Oracles）**: 方策空間での応答学習

**3. 混合MARL（Mixed MARL）**

**定義**: 協調と競争が混在（チーム内協調、チーム間競争等）。

**例**:
- **チーム対戦ゲーム**: サッカーゲーム、Dota 2（5 vs 5）
- **市場シミュレーション**: 企業内協調、企業間競争
- **交通制御**: 個別車両の利益と全体最適化

#### 主要な手法

**1. 自己対戦（Self-Play）**

**概念**: エージェントが自分自身（または過去バージョン）と対戦して学習。

**プロセス**:
```
1. 初期方策 π₀ を用意
2. 現在方策 πᵢ vs πᵢ で対戦
3. 対戦結果から学習し πᵢ₊₁ に更新
4. πᵢ₊₁ vs πᵢ₊₁ で対戦
5. 繰り返し
```

**利点**:
- 人間の対戦相手不要
- 常に同レベルの相手と対戦可能
- 継続的な改善

**課題**:
- **Cycling問題**: 戦略が循環（じゃんけんのように）
- **Forgetting**: 過去の戦略に対する脆弱性

**解決策**:
- **リーグトレーニング**: 過去の方策も保存して多様な相手と対戦
- **優先ランク付き Fictitious Self-Play**: 強い方策を優先的に選択

**2. 独立学習（Independent Learning）**

**概念**: 各エージェントが他エージェントを環境の一部とみなし、独立に学習。

**手法**:
- **Independent Q-Learning（IQL）**: 各エージェントが独立にQ学習
- **Independent Actor-Critic**: 各エージェントが独立にActor-Critic

**利点**:
- シンプル、スケーラブル
- 個別実装が容易

**欠点**:
- **非定常性**: 他エージェントの学習で環境が変化
- **収束保証なし**: マルコフ性の破綻
- **協調困難**: 暗黙の協調しか不可能

**3. 集中学習・分散実行（Centralized Training, Decentralized Execution: CTDE）**

**概念**: 学習時は全エージェントの情報を利用（集中）、実行時は各エージェントが独立判断（分散）。

**手法例: QMIX**
- **学習時**: 全エージェントの観測・行動履歴を使用
- **実行時**: 各エージェントは自分の観測のみで行動選択
- **単調性制約**: 個別Q関数の和が共同Q関数を単調に表現

```
Q_tot(s, a₁,...,aₙ) = f(Q₁(o₁,a₁), Q₂(o₂,a₂), ..., Qₙ(oₙ,aₙ))
※ f は単調な結合関数
```

**利点**:
- 学習時に全体情報で安定学習
- 実行時は分散制御（通信不要）

**欠点**:
- 学習時の計算・通信コスト

#### 代表的なプログラム

**1. AlphaGo（2016年）**

**概要**: GoogleのDeepMindが開発した囲碁AI。2016年に世界トップ棋士イ・セドルに勝利。

**技術**:
- **深層強化学習**: 価値ネットワーク + 方策ネットワーク
- **モンテカルロ木探索（MCTS）**: 探索と学習の組み合わせ
- **自己対戦**: 数百万局の自己対戦で強化
- **教師あり学習**: プロ棋士の棋譜から初期方策学習

**学習プロセス**:
```
1. 教師あり学習: プロ棋譜で方策ネットワーク初期化
2. 強化学習: 自己対戦で方策改善
3. 価値ネットワーク学習: 局面評価
4. MCTS統合: 探索と評価の組み合わせ
```

**マルチエージェント側面**:
- 対戦相手も学習する（競争的MARL）
- 自己対戦で多様な戦略に対応

**2. AlphaZero（2017年）**

**概要**: AlphaGoの汎用版。囲碁・将棋・チェスでトップレベルに到達。

**AlphaGoからの進化**:
- **ゼロから学習**: 人間の棋譜不要、完全自己対戦のみ
- **汎用性**: ゲームルールのみ入力で複数ゲームに対応
- **単一ネットワーク**: 方策と価値を統合ネットワークで出力

**学習時間**:
- 囲碁: 40日間（4.9百万局）
- 将棋: 9時間
- チェス: 34時間

**3. AlphaStar（2019年）**

**概要**: DeepMindのStarCraft IIプロレベルAI。

**技術**:
- **Imitation Learning**: プロプレイヤーの行動から初期学習
- **Multi-Agent RL**: リーグトレーニング（過去バージョンと対戦）
- **長期戦略学習**: 数分～数十分の長期ゲーム

**マルチエージェント特性**:
- 複数ユニットの協調制御
- 相手戦略の予測と対応

**4. OpenAI Five（2018年）**

**概要**: OpenAIのDota 2プロレベルAI（5 vs 5チーム戦）。

**技術**:
- **PPO（Proximal Policy Optimization）**: 安定した方策学習
- **Large-Scale Training**: 数万CPUコアで並列学習（1日あたり180年分のプレイ経験）
- **Long-Term Credit Assignment**: 45分ゲームでの長期戦略

**マルチエージェント特性**:
- **5エージェントの協調**: チーム内での役割分担
- **競争的学習**: 相手チームとの対戦
- **通信なし**: 各エージェントは観測のみで判断

**学習規模**:
- 学習期間: 10ヶ月
- 計算量: 800ペタフロップス日
- ゲーム経験: 数万年分

**5. その他の代表例**

- **DeepMind Lab Football**: サッカーゲームの協調学習
- **Pluribus（2019年）**: 6人制ポーカーで人間プロに勝利（不完全情報ゲーム）
- **OpenAI Dactyl（2018年）**: ロボットハンドによる物体操作（実世界MARL）

#### 図解（MARLの構造）

```
【協調的MARL例: ロボットサッカー】

エージェント1 → 行動1 ┐
エージェント2 → 行動2 ├→ 環境（ボール、相手）
エージェント3 → 行動3 ┘
         ↓
   共通報酬（ゴール成功度）
         ↓
   各エージェントが学習


【競争的MARL例: 囲碁（AlphaGo）】

黒エージェント → 着手 ┐
                      ├→ 盤面（環境）
白エージェント → 着手 ┘
         ↓
   対立報酬（勝敗）
   黒: +1/-1, 白: -1/+1
         ↓
   自己対戦で学習
```

#### MARLの課題

**1. 非定常性（Non-Stationarity）**
- 他エージェントの学習で環境が動的変化
- 単一エージェントRLの収束理論が適用不可

**2. スケーラビリティ**
- エージェント数 $n$ に対し、状態空間 $O(|S|^n)$、行動空間 $O(|A|^n)$
- 計算・通信コストが指数的増加

**3. クレジット割当問題（Credit Assignment）**
- チーム報酬から個別貢献度を判定困難
- 誰の行動が成功/失敗の原因か不明

**4. 部分観測性（Partial Observability）**
- 各エージェントは環境の一部のみ観測
- 他エージェントの意図・観測が不明

**5. 通信制約**
- 実環境では通信帯域・遅延の制約
- どの情報を共有すべきか

#### 試験での問われ方

**典型問題**：「複数プレーヤーが存在し協調的・競争的な関係を考慮しながら学習を行う強化学習を（A）と呼び、この手法を用いた代表的なプログラムとして（B）がある。」

**解答**:
- **(A)**: **マルチエージェント強化学習**（Multi-Agent Reinforcement Learning, MARL）
- **(B)**: **AlphaGo**、AlphaZero、OpenAI Five、AlphaStar等

**キーワード認識**:
- 「**複数エージェント**」「**複数プレーヤー**」→ マルチエージェント
- 「**協調的**」「**競争的**」→ MARL の特徴
- 「**自己対戦**」→ 競争的MARLの手法
- 「**AlphaGo**」→ 囲碁AI、競争的MARLの代表

**選択肢問題例**：

**問**: 以下のうち、マルチエージェント強化学習の代表的なプログラムとして最も適切でないものを選べ。
- A. AlphaGo（囲碁AI）
- B. AlphaZero（汎用ゲームAI）
- C. DQN（Atari 2600ゲーム）
- D. OpenAI Five（Dota 2）

**正解**: C（DQNは単一エージェント強化学習）

**判別ポイント**:
- ✅ AlphaGo、AlphaZero、OpenAI Five → **対戦・協調が必要**（MARL）
- ❌ DQN → **単一プレーヤー**のAtariゲーム（単一エージェントRL）

**ひっかけポイント**:
- DQNも「ゲームAI」だが、**対戦相手は固定プログラム**（学習しない）
- MARLは「**複数の学習エージェント**」が必須
- AlphaGoは「**深層強化学習**」でもあるが、問われているのは「**マルチエージェント**」の側面

**比較表（試験頻出）**:

| 項目 | 単一エージェントRL | マルチエージェントRL |
|------|-------------------|---------------------|
| **エージェント数** | 1 | 2以上 |
| **環境** | 静的 | 動的（他エージェントの影響） |
| **目標** | 個別最適化 | 協調/競争 |
| **非定常性** | なし | あり（他エージェントの学習） |
| **代表例** | DQN（Atari） | AlphaGo（囲碁） |
| **学習手法** | Q学習、方策勾配 | 自己対戦、CTDE |
| **応用** | ロボット単体制御 | チーム制御、対戦ゲーム |

### 実例

**例1: AlphaGo vs イ・セドル（2016年）**
- **問題**: 世界トップ棋士に勝てるか
- **手法**: 教師あり学習 + 自己対戦強化学習 + MCTS
- **結果**: 4勝1敗、囲碁AIが人間を超えた歴史的瞬間

**例2: OpenAI Five vs プロチーム（2018年）**
- **問題**: 5人協調型ゲームで人間に勝てるか
- **手法**: PPO + 大規模並列学習（180年/日の経験）
- **結果**: プロチームに勝利、協調学習の実証

**例3: 倉庫ロボット群制御**
- **問題**: 複数ロボットが効率的に荷物を運搬
- **手法**: 協調的MARL（QMIX等）、通信あり/なし
- **結果**: 人間管理より高効率、衝突回避も実現

### 補足

**実務での注意点**:
- **計算コスト**: 大規模学習には膨大なリソース必要（AlphaGo級は数百GPU月）
- **Sim-to-Real Gap**: シミュレータでの学習を実環境に転移する困難
- **安全性**: 協調失敗や予期しない創発行動のリスク

**今後の発展**:
- **説明可能性**: なぜその行動を選択したか説明
- **少量データ学習**: サンプル効率の改善
- **汎用MARL**: ゲーム以外の実世界問題（交通、経済、ロボティクス）

**関連トピック**:
- [強化学習基礎](reinforcement_learning.md#定義) - 単一エージェントRL
- [ゲーム理論](../10_math_statistics/) - ナッシュ均衡等
- [ロボティクス](../07_ai_applications/) - 実世界応用

---

### 最新動向
- **モデルベース強化学習**: 環境モデルを学習
- **逆強化学習**: 専門家の行動から報酬関数を推定
- **オフライン強化学習**: 事前収集データのみで学習
- **Sim-to-Real転移**: シミュレータから実環境への方策転移（ドメインランダマイゼーション等）
- **大規模言語モデル統合**: LLMを方策・報酬設計に活用

---

## 分散型強化学習（Distributed Reinforcement Learning）★試験頻出

### 要点
- **複数の計算資源（ワーカー）を並列活用**して学習を高速化する強化学習のアーキテクチャ
- 各ワーカーが独立に環境と相互作用し、**経験を共有または統合**してパラメータを更新
- A3C、Ape-X、IMPALAなどが代表的手法。学習時間を大幅に短縮可能

### 定義
**分散型強化学習（Distributed Reinforcement Learning）**とは、複数のワーカー（エージェント）を並列に実行し、各ワーカーが収集した経験を統合または共有することで、学習を高速化・安定化する強化学習のフレームワーク。計算リソースを分散活用し、データ収集と学習の効率を向上させる。

### 重要キーワード
- **ワーカー（Worker）**: 環境と相互作用して経験を収集する並列実行エージェント
- **並列化（Parallelization）**: 複数プロセス・スレッドでの同時実行
- **経験の共有**: ワーカー間で収集した遷移データ（状態・行動・報酬）を統合
- **非同期更新（Asynchronous Update）**: ワーカーが独立にパラメータを更新（A3C）
- **同期更新（Synchronous Update）**: 全ワーカーの経験を集約後に一括更新
- **A3C（Asynchronous Advantage Actor-Critic）**: 非同期分散RLの代表手法
- **Ape-X（Distributed Prioritized Experience Replay）**: 優先度付き経験再生の分散版
- **IMPALA（Importance Weighted Actor-Learner Architecture）**: 大規模分散RL

---

### 詳細

#### 分散型強化学習の背景

**従来の課題**:
- 単一エージェントの学習は**時間がかかる**（数百万ステップ必要）
- サンプル効率が低く、環境との相互作用が律速
- 1つのGPUでは計算リソースが不足

**分散化の利点**:
- ✅ **学習時間の短縮**: 並列実行で経験収集を高速化
- ✅ **探索の多様化**: 複数ワーカーが異なる方策で探索
- ✅ **安定性向上**: 多様な経験で学習のばらつきを抑制
- ✅ **スケーラビリティ**: ワーカー数を増やして性能向上

#### 基本アーキテクチャ

**構成要素**:

1. **ワーカー（Actor）**: 
   - 環境と相互作用し、経験（$s, a, r, s'$）を収集
   - 各ワーカーは独立した環境インスタンスを持つ
   - 複数のCPUコア・マシンで並列実行

2. **学習器（Learner）**:
   - ワーカーから集約した経験でパラメータを更新
   - GPU等で高速計算
   - 更新後のパラメータをワーカーに配布

3. **経験バッファ（オプション）**:
   - ワーカーが収集した経験を一時保存
   - 優先度付き再生（Prioritized Replay）等で効率化

**図解**:
```
┌──────────┐  経験   ┌──────────┐
│ ワーカー1 │────────→│          │
│(環境A)   │←────────│          │
└──────────┘ パラメータ│          │
                     │  学習器   │ (GPU)
┌──────────┐  経験   │ (Learner) │
│ ワーカー2 │────────→│          │
│(環境B)   │←────────│          │
└──────────┘ パラメータ│          │
                     └──────────┘
┌──────────┐  経験        ↑
│ ワーカーN │────────────→│
│(環境N)   │←────────────│
└──────────┘ パラメータ  勾配計算・更新
```

#### 代表的手法

**1. A3C（Asynchronous Advantage Actor-Critic）** ★試験超重要

**提案**: DeepMind（2016年）

**特徴**:
- **非同期更新**: 各ワーカーが独立に大域パラメータを更新
- **共有パラメータ**: 全ワーカーがメモリ上の同一パラメータを参照
- **経験再生なし**: 各ワーカーがオンラインで学習

**アルゴリズム**:
```
1. 大域パラメータ θ を初期化
2. 各ワーカーを並列起動:
   a. ローカルパラメータ θ' = θ（コピー）
   b. 環境で n ステップ実行し経験収集
   c. Advantage を計算
   d. 勾配 ∇θ を計算
   e. 大域パラメータを非同期更新: θ ← θ + α∇θ
   f. 繰り返し
```

**利点**:
- ✅ 経験バッファ不要（メモリ効率）
- ✅ 多様な方策で探索
- ✅ CPUのみで高速学習可能

**欠点**:
- ❌ 非同期更新で学習が不安定になる可能性
- ❌ 同期版（A2C）の方が安定する場合も

**2. Ape-X（Distributed Prioritized Experience Replay）**

**提案**: DeepMind（2018年）

**特徴**:
- **分散データ収集**: 多数のワーカーが並列に経験を収集
- **集中学習**: 1つの学習器がGPUで高速更新
- **優先度付き経験再生**: 重要な経験を優先的に学習

**構成**:
```
ワーカー1 ─┐
ワーカー2 ─┼→ 共有経験バッファ → 学習器（GPU） → パラメータ配布
ワーカーN ─┘     (優先度付き)
```

**利点**:
- ✅ データ収集と学習を分離（効率的）
- ✅ GPU1台で大規模学習可能
- ✅ 安定した更新

**3. IMPALA（Importance Weighted Actor-Learner Architecture）**

**提案**: DeepMind（2018年）

**特徴**:
- **超大規模並列**: 数百～数千のワーカー
- **オフポリシー補正**: 重要度サンプリングで方策のズレを補正
- **高スループット**: V-trace等の効率的アルゴリズム

**スケール**:
- ワーカー数: 100～1000+
- 学習速度: 従来の10倍以上

---

### 試験での問われ方（★超重要）

#### 典型的な選択肢問題

**問**: 分散型強化学習の特徴として、最も**適切**な選択肢を1つ選べ。

**A. 単一のエージェントが逐次的に学習するため、安定性が高い**
- ❌ 不適切。「単一」「逐次的」が誤り → 分散型は**複数ワーカーを並列実行**

**B. 複数の計算資源を並列活用し、学習時間を短縮できる**
- ⭕ **正解**。分散型RLの最大の特徴

**C. 環境との相互作用を一切行わず、既存データのみで学習する**
- ❌ 不適切。これは**オフライン強化学習**の説明

**D. エージェント間の競争や協調を扱う手法である**
- ❌ 不適切。これは**マルチエージェント強化学習（MARL）**の説明

**正解**: **B**

---

#### 判別フローチャート

```
Q: 並列計算資源を活用？
    ↓ YES → 分散型強化学習（A3C、Ape-X等）
    ↓ NO
Q: 複数の学習エージェント？
    ↓ YES → マルチエージェント強化学習（AlphaGo等）
    ↓ NO
Q: 事前収集データのみ？
    ↓ YES → オフライン強化学習
    ↓ NO
    → 標準的な強化学習（DQN等）
```

---

#### ひっかけポイント（試験頻出）

**混同しやすい概念**:

| 概念 | 並列化 | エージェント数 | 学習の焦点 | 代表例 |
|------|--------|--------------|----------|--------|
| **分散型RL** | ⭕複数ワーカー | 1（方策は共有） | **学習高速化** | A3C、Ape-X |
| **マルチエージェントRL** | - | **2以上（独立方策）** | **協調・競争** | AlphaGo、OpenAI Five |
| **オフライン強化学習** | - | 1 | **既存データ活用** | Batch RL |
| **アンサンブル学習** | ⭕複数モデル | 1 | **予測精度向上** | ランダムフォレスト |

**重要な区別**:

1. **分散型RL vs マルチエージェントRL**:
   - 分散型: **同じ方策を並列学習**（ワーカーは共有パラメータのコピー）
   - マルチエージェント: **異なる方策を独立学習**（各エージェントが独自目的）

2. **分散型RL vs アンサンブル学習**:
   - 分散型RL: **1つの方策を複数ワーカーで学習**
   - アンサンブル: **複数の独立モデルを組み合わせて予測**

3. **A3C vs AlphaGo**:
   - A3C: **分散型RL**（並列ワーカーで学習高速化）
   - AlphaGo: **マルチエージェントRL**（自己対戦で協調・競争学習）
   - ※AlphaGoも内部で分散学習を使用するが、主眼は「対戦による学習」

**誤答パターン**:
- ❌「複数エージェントが競争」→ マルチエージェントRLと混同
- ❌「既存データのみで学習」→ オフライン強化学習と混同
- ❌「複数モデルの予測を統合」→ アンサンブル学習と混同

**正解パターン**:
- ⭕「複数の計算資源を並列活用」
- ⭕「学習時間を短縮」
- ⭕「ワーカーが経験を共有」
- ⭕「A3Cが代表例」

---

### 実例

**例1: OpenAI Dota 2（分散学習の活用）**
- **規模**: 128,000 CPUコア、256 GPU使用
- **学習時間**: 10か月相当の経験を10日で収集
- **手法**: PPOを大規模分散化
- **結果**: プロレベルのチーム協調戦略を獲得

**例2: AlphaStar（StarCraft II）**
- **並列学習**: 数百台のマシンで分散学習
- **学習量**: 人間の200年分のプレイ経験
- **手法**: 自己対戦 + 分散Actor-Learnerアーキテクチャ

**例3: Google Dopamine（研究用フレームワーク）**
- **目的**: 分散RLの研究・実験
- **機能**: A3C、Ape-X等の実装を提供
- **利点**: 小規模でも分散学習を試せる

---

### 補足

#### 実務での選択基準

**分散型RLを使うべき場面**:
- ✅ 学習時間を短縮したい（締め切りがある）
- ✅ 複数のCPU/GPUリソースがある
- ✅ 探索の多様性が重要（多様な初期条件）

**使わないほうが良い場面**:
- ❌ 環境が非常に高速（並列化のオーバーヘッドが無駄）
- ❌ リソースが限定的（1CPU/1GPUのみ）
- ❌ デバッグ・検証段階（シンプルな単一エージェントで十分）

#### 技術的課題

1. **同期オーバーヘッド**: ワーカー間の通信コスト
2. **パラメータ競合**: 非同期更新での整合性
3. **環境の違い**: 各ワーカーの環境設定を統一する必要
4. **スケーラビリティ限界**: ワーカー数を増やしても効果が飽和

#### 今後の発展

- **階層的分散学習**: マルチレベルでの並列化
- **エッジコンピューティング**: 分散デバイスでのRL
- **フェデレーテッドRL**: プライバシー保護しながら分散学習

---

### 定義（再掲）

**分散型強化学習（Distributed Reinforcement Learning）**:
- 複数のワーカーを並列実行し、経験を統合して学習を高速化する強化学習のアーキテクチャ
- 単一の方策を複数の計算リソースで効率的に学習
- 代表手法：A3C、Ape-X、IMPALA
