# 特徴量エンジニアリング（Feature Engineering）

## 要点
- 生データから機械学習モデルに有用な特徴量を生成・選択・変換するプロセス
- **次元の呪い**：次元増加でデータが疎になり必要データ量が指数増加、過学習リスク上昇
- 対策：特徴選択、次元削減（PCA等）、ドメイン知識活用、正則化

## 定義
特徴量エンジニアリングは、生データを機械学習アルゴリズムが効果的に学習できる形式に変換するプロセス。**特徴量生成・選択・抽出・変換**を含み、モデル性能を大きく左右する。

---

## 特徴量エンジニアリングの3つの主要手法

### 1. 特徴量生成（Feature Generation）
**定義**：既存データから新しい特徴量を作成する

**例**：
- 複数特徴の組み合わせ：面積 = 縦 × 横
- 多項式特徴：$x^2$, $x^3$
- 集約統計：平均、分散、最大値、最小値
- 時系列特徴：移動平均、差分、ラグ特徴

**具体例（不動産価格予測）**：
```
元データ: 面積、築年数
↓ 特徴量生成
新特徴: 築年数の2乗、面積×築年数、1m²あたり価格
```

---

### 2. 特徴選択（Feature Selection）
**定義**：既存の特徴量から重要なものを選ぶ

**手法**：
- **フィルタ法**: 相関係数、相互情報量で評価
- **ラッパー法**: モデル性能を基準に選択
- **埋め込み法**: Lasso（L1正則化）で自動選択

**例**：
```
100個の特徴 → 重要度評価 → 上位10個を選択
```

**利点**：
- 次元削減（次元の呪い対策）
- 過学習防止
- 解釈性向上

---

### 3. 特徴量抽出（Feature Extraction）★試験頻出
**定義**：生データから有用な特徴を**計算・変換**して抽出する処理

**特徴選択との違い**：
| 項目 | 特徴選択 | 特徴量抽出 |
|------|---------|-----------|
| 操作 | 既存から選ぶ | 変換・生成して抽出 |
| 元特徴 | 保持される | 変換される |
| 次元 | 減少 | 通常は減少 |
| 解釈性 | 高い（元の特徴） | 低い（変換後） |

**代表的手法**：

**1. PCA（主成分分析）**
- 分散を最大化する軸で次元圧縮
- 100次元 → 10次元（情報の90%保持）

**2. オートエンコーダ**
- 深層学習による非線形特徴抽出
- 入力→圧縮→復元の中間層を特徴として使用

**3. 事前学習モデルの中間層**
- CNNの畳み込み層出力を特徴として使用
- BERTの埋め込み層を特徴として使用

**4. 音声・画像の特徴抽出**
- MFCC（音声のメル周波数ケプストラム係数）
- HOG（画像の勾配方向ヒストグラム）
- SIFT（スケール不変特徴変換）

**具体例（画像分類）**：
```
元データ: 224×224×3のピクセル値（150,528次元）
↓ CNNで特徴抽出
抽出特徴: 2048次元のベクトル
↓ 分類器（SVM等）
予測
```

---

### 特徴量エンジニアリングの比較表

| 手法 | 操作 | 入力 | 出力 | 用途 |
|------|------|------|------|------|
| **特徴量生成** | 新規作成 | 既存特徴 | 追加特徴 | 表現力向上 |
| **特徴選択** | 選択 | 既存特徴 | 一部特徴 | 次元削減、過学習防止 |
| **特徴量抽出** | 変換 | 生データ | 変換特徴 | 次元削減、有用特徴獲得 |

---

## 試験での問われ方（特徴量抽出）

### 典型的な出題パターン

**パターン1：特徴量抽出の説明として最も不適切な選択肢を選べ**

✅ **適切な選択肢（正しい説明）**：
- **「生データから有用な特徴を計算・変換して抽出する処理」** → ✅ 正しい
- **「PCAや自己符号化器を用いて次元削減を行う」** → ✅ 正しい
- **「画像や音声から特徴ベクトルを生成する」** → ✅ 正しい
- **「元のデータを変換して低次元表現を得る」** → ✅ 正しい
- **「CNNの中間層出力を特徴として利用する」** → ✅ 正しい
- **「MFCCやHOGなどの特徴量を計算する」** → ✅ 正しい

❌ **不適切な選択肢（誤った説明、試験で狙われる）**：
- **「既存の特徴量から重要なものを選択する処理」** → ❌ **最も不適切**（これは特徴選択の説明）
- **「特徴量を削除せずに全て使用する」** → ❌ 不適切（次元削減が目的）
- **「元のデータをそのまま保持する」** → ❌ 不適切（変換が目的）
- **「データ拡張の一種である」** → ❌ 不適切（別の手法）
- **「特徴量の数を増やす手法」** → ❌ 不適切（通常は減らす）

---

**パターン2：特徴選択と特徴量抽出の違い**

> 「特徴選択と特徴量抽出の違いとして、最も適切な選択肢を1つ選べ。」

✅ **適切な選択肢**：
- **「特徴選択は既存から選ぶ、特徴量抽出は変換して生成する」** → ✅ **最も適切**
- **「特徴選択は解釈性が高い、特徴量抽出は解釈性が低い」** → ✅ 適切
- **「特徴選択は元の特徴を保持、特徴量抽出は新しい特徴を作る」** → ✅ 適切

❌ **不適切な選択肢**：
- **「特徴選択と特徴量抽出は同じ意味」** → ❌ 不適切（別の手法）
- **「特徴量抽出は元の特徴をそのまま選ぶ」** → ❌ 不適切（変換する）

---

**パターン3：特徴量抽出の具体例**

**特徴量抽出に該当する例**：
- ✅ PCAで100次元→10次元に圧縮
- ✅ オートエンコーダの中間層を使用
- ✅ CNNで画像から特徴ベクトルを抽出
- ✅ 音声からMFCCを計算
- ✅ 画像からHOG特徴を計算

**特徴選択に該当する例**（混同注意）：
- ❌ 100個の特徴から重要な10個を選択
- ❌ Lassoで不要な特徴を削除
- ❌ 相関係数で重要特徴を選定

---

### 引っ掛けポイント

| ひっかけ | 正しい理解 |
|----------|------------|
| ❌ 特徴量抽出 = 特徴選択 | ✅ 別の手法（変換 vs 選択） |
| ❌ 既存特徴から選ぶのが特徴量抽出 | ✅ それは特徴選択、抽出は変換 |
| ❌ 次元が増える | ✅ 通常は次元削減 |
| ❌ 元のデータを保持 | ✅ 変換後のデータを使用 |
| ❌ 解釈性が高い | ✅ 変換後は解釈性低下（PCA等） |

---

## 次元の呪い（Curse of Dimensionality）

### 定義
**次元の呪い**は、特徴量の数（次元）が増加すると、データが高次元空間に疎に分布し、機械学習モデルの性能が悪化する現象。1957年にリチャード・ベルマンが提唱。

### 主な問題点

#### 1. データの疎密性（Sparsity）
高次元空間では、データ点間の距離が広がり、データが「疎」に分布：

**具体例**（2次元 vs 10次元）：
- **2次元**：100個のデータ点で空間を10×10のグリッドでカバー可能
- **10次元**：同じ密度を保つには $10^{10}$ = **100億個**のデータ点が必要

**必要データ量**：
$$N \propto k^d$$
- $N$: 必要なサンプル数
- $k$: 各次元の分割数
- $d$: 次元数（指数関数的に増加）

#### 2. 距離の意味の喪失
高次元では全ての点の距離が均一化され、「近い」「遠い」の区別が困難に：

**距離の均一化**：
```
低次元（2D）：
点A-点B: 距離3
点A-点C: 距離10
→ 明確な差（3.3倍）

高次元（100D）：
点A-点B: 距離50.2
点A-点C: 距離51.8
→ わずかな差（1.03倍のみ）
```

**数学的説明**：
- 最近傍と最遠点の距離比が1に収束：$\frac{d_{max}}{d_{min}} \to 1$
- k-NNなどの距離ベースアルゴリズムが機能不全

#### 3. 過学習のリスク増加
特徴量が多いほど、モデルが訓練データのノイズまで学習：

**パラメータ数の増加**：
- 入力次元100 → 全結合層（100ユニット）: **10,000パラメータ**
- 入力次元1000 → 全結合層（100ユニット）: **100,000パラメータ**

**データ対パラメータ比**：
- データ1,000個、パラメータ10,000個 → 過学習リスク大
- 「パラメータ数 < データ数」が経験則

#### 4. 計算コストの増加
次元数に応じて計算量が増加：

**計算量の例**：
- **距離計算**：$O(d)$（d次元）
- **k-NN探索**：$O(nd)$（n個のデータ、d次元）
- **SVMのカーネル計算**：$O(n^2 d)$

### 次元の呪いの影響を受けやすいアルゴリズム

| アルゴリズム | 影響度 | 理由 |
|-------------|-------|------|
| **k-NN** | ❌ 非常に大 | 距離ベース、高次元で距離が無意味化 |
| **決定木** | △ 中程度 | 分岐で次元を分割、深い木で過学習 |
| **SVM（RBFカーネル）** | ❌ 大 | 距離ベース、高次元で計算コスト増大 |
| **線形回帰** | △ 中程度 | 多重共線性、過学習のリスク |
| **ランダムフォレスト** | ○ 小～中 | 特徴量ランダム選択で緩和 |
| **深層学習** | ○ 小 | 階層的特徴抽出、高次元に強い |
| **正則化回帰（Lasso/Ridge）** | ○ 小 | 正則化で過学習抑制 |

### 次元の呪いへの対策

#### 1. 次元削減（Dimensionality Reduction）
**主成分分析（PCA）**：
- 分散を最大化する軸を見つけて次元圧縮
- 100次元 → 10次元（情報の90%保持）

**t-SNE、UMAP**：
- 非線形次元削減、可視化に有効

**オートエンコーダ**：
- 深層学習による非線形次元削減

#### 2. 特徴選択（Feature Selection）
**フィルタ法**：
- 相関係数、相互情報量で重要特徴を選択

**ラッパー法**：
- モデル性能を評価しながら選択（後退選択、前進選択）

**埋め込み法**：
- Lasso（L1正則化）で自動的に不要特徴を削除

#### 3. 正則化（Regularization）
**L1正則化（Lasso）**：
- 重みをゼロにして実質的に次元削減
- $\min \|y - Xw\|^2 + \lambda \|w\|_1$

**L2正則化（Ridge）**：
- 重みを小さく保つ
- $\min \|y - Xw\|^2 + \lambda \|w\|^2$

#### 4. ドメイン知識の活用
- 専門知識で有用な特徴量を事前選択
- 物理法則、業務知識に基づく特徴量生成

#### 5. アンサンブル手法
**ランダムフォレスト**：
- 各決定木で特徴量をランダム選択
- 特徴量重要度で事後評価

### 次元の呪いの逆説（高次元の利点）

**高次元の利点（場合による）**：
- **表現力の向上**：複雑なパターンを表現可能
- **分離可能性の向上**：高次元では線形分離しやすい（カバーの定理）
- **深層学習の成功**：適切な構造で高次元を扱える

**カバーの定理（Cover's Theorem）**：
- 高次元空間では、データが線形分離可能になる確率が高まる
- ただし、データ量が十分に必要

---

## 重要キーワード
- **次元の呪い（Curse of Dimensionality）**: 次元増加でデータが疎になり必要データ量が指数増加する現象
- **データの疎密性（Sparsity）**: 高次元空間でデータ点が疎に分布する状態
- **距離の均一化**: 高次元で全ての点の距離が等しく見える現象
- **次元削減（Dimensionality Reduction）**: 高次元データを低次元に圧縮する手法
- **特徴選択（Feature Selection）**: 重要な特徴量のみを選択する手法
- **PCA（主成分分析）**: 分散を最大化する軸で次元削減
- **L1正則化（Lasso）**: 不要な特徴量の重みをゼロにする正則化
- **カバーの定理（Cover's Theorem）**: 高次元では線形分離可能性が向上する定理
- **特徴量エンジニアリング（Feature Engineering）**: 特徴量の生成・選択・変換の総称
- **特徴量生成（Feature Generation）**: 新しい特徴量を作成するプロセス
- **特徴量変換（Feature Transformation）**: 既存特徴量を変換するプロセス

---

## 試験での問われ方

### 典型的な出題パターン

#### パターン1：選択肢問題（不適切な説明を選ぶ）
> 「次元の呪いの説明として、最も不適切な選択肢を1つ選べ。」

✅ **適切な選択肢（正しい説明）**：
- **「次元が増えるとデータが空間に疎に分布する」** → ✅ 正しい
- **「必要なデータ量が次元数に対して指数関数的に増加する」** → ✅ 正しい
- **「高次元空間では、距離の概念が意味を失う」** → ✅ 正しい
- **「過学習のリスクが高まる」** → ✅ 正しい
- **「PCA等の次元削減が有効な対策である」** → ✅ 正しい
- **「計算コストが次元数に応じて増加する」** → ✅ 正しい
- **「k-NNなどの距離ベースアルゴリズムが影響を受けやすい」** → ✅ 正しい

❌ **不適切な選択肢（誤った説明、試験で狙われる）**：
- **「次元（特徴量）が増えれば増えるほど、モデルの精度は必ず向上する」** → ❌ **最も不適切**（逆に悪化する）
- **「高次元空間では、データ量が少なくても十分に学習できる」** → ❌ 不適切（より多くのデータが必要）
- **「次元削減は不要で、全ての特徴量を使うべきである」** → ❌ 不適切（次元削減は有効）
- **「高次元空間では、距離の概念がより正確になる」** → ❌ 不適切（距離が無意味化）
- **「計算コストは次元数に関係なく一定である」** → ❌ 不適切（増加する）
- **「全ての機械学習アルゴリズムが同程度に影響を受ける」** → ❌ 不適切（影響度は異なる）

#### パターン2：対策に関する問題
> 「次元の呪いへの対策として、最も適切な選択肢を1つ選べ。」

✅ **適切な選択肢**：
- **「PCA（主成分分析）で次元削減を行う」** → ✅ 最も適切
- **「L1正則化（Lasso）で不要な特徴量を削除する」** → ✅ 適切
- **「特徴選択で重要な特徴量のみを使用する」** → ✅ 適切
- **「ドメイン知識を活用して有用な特徴量を選ぶ」** → ✅ 適切

❌ **不適切な選択肢**：
- **「特徴量を可能な限り増やす」** → ❌ 不適切（逆効果）
- **「データ量を減らす」** → ❌ 不適切（増やすべき）

### 比較されやすい概念
- **次元削減 vs 特徴選択**: 変換して圧縮 vs 選んで削除
- **PCA vs t-SNE**: 線形・全体構造保持 vs 非線形・局所構造保持
- **L1正則化 vs L2正則化**: スパース化（ゼロに） vs 小さく保つ
- **高次元の問題 vs 高次元の利点**: データ疎密・距離無意味化 vs 表現力・分離可能性
- **次元の呪い vs 過学習**: 原因の一つ vs 結果の症状

### 引っ掛けポイント

| ひっかけ | 正しい理解 |
|----------|------------|
| ❌ 次元が多いほど精度向上 | ✅ 過学習で精度低下する場合が多い |
| ❌ データ量は関係ない | ✅ 指数関数的にデータ量が必要 |
| ❌ 距離がより正確に | ✅ 距離が無意味化する |
| ❌ 次元削減は常に不要 | ✅ 有効な対策の一つ |
| ❌ 全てのアルゴリズムが同等に影響 | ✅ k-NN等の距離ベースが特に影響大 |
| ❌ 高次元は常に悪い | ✅ 適切な手法で扱えば利点もある |
| ❌ 計算コスト不変 | ✅ 次元数に応じて増加 |

### 頻出の数値・公式
- **必要データ量**：$N \propto k^d$（指数関数的増加）
- **PCAの分散寄与率**：累積90%以上が目安
- **距離の均一化**：$\frac{d_{max}}{d_{min}} \to 1$（d→∞）

---

## 実例

### 画像認識（高次元データ）
- **100×100ピクセルのグレースケール画像**：10,000次元
- **対策**：CNNで階層的特徴抽出、PCAで数百次元に削減

### ゲノムデータ解析
- **遺伝子数**：数万次元
- **サンプル数**：数百～数千
- **対策**：特徴選択で重要遺伝子のみ使用、正則化回帰

### テキスト分類（Bag-of-Words）
- **語彙数**：数万～数十万次元
- **対策**：TF-IDF、Word2Vec、BERTで低次元埋め込み

### 具体例：k-NNでの次元の呪い
```
2次元データ（100個）：
- 最近傍距離: 平均2.0
- 最遠点距離: 平均10.0
- 距離比: 5.0倍（明確な区別）

100次元データ（100個）：
- 最近傍距離: 平均50.2
- 最遠点距離: 平均51.8
- 距離比: 1.03倍（ほぼ区別不能）
```

---

## 補足

### 実務的観点
- **特徴量エンジニアリングの重要性**：「データと特徴量がアルゴリズムの性能の80%を決める」
- **ドメイン知識の活用**：業務知識で有用な特徴量を優先
- **自動特徴量生成**：FeatureToolsなどのツール活用
- **特徴量の解釈性**：ビジネス判断には解釈可能な特徴量を残す

### 深層学習時代の変化
- **表現学習**：深層学習が自動的に有用な特徴を学習
- **End-to-End学習**：手動の特徴量エンジニアリングが不要に
- **ただし**：データ量が十分にある場合のみ有効

### 関連トピック
- [次元削減（PCA）](unsupervised_learning.md) - 主成分分析の詳細
- [正則化](overfitting_underfitting.md) - L1/L2正則化の詳細
- [評価指標](evaluation_metrics.md) - モデル性能の評価
- [過学習対策](overfitting_underfitting.md) - 過学習と次元の呪いの関係
