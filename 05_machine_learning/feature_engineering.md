# 特徴量エンジニアリング（Feature Engineering）

## 要点
- 生データから機械学習モデルに有用な特徴量を生成・選択・変換するプロセス
- **次元の呪い**：次元増加でデータが疎になり必要データ量が指数増加、過学習リスク上昇
- 対策：特徴選択、次元削減（PCA等）、ドメイン知識活用、正則化

## 定義
特徴量エンジニアリングは、生データを機械学習アルゴリズムが効果的に学習できる形式に変換するプロセス。**特徴量生成・選択・抽出・変換**を含み、モデル性能を大きく左右する。

---

## 特徴量エンジニアリングの3つの主要手法

### 1. 特徴量生成（Feature Generation）
**定義**：既存データから新しい特徴量を作成する

**例**：
- 複数特徴の組み合わせ：面積 = 縦 × 横
- 多項式特徴：$x^2$, $x^3$
- 集約統計：平均、分散、最大値、最小値
- 時系列特徴：移動平均、差分、ラグ特徴

**具体例（不動産価格予測）**：
```
元データ: 面積、築年数
↓ 特徴量生成
新特徴: 築年数の2乗、面積×築年数、1m²あたり価格
```

---

### 2. 特徴選択（Feature Selection）
**定義**：既存の特徴量から重要なものを選ぶ

**手法**：
- **フィルタ法**: 相関係数、相互情報量で評価
- **ラッパー法**: モデル性能を基準に選択
- **埋め込み法**: Lasso（L1正則化）で自動選択

**例**：
```
100個の特徴 → 重要度評価 → 上位10個を選択
```

**利点**：
- 次元削減（次元の呪い対策）
- 過学習防止
- 解釈性向上

---

### 3. 特徴量抽出（Feature Extraction）★試験頻出
**定義**：生データから有用な特徴を**計算・変換**して抽出する処理

**特徴選択との違い**：
| 項目 | 特徴選択 | 特徴量抽出 |
|------|---------|-----------|
| 操作 | 既存から選ぶ | 変換・生成して抽出 |
| 元特徴 | 保持される | 変換される |
| 次元 | 減少 | 通常は減少 |
| 解釈性 | 高い（元の特徴） | 低い（変換後） |

**代表的手法**：

**1. PCA（主成分分析）**
- 分散を最大化する軸で次元圧縮
- 100次元 → 10次元（情報の90%保持）

**2. オートエンコーダ**
- 深層学習による非線形特徴抽出
- 入力→圧縮→復元の中間層を特徴として使用

**3. 事前学習モデルの中間層**
- CNNの畳み込み層出力を特徴として使用
- BERTの埋め込み層を特徴として使用

**4. 音声・画像の特徴抽出**
- MFCC（音声のメル周波数ケプストラム係数）
- HOG（画像の勾配方向ヒストグラム）
- SIFT（スケール不変特徴変換）

**具体例（画像分類）**：
```
元データ: 224×224×3のピクセル値（150,528次元）
↓ CNNで特徴抽出
抽出特徴: 2048次元のベクトル
↓ 分類器（SVM等）
予測
```

---

### 特徴量エンジニアリングの比較表

| 手法 | 操作 | 入力 | 出力 | 用途 |
|------|------|------|------|------|
| **特徴量生成** | 新規作成 | 既存特徴 | 追加特徴 | 表現力向上 |
| **特徴選択** | 選択 | 既存特徴 | 一部特徴 | 次元削減、過学習防止 |
| **特徴量抽出** | 変換 | 生データ | 変換特徴 | 次元削減、有用特徴獲得 |

---

## 試験での問われ方（特徴量抽出）

### 典型的な出題パターン

**パターン1：特徴量抽出の説明として最も不適切な選択肢を選べ**

✅ **適切な選択肢（正しい説明）**：
- **「生データから有用な特徴を計算・変換して抽出する処理」** → ✅ 正しい
- **「PCAや自己符号化器を用いて次元削減を行う」** → ✅ 正しい
- **「画像や音声から特徴ベクトルを生成する」** → ✅ 正しい
- **「元のデータを変換して低次元表現を得る」** → ✅ 正しい
- **「CNNの中間層出力を特徴として利用する」** → ✅ 正しい
- **「MFCCやHOGなどの特徴量を計算する」** → ✅ 正しい

❌ **不適切な選択肢（誤った説明、試験で狙われる）**：
- **「既存の特徴量から重要なものを選択する処理」** → ❌ **最も不適切**（これは特徴選択の説明）
- **「特徴量を削除せずに全て使用する」** → ❌ 不適切（次元削減が目的）
- **「元のデータをそのまま保持する」** → ❌ 不適切（変換が目的）
- **「データ拡張の一種である」** → ❌ 不適切（別の手法）
- **「特徴量の数を増やす手法」** → ❌ 不適切（通常は減らす）

---

**パターン2：特徴選択と特徴量抽出の違い**

> 「特徴選択と特徴量抽出の違いとして、最も適切な選択肢を1つ選べ。」

✅ **適切な選択肢**：
- **「特徴選択は既存から選ぶ、特徴量抽出は変換して生成する」** → ✅ **最も適切**
- **「特徴選択は解釈性が高い、特徴量抽出は解釈性が低い」** → ✅ 適切
- **「特徴選択は元の特徴を保持、特徴量抽出は新しい特徴を作る」** → ✅ 適切

❌ **不適切な選択肢**：
- **「特徴選択と特徴量抽出は同じ意味」** → ❌ 不適切（別の手法）
- **「特徴量抽出は元の特徴をそのまま選ぶ」** → ❌ 不適切（変換する）

---

**パターン3：特徴量抽出の具体例**

**特徴量抽出に該当する例**：
- ✅ PCAで100次元→10次元に圧縮
- ✅ オートエンコーダの中間層を使用
- ✅ CNNで画像から特徴ベクトルを抽出
- ✅ 音声からMFCCを計算
- ✅ 画像からHOG特徴を計算

**特徴選択に該当する例**（混同注意）：
- ❌ 100個の特徴から重要な10個を選択
- ❌ Lassoで不要な特徴を削除
- ❌ 相関係数で重要特徴を選定

---

### 引っ掛けポイント

| ひっかけ | 正しい理解 |
|----------|------------|
| ❌ 特徴量抽出 = 特徴選択 | ✅ 別の手法（変換 vs 選択） |
| ❌ 既存特徴から選ぶのが特徴量抽出 | ✅ それは特徴選択、抽出は変換 |
| ❌ 次元が増える | ✅ 通常は次元削減 |
| ❌ 元のデータを保持 | ✅ 変換後のデータを使用 |
| ❌ 解釈性が高い | ✅ 変換後は解釈性低下（PCA等） |

---

## 次元の呪い（Curse of Dimensionality）

### 定義
**次元の呪い**は、特徴量の数（次元）が増加すると、データが高次元空間に疎に分布し、機械学習モデルの性能が悪化する現象。1957年にリチャード・ベルマンが提唱。

### 主な問題点

#### 1. データの疎密性（Sparsity）
高次元空間では、データ点間の距離が広がり、データが「疎」に分布：

**具体例**（2次元 vs 10次元）：
- **2次元**：100個のデータ点で空間を10×10のグリッドでカバー可能
- **10次元**：同じ密度を保つには $10^{10}$ = **100億個**のデータ点が必要

**必要データ量**：
$$N \propto k^d$$
- $N$: 必要なサンプル数
- $k$: 各次元の分割数
- $d$: 次元数（指数関数的に増加）

#### 2. 距離の意味の喪失
高次元では全ての点の距離が均一化され、「近い」「遠い」の区別が困難に：

**距離の均一化**：
```
低次元（2D）：
点A-点B: 距離3
点A-点C: 距離10
→ 明確な差（3.3倍）

高次元（100D）：
点A-点B: 距離50.2
点A-点C: 距離51.8
→ わずかな差（1.03倍のみ）
```

**数学的説明**：
- 最近傍と最遠点の距離比が1に収束：$\frac{d_{max}}{d_{min}} \to 1$
- k-NNなどの距離ベースアルゴリズムが機能不全

#### 3. 過学習のリスク増加
特徴量が多いほど、モデルが訓練データのノイズまで学習：

**パラメータ数の増加**：
- 入力次元100 → 全結合層（100ユニット）: **10,000パラメータ**
- 入力次元1000 → 全結合層（100ユニット）: **100,000パラメータ**

**データ対パラメータ比**：
- データ1,000個、パラメータ10,000個 → 過学習リスク大
- 「パラメータ数 < データ数」が経験則

#### 4. 計算コストの増加
次元数に応じて計算量が増加：

**計算量の例**：
- **距離計算**：$O(d)$（d次元）
- **k-NN探索**：$O(nd)$（n個のデータ、d次元）
- **SVMのカーネル計算**：$O(n^2 d)$

### 次元の呪いの影響を受けやすいアルゴリズム

| アルゴリズム | 影響度 | 理由 |
|-------------|-------|------|
| **k-NN** | ❌ 非常に大 | 距離ベース、高次元で距離が無意味化 |
| **決定木** | △ 中程度 | 分岐で次元を分割、深い木で過学習 |
| **SVM（RBFカーネル）** | ❌ 大 | 距離ベース、高次元で計算コスト増大 |
| **線形回帰** | △ 中程度 | 多重共線性、過学習のリスク |
| **ランダムフォレスト** | ○ 小～中 | 特徴量ランダム選択で緩和 |
| **深層学習** | ○ 小 | 階層的特徴抽出、高次元に強い |
| **正則化回帰（Lasso/Ridge）** | ○ 小 | 正則化で過学習抑制 |

### 次元の呪いへの対策

#### 1. 次元削減（Dimensionality Reduction）★試験頻出

**定義**：高次元データを低次元空間に射影・変換する技術。データの本質的な構造を保ちながら次元を削減し、計算コスト削減や可視化を実現。

---

##### 主な次元削減手法

**1. PCA（主成分分析：Principal Component Analysis）**

**最も代表的な線形次元削減手法**

**原理**：
- データの分散が最大となる方向（主成分）を見つける
- 直交する主成分を順に抽出
- 上位k個の主成分のみを使用して次元削減

**プロセス**：
```
元データ（100次元）
  ↓
共分散行列を計算
  ↓
固有値分解
  ↓
固有値が大きい順に固有ベクトル（主成分）を選択
  ↓
第1〜第10主成分のみ使用
  ↓
削減後データ（10次元）
```

**特徴**：
- ✅ 線形変換で高速
- ✅ 理論的に明確（分散最大化）
- ✅ データの大まかな構造を保持
- ❌ 線形なので複雑な非線形構造は捉えられない
- ❌ 変換後の軸の解釈性が低い

**用途**：
- 前処理（次元削減後に機械学習）
- データ可視化（2次元・3次元に削減）
- ノイズ除去

**実例**：
```
画像データ（1000×1000ピクセル = 100万次元）
  ↓ PCA
100次元に削減（情報の95%保持）
  ↓
機械学習モデルで分類
```

---

**2. t-SNE（t-Distributed Stochastic Neighbor Embedding）**

**非線形次元削減、可視化に特化**

**原理**：
- 高次元空間での点間の類似度を保持しながら低次元（2D/3D）に配置
- 近い点は近く、遠い点は遠くに配置
- 局所的な構造を重視

**特徴**：
- ✅ 非線形構造を捉える
- ✅ クラスタが明瞭に可視化される
- ✅ 複雑なデータの可視化に最適
- ❌ 計算コストが高い（大規模データに不向き）
- ❌ 2D/3Dへの可視化専用（任意の次元に削減できない）
- ❌ パラメータ調整（パープレキシティ等）が必要

**用途**：
- 高次元データの2D/3D可視化
- クラスタ構造の確認
- 異常検知の可視化

**実例**：
```
MNIST（手書き数字、784次元）
  ↓ t-SNE
2次元に削減
  ↓
散布図で可視化 → 数字0〜9がクラスタとして明瞭に分離
```

---

**3. UMAP（Uniform Manifold Approximation and Projection）**

**t-SNEの改良版、高速で大域構造も保持**

**特徴**：
- t-SNEより高速
- 大域的な構造も保持
- 大規模データに対応

---

**4. オートエンコーダ（Autoencoder）**

**深層学習による非線形次元削減**

**構造**：
```
入力（高次元）
  ↓
Encoder（圧縮）
  ↓
潜在変数（低次元） ← これを特徴量として使用
  ↓
Decoder（復元）
  ↓
出力（元の次元）
```

**学習目標**：入力と出力が一致するように学習

**特徴**：
- ✅ 非線形変換で複雑な構造を捉える
- ✅ 任意の次元に削減可能
- ✅ 画像・音声・テキスト全てに適用可能
- ❌ 学習に時間がかかる
- ❌ ハイパーパラメータ調整が必要

**変種**：
- **VAE（変分オートエンコーダ）**：潜在空間を確率分布でモデル化
- **Denoising Autoencoder**：ノイズ除去を学習

---

**5. LDA（線形判別分析：Linear Discriminant Analysis）**

**教師あり次元削減（クラスラベルを利用）**

**原理**：
- クラス間の分散を最大化、クラス内の分散を最小化する軸を見つける
- PCAと異なり、クラス情報を使用

**特徴**：
- ✅ 分類タスクで有効
- ✅ クラスの分離を最大化
- ❌ 最大でもクラス数-1次元までしか削減できない
- ❌ 線形変換のみ

---

##### 次元削減手法の比較表

| 手法 | 種類 | 変換 | 教師 | 主な用途 | 計算速度 | 任意次元 |
|------|------|------|------|---------|---------|---------|
| **PCA** | 教師なし | 線形 | 不要 | 前処理、可視化 | 高速 | ✅ |
| **t-SNE** | 教師なし | 非線形 | 不要 | 可視化（2D/3D） | 低速 | ❌（2D/3D専用） |
| **UMAP** | 教師なし | 非線形 | 不要 | 可視化、前処理 | 中速 | ✅ |
| **オートエンコーダ** | 教師なし | 非線形 | 不要 | 前処理、生成 | 中速 | ✅ |
| **LDA** | 教師あり | 線形 | 必要 | 分類前処理 | 高速 | ✅（制限あり） |

---

##### 次元削減に「不適切」な手法（試験頻出）

次元削減は**データの次元を減らす**ことが目的。以下は次元削減ではない：

❌ **不適切な手法**：

1. **BoW（Bag of Words）**：
   - テキストを高次元ベクトルに変換（語彙数次元）
   - むしろ**次元を増やす**（次元削減の逆）
   - 例：100単語の語彙 → 100次元ベクトル

2. **N-Gram**：
   - 単語パターンを抽出（次元削減ではない）
   - 語彙爆発で**次元がさらに増える**
   - 例：Bigram → 語彙数の2乗のパターン

3. **One-Hot Encoding**：
   - カテゴリ変数を2値ベクトルに変換
   - カテゴリ数分の次元に**拡張**（次元増加）
   - 例：10種類のカテゴリ → 10次元ベクトル

4. **TF-IDF**：
   - 単語の重要度を計算（次元は変わらない）
   - BoWに重み付けしたもの（高次元のまま）

5. **探索アルゴリズム（BFS、DFS、A*等）**：
   - グラフ探索、経路探索の手法
   - 次元削減とは**全く無関係**

6. **決定木、ランダムフォレスト**：
   - 分類・回帰アルゴリズム
   - 次元削減ではない（特徴量重要度は計算できる）

7. **K-means、階層的クラスタリング**：
   - クラスタリング手法
   - 次元削減ではない（データのグループ化）

8. **フォルマント分析**：
   - 音声の周波数特性分析
   - 特徴抽出だが、一般的な次元削減手法ではない

---

##### 試験での問われ方

**典型問題**：

> 「次元削減に用いられる手法として、最も不適切な選択肢を1つ選べ。」

✅ **適切な手法（次元削減に使われる）**：
- **PCA（主成分分析）** → ✅ 最も代表的
- **t-SNE** → ✅ 可視化用の次元削減
- **オートエンコーダ** → ✅ 深層学習による次元削減
- **UMAP** → ✅ 非線形次元削減
- **LDA（線形判別分析）** → ✅ 教師あり次元削減

❌ **不適切な手法（次元削減ではない）**：
- **BoW（Bag of Words）** → ❌ **最も不適切**（次元を増やす）
- **One-Hot Encoding** → ❌ 次元を増やす
- **TF-IDF** → ❌ 重み付けのみ（次元削減ではない）
- **探索アルゴリズム（BFS/DFS/A*）** → ❌ 全く無関係
- **決定木** → ❌ 分類手法（次元削減ではない）
- **K-means** → ❌ クラスタリング（次元削減ではない）

---

**ひっかけポイント**：

| ひっかけ | 正しい理解 |
|----------|------------|
| ❌ BoWは次元削減に使える | ✅ BoWは次元を**増やす**（語彙数次元） |
| ❌ TF-IDFは次元削減 | ✅ TF-IDFは重み付け（次元は変わらない） |
| ❌ One-Hot Encodingは次元削減 | ✅ One-Hot Encodingは次元を**増やす** |
| ❌ K-meansは次元削減 | ✅ K-meansは**クラスタリング** |
| ❌ 決定木は次元削減 | ✅ 決定木は**分類・回帰** |
| ✅ PCAは次元削減 | ✅ 正しい（最も代表的） |
| ✅ t-SNEは次元削減 | ✅ 正しい（可視化用） |
| ✅ オートエンコーダは次元削減 | ✅ 正しい（深層学習） |

---

**比較問題**：

> 「以下のうち、データの次元を増やす（または削減しない）手法を全て選べ。」

✅ **次元を増やす/削減しない**：
- **BoW**（語彙数次元に増加）
- **One-Hot Encoding**（カテゴリ数次元に増加）
- **多項式特徴量**（$x^2$, $x^3$等を追加）
- **TF-IDF**（次元は変わらない）

❌ **次元を減らす**：
- **PCA**（主成分のみ使用）
- **t-SNE**（2D/3Dに削減）
- **特徴選択**（一部特徴のみ選択）

---

##### 実務での使い分け

| 状況 | 推奨手法 | 理由 |
|------|---------|------|
| **前処理として次元削減** | PCA | 高速、理論的に明確 |
| **可視化（2D/3D）** | t-SNE、UMAP | クラスタが明瞭 |
| **画像・音声の特徴抽出** | オートエンコーダ | 非線形構造を捉える |
| **分類タスク** | LDA | クラス分離を最大化 |
| **大規模データ** | PCA、UMAP | t-SNEは遅い |
| **解釈性が重要** | 特徴選択 | 元の特徴を保持 |

---

#### 2. 特徴選択（Feature Selection）
**フィルタ法**：
- 相関係数、相互情報量で重要特徴を選択

**ラッパー法**：
- モデル性能を評価しながら選択（後退選択、前進選択）

**埋め込み法**：
- Lasso（L1正則化）で自動的に不要特徴を削除

#### 3. 正則化（Regularization）
**L1正則化（Lasso）**：
- 重みをゼロにして実質的に次元削減
- $\min \|y - Xw\|^2 + \lambda \|w\|_1$

**L2正則化（Ridge）**：
- 重みを小さく保つ
- $\min \|y - Xw\|^2 + \lambda \|w\|^2$

#### 4. ドメイン知識の活用
- 専門知識で有用な特徴量を事前選択
- 物理法則、業務知識に基づく特徴量生成

#### 5. アンサンブル手法
**ランダムフォレスト**：
- 各決定木で特徴量をランダム選択
- 特徴量重要度で事後評価

### 次元の呪いの逆説（高次元の利点）

**高次元の利点（場合による）**：
- **表現力の向上**：複雑なパターンを表現可能
- **分離可能性の向上**：高次元では線形分離しやすい（カバーの定理）
- **深層学習の成功**：適切な構造で高次元を扱える

**カバーの定理（Cover's Theorem）**：
- 高次元空間では、データが線形分離可能になる確率が高まる
- ただし、データ量が十分に必要

---

## 重要キーワード
- **次元の呪い（Curse of Dimensionality）**: 次元増加でデータが疎になり必要データ量が指数増加する現象
- **データの疎密性（Sparsity）**: 高次元空間でデータ点が疎に分布する状態
- **距離の均一化**: 高次元で全ての点の距離が等しく見える現象
- **次元削減（Dimensionality Reduction）**: 高次元データを低次元に圧縮する手法
- **特徴選択（Feature Selection）**: 重要な特徴量のみを選択する手法
- **PCA（主成分分析）**: 分散を最大化する軸で次元削減
- **L1正則化（Lasso）**: 不要な特徴量の重みをゼロにする正則化
- **カバーの定理（Cover's Theorem）**: 高次元では線形分離可能性が向上する定理
- **特徴量エンジニアリング（Feature Engineering）**: 特徴量の生成・選択・変換の総称
- **特徴量生成（Feature Generation）**: 新しい特徴量を作成するプロセス
- **特徴量変換（Feature Transformation）**: 既存特徴量を変換するプロセス

---

## 試験での問われ方

### 典型的な出題パターン

#### パターン1：選択肢問題（不適切な説明を選ぶ）
> 「次元の呪いの説明として、最も不適切な選択肢を1つ選べ。」

✅ **適切な選択肢（正しい説明）**：
- **「次元が増えるとデータが空間に疎に分布する」** → ✅ 正しい
- **「必要なデータ量が次元数に対して指数関数的に増加する」** → ✅ 正しい
- **「高次元空間では、距離の概念が意味を失う」** → ✅ 正しい
- **「過学習のリスクが高まる」** → ✅ 正しい
- **「PCA等の次元削減が有効な対策である」** → ✅ 正しい
- **「計算コストが次元数に応じて増加する」** → ✅ 正しい
- **「k-NNなどの距離ベースアルゴリズムが影響を受けやすい」** → ✅ 正しい

❌ **不適切な選択肢（誤った説明、試験で狙われる）**：
- **「次元（特徴量）が増えれば増えるほど、モデルの精度は必ず向上する」** → ❌ **最も不適切**（逆に悪化する）
- **「高次元空間では、データ量が少なくても十分に学習できる」** → ❌ 不適切（より多くのデータが必要）
- **「次元削減は不要で、全ての特徴量を使うべきである」** → ❌ 不適切（次元削減は有効）
- **「高次元空間では、距離の概念がより正確になる」** → ❌ 不適切（距離が無意味化）
- **「計算コストは次元数に関係なく一定である」** → ❌ 不適切（増加する）
- **「全ての機械学習アルゴリズムが同程度に影響を受ける」** → ❌ 不適切（影響度は異なる）

#### パターン2：対策に関する問題
> 「次元の呪いへの対策として、最も適切な選択肢を1つ選べ。」

✅ **適切な選択肢**：
- **「PCA（主成分分析）で次元削減を行う」** → ✅ 最も適切
- **「L1正則化（Lasso）で不要な特徴量を削除する」** → ✅ 適切
- **「特徴選択で重要な特徴量のみを使用する」** → ✅ 適切
- **「ドメイン知識を活用して有用な特徴量を選ぶ」** → ✅ 適切

❌ **不適切な選択肢**：
- **「特徴量を可能な限り増やす」** → ❌ 不適切（逆効果）
- **「データ量を減らす」** → ❌ 不適切（増やすべき）

### 比較されやすい概念
- **次元削減 vs 特徴選択**: 変換して圧縮 vs 選んで削除
- **PCA vs t-SNE**: 線形・全体構造保持 vs 非線形・局所構造保持
- **L1正則化 vs L2正則化**: スパース化（ゼロに） vs 小さく保つ
- **高次元の問題 vs 高次元の利点**: データ疎密・距離無意味化 vs 表現力・分離可能性
- **次元の呪い vs 過学習**: 原因の一つ vs 結果の症状

### 引っ掛けポイント

| ひっかけ | 正しい理解 |
|----------|------------|
| ❌ 次元が多いほど精度向上 | ✅ 過学習で精度低下する場合が多い |
| ❌ データ量は関係ない | ✅ 指数関数的にデータ量が必要 |
| ❌ 距離がより正確に | ✅ 距離が無意味化する |
| ❌ 次元削減は常に不要 | ✅ 有効な対策の一つ |
| ❌ 全てのアルゴリズムが同等に影響 | ✅ k-NN等の距離ベースが特に影響大 |
| ❌ 高次元は常に悪い | ✅ 適切な手法で扱えば利点もある |
| ❌ 計算コスト不変 | ✅ 次元数に応じて増加 |

### 頻出の数値・公式
- **必要データ量**：$N \propto k^d$（指数関数的増加）
- **PCAの分散寄与率**：累積90%以上が目安
- **距離の均一化**：$\frac{d_{max}}{d_{min}} \to 1$（d→∞）

---

## 実例

### 画像認識（高次元データ）
- **100×100ピクセルのグレースケール画像**：10,000次元
- **対策**：CNNで階層的特徴抽出、PCAで数百次元に削減

### ゲノムデータ解析
- **遺伝子数**：数万次元
- **サンプル数**：数百～数千
- **対策**：特徴選択で重要遺伝子のみ使用、正則化回帰

### テキスト分類（Bag-of-Words）
- **語彙数**：数万～数十万次元
- **対策**：TF-IDF、Word2Vec、BERTで低次元埋め込み

### 具体例：k-NNでの次元の呪い
```
2次元データ（100個）：
- 最近傍距離: 平均2.0
- 最遠点距離: 平均10.0
- 距離比: 5.0倍（明確な区別）

100次元データ（100個）：
- 最近傍距離: 平均50.2
- 最遠点距離: 平均51.8
- 距離比: 1.03倍（ほぼ区別不能）
```

---

## 補足

### 実務的観点
- **特徴量エンジニアリングの重要性**：「データと特徴量がアルゴリズムの性能の80%を決める」
- **ドメイン知識の活用**：業務知識で有用な特徴量を優先
- **自動特徴量生成**：FeatureToolsなどのツール活用
- **特徴量の解釈性**：ビジネス判断には解釈可能な特徴量を残す

### 深層学習時代の変化
- **表現学習**：深層学習が自動的に有用な特徴を学習
- **End-to-End学習**：手動の特徴量エンジニアリングが不要に
- **ただし**：データ量が十分にある場合のみ有効

### 関連トピック
- [次元削減（PCA）](unsupervised_learning.md) - 主成分分析の詳細
- [正則化](overfitting_underfitting.md) - L1/L2正則化の詳細
- [評価指標](evaluation_metrics.md) - モデル性能の評価
- [過学習対策](overfitting_underfitting.md) - 過学習と次元の呪いの関係
