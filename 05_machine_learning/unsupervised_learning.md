# 教師なし学習（Unsupervised Learning）

## 要点
教師なし学習は正解ラベルなしでデータの構造やパターンを発見する機械学習。代表手法は**クラスタリング**（k-means、階層的クラスタリング）と**次元削減**（PCA、t-SNE）。データのグループ化、異常検知、可視化に活用。

## 定義
教師なし学習とは、正解ラベル（教師信号）を持たないデータから、データの潜在的な構造、パターン、関係性を自動的に発見する機械学習の枠組み。データの類似性に基づいてグループ化（クラスタリング）したり、データの次元を削減して本質的な特徴を抽出したりする。

## 重要キーワード
- **クラスタリング（Clustering）**: データを類似性に基づいてグループ分け
- **k-means法**: 最も基本的なクラスタリング手法、k個の中心点でグループ化
- **階層的クラスタリング**: データを階層的に統合または分割
- **次元削減（Dimensionality Reduction）**: 高次元データを低次元に圧縮
- **PCA（主成分分析）**: 分散を最大化する軸を見つける次元削減
- **t-SNE**: 高次元データの可視化に特化した次元削減
- **異常検知（Anomaly Detection）**: 正常パターンから外れたデータの検出

---

## k-means法

### 要点
k-means法は最も基本的なクラスタリング手法。データをk個のクラスタに分割し、各クラスタ中心（重心）との距離を最小化するよう反復的に調整。顧客セグメンテーション、画像圧縮、異常検知などに活用。

### 定義
k-means法とは、n個のデータ点をk個のクラスタに分割する非階層的クラスタリング手法。各データ点を最も近いクラスタ中心に割り当て、クラスタ中心を更新する処理を収束するまで繰り返す。目的はクラスタ内のデータ点の分散（クラスタ中心との距離の二乗和）を最小化すること。

目的関数：
$$J = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2$$

ここで、$C_i$ はクラスタi、$\mu_i$ はクラスタiの中心（重心）。

### 重要キーワード
- **k（クラスタ数）**: 事前に指定する必要があるハイパーパラメータ
- **クラスタ中心（セントロイド）**: 各クラスタの重心、データ点の平均位置
- **反復最適化**: 割り当てと更新を繰り返す
- **収束**: クラスタ割り当てが変化しなくなった状態
- **初期値依存性**: 初期クラスタ中心の選び方で結果が変わる
- **エルボー法**: 最適なk値を決定する手法
- **シルエット係数**: クラスタリングの質を評価する指標

### 詳細

#### アルゴリズムの流れ
```
入力: データ点 X = {x₁, x₂, ..., xₙ}, クラスタ数 k
出力: 各データ点のクラスタ割り当て

1. 初期化: k個のクラスタ中心 μ₁, μ₂, ..., μₖ をランダムに選択
   （通常はデータ点からランダムにk個選ぶ）

2. 繰り返し（収束するまで）:
   
   【割り当てステップ】
   a. 各データ点 xᵢ について:
      - 最も近いクラスタ中心を見つける
      - そのクラスタに割り当て
      cᵢ = argmin_j ||xᵢ - μⱼ||²
   
   【更新ステップ】
   b. 各クラスタ j について:
      - そのクラスタに属するデータ点の平均を計算
      - クラスタ中心を更新
      μⱼ = (1/|Cⱼ|) Σ_{x∈Cⱼ} x

3. 収束条件: クラスタ割り当てが変化しない、または
   目的関数の変化が閾値以下
```

#### 図解（k=3の場合）
```
初期状態:
  ×  ・×    ★ (中心1)
 ・ ×・・
    ・×  ◆ (中心2)
   × ・
 ▲ (中心3) ・・×

↓ 割り当てステップ（最も近い中心に割り当て）

  ×  ・×    ★
 ・ ×・・      [クラスタ1: 青]
    ・×  ◆    [クラスタ2: 赤]
   × ・        [クラスタ3: 緑]
 ▲  ・・×

↓ 更新ステップ（各クラスタの重心を計算）

  ×  ・×  ★'（新中心1）
 ・ ×・・
    ・×◆'（新中心2）
   × ・
 ▲'（新中心3）・・×

↓ 収束するまで繰り返し
```

#### k-means法の特徴

**利点**:
- **シンプル**: 実装が容易、理解しやすい
- **高速**: 大規模データにも適用可能（計算量 O(nkd)、n:データ数、d:次元）
- **スケーラブル**: ミニバッチk-meansで更に高速化可能

**欠点**:
- **k値の事前指定**: クラスタ数を事前に決める必要がある
- **初期値依存**: 初期中心の選び方で結果が変わる（k-means++で改善）
- **球状クラスタ前提**: 複雑な形状のクラスタには不向き
- **外れ値に敏感**: 平均を使うため外れ値の影響を受けやすい

### k-means法の活用例（実例）

#### 1. **顧客セグメンテーション（マーケティング）**
**最も代表的な活用例**

**目的**: 顧客を購買行動や属性に基づいてグループ分け

**具体例**:
- ECサイトの顧客を「購入頻度」「購入金額」「最終購入日」でクラスタリング
- 結果: 「優良顧客」「休眠顧客」「新規顧客」などに分類
- 活用: セグメントごとに異なるマーケティング施策

**データ例**:
```
顧客A: 購入頻度=月10回, 購入金額=5万円
顧客B: 購入頻度=月1回,  購入金額=5千円
顧客C: 購入頻度=月8回,  購入金額=4.5万円
...
→ k=4でクラスタリング
→ クラスタ1: VIP顧客（高頻度・高金額）
  クラスタ2: 定期顧客（中頻度・中金額）
  クラスタ3: 休眠顧客（低頻度・低金額）
  クラスタ4: 新規顧客（最近登録）
```

#### 2. **画像圧縮・減色**

**目的**: 画像の色数を削減してデータサイズを圧縮

**手法**:
- 画像の各ピクセルをRGB値（3次元ベクトル）として扱う
- k-meansでk色に代表色を決定
- 各ピクセルを最も近い代表色に置き換え

**例**:
```
元画像: 24ビットカラー（1677万色）
↓ k=16でk-means適用
圧縮画像: 16色のみ使用
→ データサイズ大幅削減、視覚的には大差なし
```

#### 3. **文書クラスタリング**

**目的**: 大量の文書を内容に基づいて自動分類

**手法**:
- 文書をTF-IDF等でベクトル化
- k-meansで類似文書をグループ化

**活用**:
- ニュース記事の自動分類（政治、経済、スポーツ等）
- 顧客問い合わせの自動振り分け
- 文書検索システム

#### 4. **異常検知**

**目的**: 正常データのパターンを学習し、異常を検出

**手法**:
- 正常データでk-meansを適用
- 新データが全クラスタ中心から遠い場合、異常と判定

**活用**:
- 製造業での不良品検出
- ネットワーク侵入検知
- クレジットカード不正利用検知

**例**:
```
正常トランザクション → k-meansでクラスタ化
新規トランザクション:
  - 最近傍クラスタまでの距離 > 閾値 → 異常
  - 最近傍クラスタまでの距離 ≤ 閾値 → 正常
```

#### 5. **画像セグメンテーション**

**目的**: 画像を意味のある領域に分割

**手法**:
- ピクセルの色・位置情報でクラスタリング
- 同じクラスタのピクセルを1つの領域として扱う

**活用**:
- 医療画像診断（腫瘍領域の抽出）
- 自動運転（道路・歩行者・車両の領域分割）

#### 6. **レコメンデーションシステム**

**目的**: 類似ユーザーや商品をグループ化

**手法**:
- ユーザーの行動履歴をベクトル化
- k-meansで類似ユーザーをグループ化
- 同じクラスタのユーザーが好む商品を推薦

#### 7. **遺伝子発現データ解析**

**目的**: 遺伝子を発現パターンで分類

**手法**:
- 各遺伝子の複数条件下での発現量をベクトル化
- k-meansで類似発現パターンの遺伝子をグループ化

**活用**: 創薬、疾患分類

### 試験での問われ方

#### 典型設問
- **「k-means法の活用例として最も適切なもの」→ 顧客セグメンテーション**
- **「教師なし学習の活用例」→ クラスタリング、次元削減、異常検知**
- **「正解ラベルなしでデータをグループ化」→ k-means法等のクラスタリング**
- k-means法のアルゴリズム（割り当て→更新の反復）
- k-means法の欠点（k値の事前指定、初期値依存性）
- 最適なk値の決定方法（エルボー法）

#### ひっかけポイントと違いの整理

**混同注意**:
- **k-means vs k-NN（k近傍法）**: k-meansは教師なし学習のクラスタリング、k-NNは教師あり学習の分類・回帰
- **k-means vs 階層的クラスタリング**: k-meansは非階層的（k個に分割）、階層的は樹形図を作成
- **クラスタリング vs 分類**: クラスタリングは正解ラベルなし、分類は正解ラベルあり

**出題パターン**:
- 「k-means法で事前に決める必要があるパラメータ」→**k（クラスタ数）**
- 「k-means法の欠点」→**初期値依存性、k値の事前指定、球状クラスタ前提**
- 「エルボー法の目的」→**最適なk値の決定**
- 「k-means法が適さないケース」→**複雑な形状のクラスタ、異なるサイズ・密度のクラスタ**

**選択肢で出やすい活用例の対比**:
| 活用例 | k-means適用可否 | 理由 |
|--------|----------------|------|
| **顧客セグメンテーション** | ✓ 適切 | 代表的な活用例 |
| **画像圧縮（減色）** | ✓ 適切 | 色をクラスタリング |
| **異常検知** | ✓ 適切 | 正常パターンをクラスタ化 |
| **文書分類（ラベルあり）** | ✗ 不適切 | 教師あり学習（分類）の領域 |
| **回帰分析** | ✗ 不適切 | 教師あり学習、連続値予測 |
| **画像認識（物体分類）** | ✗ 不適切 | 教師あり学習（CNN等） |

**正しい活用例の特徴**:
- 正解ラベルがない
- データをグループ化したい
- パターンや構造を発見したい
- 類似性に基づく分類

**誤った活用例の特徴**:
- 正解ラベルがある（教師あり学習）
- 特定の値を予測したい（回帰）
- 物体や文字を認識したい（分類）

### 補足

#### 実務観点

**k値の決定方法**:

**1. エルボー法（最も一般的）**:
```
k=1,2,3,...と変えながらクラスタ内分散を計算
グラフが「肘」のように曲がる点を選択

クラスタ内分散
 ↑
 |＼
 |  ＼
 |    ＼___  ← この「肘」のk値を選択
 |         ￣￣￣￣
 +---------------→ k
  1  2  3  4  5  6
```

**2. シルエット係数**:
- 各データ点のクラスタ内凝集度とクラスタ間分離度を測定
- -1〜1の値、高いほど良いクラスタリング

**3. ドメイン知識**:
- ビジネス要件から決定（例：顧客を3段階に分類したい→k=3）

**初期値問題への対処**:

**k-means++（推奨）**:
- 初期中心をデータから賢く選択
- ランダム選択より収束が速く、結果が安定

**複数回実行**:
- 異なる初期値で複数回実行
- 最も良い結果（目的関数が最小）を採用

**実装のポイント**:
```python
from sklearn.cluster import KMeans

# 基本的な使い方
kmeans = KMeans(n_clusters=3,      # クラスタ数
                init='k-means++',   # 初期化方法
                n_init=10,          # 異なる初期値で10回実行
                random_state=42)    # 再現性のため

labels = kmeans.fit_predict(X)      # クラスタ割り当て
centers = kmeans.cluster_centers_   # クラスタ中心
```

**実務での課題と対策**:
- **高次元データ**: 次元削減（PCA）後にk-means適用
- **大規模データ**: ミニバッチk-means使用
- **非球状クラスタ**: DBSCAN、ガウス混合モデル等を検討
- **カテゴリカルデータ**: k-modes法を使用

#### 関連トピック
- [教師あり学習](supervised_learning.md) - 学習パラダイムの違い
- [次元削減](feature_engineering.md) - 前処理としての活用
- [評価指標](evaluation_metrics.md) - シルエット係数等

---

## その他の教師なし学習手法

### 階層的クラスタリング

**特徴**:
- k値の事前指定不要
- デンドログラム（樹形図）で階層構造を可視化
- 凝集型（ボトムアップ）と分裂型（トップダウン）

**活用例**:
- 生物の系統樹作成
- 遺伝子の分類
- 組織階層の分析

### 主成分分析（PCA）

**目的**: 高次元データを低次元に圧縮

**手法**:
- データの分散が最大となる軸（主成分）を見つける
- 上位数個の主成分でデータを表現

**活用例**:
- データの可視化（2次元・3次元プロット）
- 特徴量の削減（次元の呪いの回避）
- ノイズ除去

### 異常検知

**手法**:
- One-Class SVM
- Isolation Forest
- オートエンコーダ

**活用例**:
- 不正取引検知
- 設備故障予兆検知
- サイバー攻撃検知

## 試験での問われ方（教師なし学習全般）

### 典型設問
- 教師なし学習の定義（正解ラベルなし）
- 教師あり学習との違い
- 代表的手法（クラスタリング、次元削減）
- k-means法の特徴と活用例
- 教師なし学習の適用場面

### 引っ掛けポイント
- **教師なし vs 教師あり**: 正解ラベルの有無
- **クラスタリング vs 分類**: ラベルなし vs ラベルあり
- **k-means vs k-NN**: 教師なし vs 教師あり、用途が異なる

## 補足（教師なし学習全般）

### 実務課題
- **評価の難しさ**: 正解がないため性能評価が困難
- **解釈性**: クラスタの意味付けは人間が行う必要
- **パラメータ調整**: k値等のハイパーパラメータ選択

### 最新動向
- **深層クラスタリング**: ニューラルネットワークと組み合わせ
- **自己教師あり学習**: ラベルなしデータから表現学習
- **半教師あり学習**: 少量のラベルと大量の非ラベルデータを併用
