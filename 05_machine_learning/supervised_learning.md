# 教師あり学習

## 要点
- 正解ラベル付きデータから入力→出力の関係を学習する機械学習手法。分類（離散値予測）と回帰（連続値予測）の2種類。
- 代表手法：線形回帰、ロジスティック回帰、決定木、ランダムフォレスト、SVM、ニューラルネットワーク。
- 学習データ・検証データ・テストデータに分割し、汎化性能を評価。過学習に注意。

## 定義
教師あり学習（Supervised Learning）とは、入力データ $x$ と対応する正解ラベル（教師信号）$y$ のペア $(x, y)$ を用いて、入力から出力への写像 $f: x \rightarrow y$ を学習する機械学習の枠組み。

## 重要キーワード
- **分類（Classification）**: 離散的なクラスラベルを予測（例：画像分類、スパム判定）
- **回帰（Regression）**: 連続値を予測（例：価格予測、気温予測）
- **訓練データ（Training Data）**: モデル学習に使用する正解ラベル付きデータ
- **テストデータ（Test Data）**: モデル評価に使用する未学習データ
- **汎化性能（Generalization）**: 未知データに対する予測精度
- **過学習（Overfitting）**: 訓練データに過剰適合し、テストデータで性能低下

---

## サポートベクターマシン（SVM）

### 要点
SVMはマージン最大化で分離超平面を求める教師あり学習手法。カーネルトリックにより、元空間で非線形分離可能なデータを高次元空間に写像し線形分離可能にする。明示的な写像計算は不要で、カーネル関数（RBF、多項式等）の内積計算のみで実現。

### 定義
サポートベクターマシン（SVM、Support Vector Machine）は、クラス間のマージン（余白）を最大化する分離超平面を見つける二値分類手法。カーネルトリックを用いることで、非線形分離問題を高次元空間での線形分離問題に変換する。

### 重要キーワード
- **サポートベクター**: 分離超平面に最も近いデータ点（決定境界を定義）
- **マージン**: 分離超平面と最近傍データ点との距離
- **カーネルトリック**: 明示的な高次元写像なしに内積を計算する技法
- **カーネル関数**: RBFカーネル（ガウシアン）、多項式カーネル、線形カーネル等
- **ソフトマージン**: 誤分類を許容するための正則化パラメータ $C$
- **決定境界**: データを分離する超平面

### 詳細

#### 背景
線形SVMは線形分離可能なデータには有効だが、実世界のデータは非線形分離が一般的。カーネルトリックは、高次元空間での計算コストを回避しながら非線形分離を可能にする画期的な手法。

#### カーネルトリックの原理
1. **元空間**: 2次元データが非線形分離（円形の境界等）
2. **写像**: カーネル関数 $K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$ で高次元空間に写像
3. **線形分離**: 高次元空間では線形分離可能
4. **計算効率**: $\phi(x)$ を明示的に計算せず、カーネル関数 $K$ の値のみ計算

#### 代表的なカーネル関数
- **線形カーネル**: $K(x, y) = x \cdot y$
- **多項式カーネル**: $K(x, y) = (x \cdot y + c)^d$
- **RBFカーネル（ガウシアン）**: $K(x, y) = \exp(-\gamma \|x - y\|^2)$
- **シグモイドカーネル**: $K(x, y) = \tanh(\alpha x \cdot y + c)$

#### 図解（概念）
```
元空間（2次元）        高次元空間（3次元以上）
   +  -               すべて線形分離可能
  + - +         →          +++++++
   - +                     -------
非線形境界              線形分離超平面
```

#### マージン最大化
目的関数：
$$\max \frac{2}{\|w\|} \quad \text{s.t.} \quad y_i(w \cdot x_i + b) \geq 1$$

ソフトマージンSVM（誤分類許容）：
$$\min \frac{1}{2}\|w\|^2 + C\sum_i \xi_i$$

### 試験での問われ方

#### 典型設問
- **「カーネル関数で高次元写像し線形分離する手法」→SVM**
- カーネルトリックの利点（明示的写像不要、計算効率）
- 代表的カーネル関数の種類（RBF、多項式）
- マージン最大化の意味と汎化性能との関係

#### ひっかけポイントと違いの整理

| 項目 | SVM | ロジスティック回帰 | ニューラルネットワーク | 決定木 |
|------|-----|------------------|---------------------|--------|
| **手法** | マージン最大化 | 確率モデル | 非線形変換の積層 | ルール分岐 |
| **カーネル** | カーネルトリック使用 | 通常使わない | 不要（活性化関数） | 不要 |
| **高次元写像** | 可能 | 困難 | 中間層で実現 | 不要 |
| **解釈性** | 低（カーネル使用時） | 高 | 低 | 高 |
| **計算コスト** | 中〜高 | 低 | 高 | 低 |
| **多クラス** | 拡張必要（OvO/OvR） | 直接対応 | 直接対応 | 直接対応 |

**混同注意**:
- **ニューラルネットワークとの違い**: SVMはカーネル関数で写像、NNは中間層の非線形変換で実現
- **ロジスティック回帰との違い**: SVMはマージン最大化、ロジスティック回帰は確率最大化（最尤推定）
- **k-NNとの違い**: SVMはモデルベース（サポートベクター保持）、k-NNはインスタンスベース（全データ保持）

**出題パターン**:
- 「カーネル関数を用いて高次元空間に写像」→**SVM**
- 「マージン最大化による分類」→**SVM**
- 「サポートベクターのみで決定境界を定義」→**SVM**
- 「非線形分離可能なデータを線形分離する手法」→**カーネルトリックを用いたSVM**

### 補足

#### 実務観点
- **用途**: テキスト分類（スパムフィルタ）、画像認識（深層学習以前）、バイオインフォマティクス
- **パラメータ調整**: カーネル関数の選択、$C$（正則化）、$\gamma$（RBFカーネルの幅）が性能に大きく影響
- **スケーリング**: 特徴量のスケールに敏感なため標準化が必須
- **計算コスト**: サンプル数 $n$ に対して $O(n^2)$〜$O(n^3)$、大規模データには不向き
- **実装**: scikit-learn（`SVC`, `LinearSVC`）、LIBSVM等

#### 深層学習との比較
- **深層学習以前**: SVMは画像認識・テキスト分類で最高性能
- **現在**: 大規模データ・複雑なタスクでは深層学習が優位
- **利点**: 少数サンプルではSVMが有効、理論的基盤が明確

#### 関連トピック
- [評価指標](evaluation_metrics.md) - 精度・再現率・F1
- [過学習・汎化](overfitting_underfitting.md) - マージン最大化と汎化性能
- [特徴量エンジニアリング](feature_engineering.md) - カーネル選択の前提

---

## 主要な教師あり学習手法

### 線形回帰
- **定義**: 連続値を予測する最も基本的な回帰手法。$y = w^T x + b$
- **損失関数**: 平均二乗誤差（MSE）
- **用途**: 価格予測、需要予測

### ロジスティック回帰
- **定義**: シグモイド関数で確率を出力する二値分類手法
- **損失関数**: 交差エントロピー（クロスエントロピー）
- **用途**: 医療診断、与信判定

### 決定木
- **定義**: データを特徴量で分岐させ、木構造で分類・回帰
- **利点**: 解釈性が高い、非線形境界に対応
- **欠点**: 過学習しやすい

### ランダムフォレスト
- **定義**: 複数の決定木をアンサンブルし多数決で予測
- **利点**: 高精度、過学習抑制、特徴量重要度が分かる
- **欠点**: 解釈性が下がる、計算コスト増

### ニューラルネットワーク
- **定義**: 非線形活性化関数を持つ多層パーセプトロン
- **利点**: 表現力が高い、大規模データで高精度
- **欠点**: 学習コスト大、解釈性低

---

## 試験での問われ方（教師あり学習全般）

### 典型設問
- 教師あり学習の定義（正解ラベル使用）
- 分類と回帰の違い
- 代表的手法の特徴と用途
- 訓練データ・検証データ・テストデータの役割
- 過学習と汎化性能のトレードオフ

### 引っ掛けポイント
- **教師あり vs 教師なし**: 正解ラベルの有無で判別
- **分類 vs 回帰**: 出力が離散値か連続値か
- **教師あり vs 強化学習**: 正解ラベルが直接与えられるか、報酬から学習するか

## 補足（教師あり学習全般）

### 実務課題
- **データ準備**: ラベル付けのコスト（アノテーション）
- **クラス不均衡**: 少数クラスの学習が困難
- **特徴量設計**: ドメイン知識が性能に直結（深層学習以外）
- **モデル選択**: 解釈性 vs 精度のトレードオフ

### 最新動向
- **深層学習**: 画像・音声・テキストで高精度
- **転移学習**: 事前学習モデルの活用
- **AutoML**: ハイパーパラメータ自動調整
