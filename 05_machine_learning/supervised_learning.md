# 教師あり学習

## 要点
- 正解ラベル付きデータから入力→出力の関係を学習する機械学習手法。分類（離散値予測）と回帰（連続値予測）の2種類。
- 代表手法：線形回帰、ロジスティック回帰、決定木、ランダムフォレスト、SVM、ニューラルネットワーク。
- 学習データ・検証データ・テストデータに分割し、汎化性能を評価。過学習に注意。

## 定義
教師あり学習（Supervised Learning）とは、入力データ $x$ と対応する正解ラベル（教師信号）$y$ のペア $(x, y)$ を用いて、入力から出力への写像 $f: x \rightarrow y$ を学習する機械学習の枠組み。

## 重要キーワード
- **分類（Classification）**: 離散的なクラスラベルを予測（例：画像分類、スパム判定）
- **回帰（Regression）**: 連続値を予測（例：価格予測、気温予測）
- **訓練データ（Training Data）**: モデル学習に使用する正解ラベル付きデータ
- **テストデータ（Test Data）**: モデル評価に使用する未学習データ
- **汎化性能（Generalization）**: 未知データに対する予測精度
- **過学習（Overfitting）**: 訓練データに過剰適合し、テストデータで性能低下

---

## サポートベクターマシン（SVM）

### 要点
SVMはマージン最大化で分離超平面を求める教師あり学習手法。カーネルトリックにより、元空間で非線形分離可能なデータを高次元空間に写像し線形分離可能にする。明示的な写像計算は不要で、カーネル関数（RBF、多項式等）の内積計算のみで実現。

### 定義
サポートベクターマシン（SVM、Support Vector Machine）は、クラス間のマージン（余白）を最大化する分離超平面を見つける二値分類手法。カーネルトリックを用いることで、非線形分離問題を高次元空間での線形分離問題に変換する。

### 重要キーワード
- **サポートベクター**: 分離超平面に最も近いデータ点（決定境界を定義）
- **マージン**: 分離超平面と最近傍データ点との距離
- **カーネルトリック**: 明示的な高次元写像なしに内積を計算する技法
- **カーネル関数**: RBFカーネル（ガウシアン）、多項式カーネル、線形カーネル等
- **ソフトマージン**: 誤分類を許容するための正則化パラメータ $C$
- **決定境界**: データを分離する超平面

### 詳細

#### 背景
線形SVMは線形分離可能なデータには有効だが、実世界のデータは非線形分離が一般的。カーネルトリックは、高次元空間での計算コストを回避しながら非線形分離を可能にする画期的な手法。

#### カーネルトリックの原理
1. **元空間**: 2次元データが非線形分離（円形の境界等）
2. **写像**: カーネル関数 $K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$ で高次元空間に写像
3. **線形分離**: 高次元空間では線形分離可能
4. **計算効率**: $\phi(x)$ を明示的に計算せず、カーネル関数 $K$ の値のみ計算

#### 代表的なカーネル関数
- **線形カーネル**: $K(x, y) = x \cdot y$
- **多項式カーネル**: $K(x, y) = (x \cdot y + c)^d$
- **RBFカーネル（ガウシアン）**: $K(x, y) = \exp(-\gamma \|x - y\|^2)$
- **シグモイドカーネル**: $K(x, y) = \tanh(\alpha x \cdot y + c)$

#### 図解（概念）
```
元空間（2次元）        高次元空間（3次元以上）
   +  -               すべて線形分離可能
  + - +         →          +++++++
   - +                     -------
非線形境界              線形分離超平面
```

#### マージン最大化
目的関数：
$$\max \frac{2}{\|w\|} \quad \text{s.t.} \quad y_i(w \cdot x_i + b) \geq 1$$

ソフトマージンSVM（誤分類許容）：
$$\min \frac{1}{2}\|w\|^2 + C\sum_i \xi_i$$

### 試験での問われ方

#### 典型設問
- **「カーネル関数で高次元写像し線形分離する手法」→SVM**
- カーネルトリックの利点（明示的写像不要、計算効率）
- 代表的カーネル関数の種類（RBF、多項式）
- マージン最大化の意味と汎化性能との関係

#### ひっかけポイントと違いの整理

| 項目 | SVM | ロジスティック回帰 | ニューラルネットワーク | 決定木 |
|------|-----|------------------|---------------------|--------|
| **手法** | マージン最大化 | 確率モデル | 非線形変換の積層 | ルール分岐 |
| **カーネル** | カーネルトリック使用 | 通常使わない | 不要（活性化関数） | 不要 |
| **高次元写像** | 可能 | 困難 | 中間層で実現 | 不要 |
| **解釈性** | 低（カーネル使用時） | 高 | 低 | 高 |
| **計算コスト** | 中〜高 | 低 | 高 | 低 |
| **多クラス** | 拡張必要（OvO/OvR） | 直接対応 | 直接対応 | 直接対応 |

**混同注意**:
- **ニューラルネットワークとの違い**: SVMはカーネル関数で写像、NNは中間層の非線形変換で実現
- **ロジスティック回帰との違い**: SVMはマージン最大化、ロジスティック回帰は確率最大化（最尤推定）
- **k-NNとの違い**: SVMはモデルベース（サポートベクター保持）、k-NNはインスタンスベース（全データ保持）

**出題パターン**:
- 「カーネル関数を用いて高次元空間に写像」→**SVM**
- 「マージン最大化による分類」→**SVM**
- 「サポートベクターのみで決定境界を定義」→**SVM**
- 「非線形分離可能なデータを線形分離する手法」→**カーネルトリックを用いたSVM**

### 補足

#### 実務観点
- **用途**: テキスト分類（スパムフィルタ）、画像認識（深層学習以前）、バイオインフォマティクス
- **パラメータ調整**: カーネル関数の選択、$C$（正則化）、$\gamma$（RBFカーネルの幅）が性能に大きく影響
- **スケーリング**: 特徴量のスケールに敏感なため標準化が必須
- **計算コスト**: サンプル数 $n$ に対して $O(n^2)$〜$O(n^3)$、大規模データには不向き
- **実装**: scikit-learn（`SVC`, `LinearSVC`）、LIBSVM等

#### 深層学習との比較
- **深層学習以前**: SVMは画像認識・テキスト分類で最高性能
- **現在**: 大規模データ・複雑なタスクでは深層学習が優位
- **利点**: 少数サンプルではSVMが有効、理論的基盤が明確

#### 関連トピック
- [評価指標](evaluation_metrics.md) - 精度・再現率・F1
- [過学習・汎化](overfitting_underfitting.md) - マージン最大化と汎化性能
- [特徴量エンジニアリング](feature_engineering.md) - カーネル選択の前提

---

## 主要な教師あり学習手法

---

## 損失関数

### 要点
損失関数（Loss Function）は、モデルの予測値と正解データとの「ずれ」を定量化する関数。学習時に損失を最小化することでモデルのパラメータを最適化。タスク（回帰・分類）により適切な損失関数を選択。誤差関数・コスト関数・目的関数とも呼ばれる。

### 定義
損失関数 $L(\hat{y}, y)$ は、モデルの予測値 $\hat{y}$ と正解ラベル $y$ との差分を計算する関数。学習の目的は損失関数を最小化するパラメータ $\theta$ を見つけること：
$$\theta^* = \arg\min_\theta \sum_{i=1}^N L(f_\theta(x_i), y_i)$$

### 重要キーワード
- **損失関数（Loss Function）**: 1サンプルの予測誤差を測る関数
- **コスト関数（Cost Function）**: 全訓練データの損失の平均（損失関数とほぼ同義で使われることも多い）
- **目的関数（Objective Function）**: 最適化の対象となる関数（損失関数に正則化項を加えた形）
- **MSE（Mean Squared Error）**: 回帰タスクの標準的損失関数
- **交差エントロピー（Cross Entropy）**: 分類タスクの標準的損失関数
- **誤差逆伝播（Backpropagation）**: 損失関数の勾配を計算し、パラメータを更新

### 詳細

#### 背景
機械学習モデルの学習は「予測と正解の差を小さくする」最適化問題。この「差」を数値化する関数が損失関数。適切な損失関数の選択は学習の成否を左右する。

#### 回帰タスクの損失関数

**1. 平均二乗誤差（MSE, Mean Squared Error）**
$$\text{MSE} = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2$$
- **特徴**: 誤差を二乗するため、大きな誤差を強く罰する
- **用途**: 線形回帰、回帰ニューラルネットワーク
- **利点**: 微分可能、数学的扱いが容易
- **欠点**: 外れ値に敏感

**2. 平均絶対誤差（MAE, Mean Absolute Error）**
$$\text{MAE} = \frac{1}{N}\sum_{i=1}^N |y_i - \hat{y}_i|$$
- **特徴**: 誤差の絶対値を取る
- **利点**: 外れ値に頑健
- **欠点**: 原点で微分不可能（最適化が難しい場合あり）

**3. Huber損失**
$$L_\delta(y, \hat{y}) = \begin{cases} \frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\ \delta |y - \hat{y}| - \frac{1}{2}\delta^2 & \text{otherwise} \end{cases}$$
- **特徴**: MSEとMAEのハイブリッド
- **利点**: 小誤差はMSE的、大誤差はMAE的に扱い、外れ値に頑健かつ微分可能

#### 分類タスクの損失関数

**1. 交差エントロピー（Cross Entropy）**
二値分類：
$$L = -[y \log(\hat{y}) + (1-y)\log(1-\hat{y})]$$

多クラス分類：
$$L = -\sum_{c=1}^C y_c \log(\hat{y}_c)$$
- **特徴**: 確率分布間の「距離」を測る（情報理論由来）
- **用途**: ロジスティック回帰、ニューラルネットワークの分類
- **利点**: 確率的解釈が可能、勾配が適切
- **前提**: 出力がソフトマックス/シグモイドで正規化された確率

**2. ヒンジ損失（Hinge Loss）**
$$L = \max(0, 1 - y \cdot \hat{y})$$
- **用途**: SVM（サポートベクターマシン）
- **特徴**: マージン最大化に適した損失関数

#### 図解（概念）
```
学習プロセス:
入力データ x → モデル f(x) → 予測値 ŷ
                                  ↓
正解データ y → 損失関数 L(ŷ, y) → 損失値
                                  ↓
         最適化（勾配降下法等）← 勾配計算（誤差逆伝播）
                ↓
         パラメータ更新
```

#### 損失関数と評価指標の違い
| 項目 | 損失関数（Loss Function） | 評価指標（Evaluation Metric） |
|------|-------------------------|---------------------------|
| **目的** | 学習時の最適化目標 | モデル性能の評価 |
| **使用場面** | 訓練中のパラメータ更新 | 訓練後の性能測定 |
| **微分可能性** | 必須（勾配計算のため） | 不要 |
| **例（分類）** | 交差エントロピー | 精度、F1スコア、AUC |
| **例（回帰）** | MSE | MAE、R² |

**重要**: 損失関数は微分可能である必要があるが、評価指標（精度等）は微分不可能でも構わない。

### 試験での問われ方

#### 典型設問
- **「モデルの出力と正解データとの差分を計算する」→損失関数**
- 回帰タスクの代表的損失関数→MSE
- 分類タスクの代表的損失関数→交差エントロピー
- 損失関数と評価指標の違い

#### ひっかけポイントと違いの整理

| 項目 | 損失関数 | 評価指標 | 活性化関数 | 最適化手法 |
|------|---------|---------|-----------|----------|
| **役割** | 予測誤差を定量化 | 性能を評価 | 非線形変換 | パラメータ更新方法 |
| **使用タイミング** | 訓練中 | 訓練後 | 各層の出力 | 訓練中 |
| **微分可能性** | 必須 | 不要 | 必須（ReLU除く） | - |
| **例** | MSE、交差エントロピー | 精度、F1 | ReLU、シグモイド | SGD、Adam |

**混同注意**:
- **損失関数 vs 評価指標**: 損失関数は微分可能で学習に使用、評価指標は人間が解釈しやすい指標
- **損失関数 vs 活性化関数**: 損失関数は出力と正解の差、活性化関数は層の出力変換
- **損失関数 vs 最適化手法**: 損失関数は「何を」最小化するか、最適化手法は「どう」最小化するか

**出題パターン**:
- 「機械学習モデルを学習するとき、モデルの出力と正解データとの間の差分を計算するために、（**損失関数**）が使用される」
- 「回帰問題で予測値と実測値の二乗誤差を最小化する」→**MSE損失**
- 「確率出力と正解ラベルの交差エントロピーを最小化」→**分類の交差エントロピー損失**
- 「損失関数の勾配を計算し、パラメータを更新する手法」→**誤差逆伝播 + 勾配降下法**

### 補足

#### 実務観点
- **タスクに応じた選択**: 回帰→MSE/MAE、二値分類→バイナリ交差エントロピー、多クラス分類→カテゴリカル交差エントロピー
- **カスタム損失関数**: ビジネス目標に合わせた損失関数の設計（例：偽陽性と偽陰性のコストが異なる場合）
- **正則化項**: 過学習防止のため、損失関数に L1/L2 正則化項を追加
  $$L_{\text{total}} = L_{\text{loss}} + \lambda R(\theta)$$
- **実装**: TensorFlow/PyTorchでは組み込み損失関数が豊富（`nn.MSELoss()`, `nn.CrossEntropyLoss()` 等）

#### デバッグのコツ
- **損失が減らない**: 学習率が大きすぎる/小さすぎる、データの前処理不適切
- **損失が発散**: 学習率が大きすぎる、勾配爆発
- **損失は減るが精度上がらない**: 評価指標と損失関数のミスマッチ、データ不均衡

#### 関連トピック
- [誤差逆伝播](../06_deep_learning/backpropagation.md) - 損失関数の勾配計算
- [最適化手法](../10_math_statistics/optimization.md) - 損失関数の最小化アルゴリズム
- [評価指標](evaluation_metrics.md) - 精度・再現率・F1等の評価指標
- [過学習と汎化](overfitting_underfitting.md) - 正則化項の追加

---

### 線形回帰
- **定義**: 連続値を予測する最も基本的な回帰手法。$y = w^T x + b$
- **損失関数**: 平均二乗誤差（MSE）
- **用途**: 価格予測、需要予測

### ロジスティック回帰
- **定義**: シグモイド関数で確率を出力する二値分類手法
- **損失関数**: 交差エントロピー（クロスエントロピー）
- **用途**: 医療診断、与信判定

### 決定木
- **定義**: データを特徴量で分岐させ、木構造で分類・回帰
- **利点**: 解釈性が高い、非線形境界に対応
- **欠点**: 過学習しやすい

### ランダムフォレスト
- **定義**: 複数の決定木をアンサンブルし多数決で予測
- **利点**: 高精度、過学習抑制、特徴量重要度が分かる
- **欠点**: 解釈性が下がる、計算コスト増

### ニューラルネットワーク
- **定義**: 非線形活性化関数を持つ多層パーセプトロン
- **利点**: 表現力が高い、大規模データで高精度
- **欠点**: 学習コスト大、解釈性低

---

## 試験での問われ方（教師あり学習全般）

### 典型設問
- 教師あり学習の定義（正解ラベル使用）
- 分類と回帰の違い
- 代表的手法の特徴と用途
- 訓練データ・検証データ・テストデータの役割
- 過学習と汎化性能のトレードオフ

### 引っ掛けポイント
- **教師あり vs 教師なし**: 正解ラベルの有無で判別
- **分類 vs 回帰**: 出力が離散値か連続値か
- **教師あり vs 強化学習**: 正解ラベルが直接与えられるか、報酬から学習するか

## 補足（教師あり学習全般）

### 実務課題
- **データ準備**: ラベル付けのコスト（アノテーション）
- **クラス不均衡**: 少数クラスの学習が困難
- **特徴量設計**: ドメイン知識が性能に直結（深層学習以外）
- **モデル選択**: 解釈性 vs 精度のトレードオフ

### 最新動向
- **深層学習**: 画像・音声・テキストで高精度
- **転移学習**: 事前学習モデルの活用
- **AutoML**: ハイパーパラメータ自動調整
