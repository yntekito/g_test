# 評価指標（Evaluation Metrics）

## 要点
- 機械学習モデルの性能を定量評価する指標群。タスク（分類/回帰）と要件（外れ値の扱い、解釈性、コスト）によって適切な指標を選択
- 分類：精度（Accuracy）、適合率・再現率・F値、AUC-ROCが主要。不均衡データでは精度だけでは不十分
- 回帰：RMSE（外れ値に敏感）、MAE（頑健）、R²（説明力）を使い分け

## 定義
機械学習モデルの予測性能を数値化する指標。タスクの特性（分類/回帰、データ分布、ビジネス要件）に応じて選択する。

---

## 分類タスクの評価指標

### 混同行列（Confusion Matrix）
予測と実際のラベルを集計した2×2表（二値分類の場合）

|  | 予測：Positive | 予測：Negative |
|---|---|---|
| **実際：Positive** | TP（真陽性） | FN（偽陰性） |
| **実際：Negative** | FP（偽陽性） | TN（真陰性） |

### 精度（Accuracy）
- **定義**: $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$
- **意味**: 全予測のうち正解した割合
- **注意**: 不均衡データ（例：陽性1%）では高精度でも無意味な場合あり

### 適合率（Precision）
- **定義**: $\text{Precision} = \frac{TP}{TP + FP}$
- **意味**: Positiveと予測したもののうち実際にPositiveだった割合
- **適用**: スパム検定（誤判定を減らしたい）、医療診断の確信度

### 再現率（Recall / 感度 Sensitivity）
- **定義**: $\text{Recall} = \frac{TP}{TP + FN}$
- **意味**: 実際のPositiveのうち正しく検出できた割合
- **適用**: がん検診（見逃しを減らしたい）、異常検知

### F値（F-measure / F1スコア）
- **定義**: $F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$（調和平均）
- **意味**: 適合率と再現率のバランス指標
- **利点**: 不均衡データでも有効。単一指標で評価可能

### AUC-ROC（ROC曲線とAUC）★試験頻出

#### 定義
**ROC曲線（Receiver Operating Characteristic Curve）**は、二値分類器の性能を閾値に依存せず評価する曲線。横軸に**偽陽性率（FPR）**、縦軸に**真陽性率（TPR = 再現率）**をプロットし、分類閾値を変化させたときの性能変化を可視化する。

**AUC（Area Under the Curve）**は、ROC曲線の下側の面積。0.5（ランダム予測）から1.0（完全分類）の範囲で、値が大きいほど性能が良い。

#### 重要キーワード
- **ROC曲線（Receiver Operating Characteristic Curve）**: 横軸FPR、縦軸TPRの性能曲線
- **AUC（Area Under the Curve）**: ROC曲線下の面積、0.5～1.0
- **TPR（True Positive Rate / 真陽性率）**: 再現率と同じ、$\frac{TP}{TP + FN}$
- **FPR（False Positive Rate / 偽陽性率）**: $\frac{FP}{FP + TN}$、実際の陰性を誤って陽性と判定する率
- **閾値（Threshold）**: 確率を陽性/陰性に分ける境界値
- **PR曲線（Precision-Recall Curve）**: 横軸再現率、縦軸適合率の曲線（不均衡データ向き）

#### 詳細

##### ROC曲線の基本

**軸の定義**:
- **横軸：FPR（偽陽性率）** = $\frac{FP}{FP + TN}$ = $\frac{\text{誤って陽性と判定した陰性}}{\text{実際の陰性全体}}$
- **縦軸：TPR（真陽性率）** = $\frac{TP}{TP + FN}$ = $\frac{\text{正しく検出した陽性}}{\text{実際の陽性全体}}$ = **再現率（Recall）**

**ROC曲線のプロット方法**:
1. 分類器が各サンプルに陽性確率を出力（例：0.1, 0.3, 0.7, 0.9）
2. 閾値を0から1まで変化させる（例：0, 0.2, 0.4, 0.6, 0.8, 1.0）
3. 各閾値で、確率≥閾値なら陽性、<閾値なら陰性と判定
4. 各閾値でFPRとTPRを計算
5. (FPR, TPR)の点をプロット、線で結ぶ

**曲線の形状**:
```
TPR
1.0 ┤     ●━━━━━━━●  ← 理想的な分類器（左上角を通る）
    │    ╱
0.8 ┤   ╱ ●         ← 良い分類器（左上寄り）
    │  ╱  
0.6 ┤ ●   ╱━━━━━━   ← ランダム予測（対角線）
    │╱   ╱
0.4 ┤   ╱ ●         ← 実際の曲線
    │  ╱
0.2 ┤ ●
    │╱
0.0 ┤●━━━━━━━━━━━━━━━━━━━━━→ FPR
    0.0  0.2  0.4  0.6  0.8  1.0
```

- **左上角（0, 1）**: 完全分類（FPR=0、TPR=1）
- **対角線**: ランダム予測（AUC=0.5）
- **左上に膨らむ**: 良い分類器（AUC > 0.5）
- **右下に膨らむ**: 悪い分類器（予測を反転すれば良くなる）

##### モデルが予測できる場合とできない場合のROC曲線★試験頻出

**ケース1：モデルが完全に予測できる場合（完全分類器）**

**特徴**：
- すべての陽性サンプルに高い確率を、すべての陰性サンプルに低い確率を出力
- 陽性と陰性の確率分布が完全に分離

**ROC曲線の形状**：
```
TPR
1.0 ┤●━━━━━━━━━━━━━●  ← 完全分類（左上角を通る）
    │                │
    │                │
    │                │
    │                │
    │                │
0.0 ┤●━━━━━━━━━━━━━━━━━━━━━→ FPR
    0.0              1.0
```

- 曲線は **(0, 0) → (0, 1) → (1, 1)** の経路（左上角を通る）
- **AUC = 1.0**（最大値）
- **解釈**：FPR=0の状態でTPR=1を達成（偽陽性なしで全陽性を検出）

**具体例**：
```
【データ】
陽性サンプル：予測確率 [0.9, 0.8, 0.95, 0.85]
陰性サンプル：予測確率 [0.1, 0.2, 0.15, 0.05]

【閾値=0.5の場合】
- すべての陽性を正しく陽性と判定（TP=4, FN=0）
- すべての陰性を正しく陰性と判定（TN=4, FP=0）
- TPR = 4/4 = 1.0
- FPR = 0/4 = 0.0

→ どの閾値でも完全分離可能
```

**実務での意味**：
- 問題が非常に簡単、または特徴量が完璧
- 過学習の可能性（訓練データでのみAUC=1.0、テストで低下）
- データリークの疑い（未来情報が混入等）

---

**ケース2：モデルがまったく予測できない場合（ランダム予測）**

**特徴**：
- 陽性と陰性に同じような確率を出力
- 予測に情報がない（コイン投げと同じ）

**ROC曲線の形状**：
```
TPR
1.0 ┤              ●  ← ランダム予測（対角線）
    │            ╱
0.8 ┤          ╱
    │        ╱  
0.6 ┤      ╱
    │    ╱
0.4 ┤  ╱
    │╱
0.2 ┤╱
0.0 ┤●━━━━━━━━━━━━━━━━━━━━━→ FPR
    0.0  0.2  0.4  0.6  0.8  1.0
```

- 曲線は **対角線**（45度の直線）
- **AUC = 0.5**（ランダムレベル）
- **解釈**：TPRとFPRが同じ比率で増加（偽陽性と真陽性が同程度）

**具体例**：
```
【データ】
陽性サンプル：予測確率 [0.6, 0.3, 0.7, 0.4]
陰性サンプル：予測確率 [0.5, 0.4, 0.6, 0.3]

【閾値=0.5の場合】
- 陽性の半分を陽性と判定（TP=2, FN=2）
- 陰性の半分を陽性と誤判定（FP=2, TN=2）
- TPR = 2/4 = 0.5
- FPR = 2/4 = 0.5

→ どの閾値でもTPR ≈ FPR（対角線）
```

**実務での意味**：
- 特徴量に予測能力がない
- モデルが学習できていない
- 問題設定のミス（目的変数と説明変数が無関係）

---

**ケース3：モデルが部分的に予測できる場合（実際の分類器）**

**特徴**：
- 陽性と陰性の確率分布が部分的に重複
- 完全ではないが、ランダムよりは良い

**ROC曲線の形状**：
```
TPR
1.0 ┤       ●━━━━━●  ← 実際の分類器（左上に膨らむ）
    │      ╱
0.8 ┤    ●╱
    │   ╱ ╱
0.6 ┤  ● ╱  ← 対角線（ランダム）
    │ ╱ ╱
0.4 ┤●╱╱
    │╱
0.2 ┤●
0.0 ┤●━━━━━━━━━━━━━━━━━━━━━→ FPR
    0.0  0.2  0.4  0.6  0.8  1.0
```

- 曲線は **対角線より左上**に位置
- **AUC = 0.5～1.0**（典型的には0.7～0.9）
- **解釈**：ランダムより優れているが完全ではない

---

**3つのケースの比較表**

| ケース | ROC曲線の形状 | AUC | 意味 | TPR vs FPR |
|--------|--------------|-----|------|-----------|
| **完全予測** | 左上角を通る（直角） | **1.0** | 完全分類 | FPR=0でTPR=1 |
| **ランダム予測** | 対角線（45度） | **0.5** | 情報なし | TPR = FPR |
| **部分的予測** | 左上に膨らむ曲線 | **0.5～1.0** | 実用的 | TPR > FPR |

---

**試験での問われ方**

**典型問題**：「モデルがまったく予測できない場合、ROC曲線はどうなるか。」

**選択肢例**：
- A. 左上角を通る直角の曲線になる → ❌ 完全予測の場合
- B. **対角線（45度の直線）になる** → ✅ **正解**（ランダム予測）
- C. 横軸に平行な直線になる → ❌ ありえない
- D. 縦軸に平行な直線になる → ❌ ありえない

**正解**: **B（対角線）**

---

**問題パターン2**：「モデルが完全に予測できる場合のAUCはいくつか。」

**選択肢例**：
- A. 0.0 → ❌
- B. 0.5 → ❌ ランダム予測
- C. **1.0** → ✅ **正解**（完全分類）
- D. 2.0 → ❌ AUCは最大1.0

**正解**: **C（1.0）**

---

**引っ掛けポイント**

| ひっかけ | 正しい理解 |
|----------|------------|
| ❌ ランダム予測でAUC=0 | ✅ ランダム予測でAUC=**0.5** |
| ❌ 完全予測でAUC=0.5 | ✅ 完全予測でAUC=**1.0** |
| ❌ AUC=0.5は悪いモデル | ✅ AUC=0.5は**ランダム（情報なし）** |
| ❌ 対角線は完全予測 | ✅ 対角線は**ランダム予測** |
| ❌ 左上角は悪いモデル | ✅ 左上角は**完全予測（最良）** |

**覚え方**：
- **左上 = 最良**（FPR低、TPR高）
- **対角線 = ランダム**（FPR = TPR）
- **AUC 0.5 = コイン投げ**
- **AUC 1.0 = 完璧**

##### AUC（Area Under the Curve）

**定義**:
ROC曲線の下側の面積。0.5～1.0の範囲を取る（理論上は0～1だが、0.5未満は予測を反転すれば改善）。

**解釈**:
- **AUC = 1.0**: 完全分類（全ての陽性を陰性より高い確率で予測）
- **AUC = 0.9～1.0**: 優秀なモデル
- **AUC = 0.8～0.9**: 良好なモデル
- **AUC = 0.7～0.8**: 許容範囲
- **AUC = 0.5**: ランダム予測（情報なし）
- **AUC < 0.5**: 予測を反転すべき（実装ミスの可能性）
- **AUC = 0.0**: 完全に逆予測（理論上可能だが実務では実装ミス）

**AUC = 0は可能か？★試験頻出**

**理論的には可能**：
- AUCの理論的な範囲は **0～1.0**
- **AUC = 0** は「すべての陽性サンプルの予測確率が、すべての陰性サンプルの予測確率より**低い**」ことを意味
- つまり、**完全に逆の予測**をしている状態（最悪の分類器）

**ROC曲線の形状（AUC=0の場合）**：
```
TPR
1.0 ┤●━━━━━━━━━━━━━━━━━━━●  ← 右下角を通る
    │                    ╱
    │                  ╱
    │                ╱
    │              ╱
    │            ╱
    │          ╱
    │        ╱
0.0 ┤●━━━━━╱━━━━━━━━━━━━━━━→ FPR
    0.0              1.0
```
- 経路: (0, 0) → (1, 0) → (1, 1)（右下角を通る）
- **対角線より右下**に位置

**実務での解釈**：

❌ **AUC=0が発生する典型的な原因**（実装ミス）：

1. **ラベルの反転**
   - 陽性と陰性を逆に定義している
   - 例：がん=0、正常=1 と設定してしまった

2. **予測確率の符号ミス**
   - 確率を1から引いてしまった
   - 例：`pred = 1 - model.predict_proba()`

3. **モデルの出力を誤解釈**
   - 陰性確率を陽性確率として使用
   - 例：2クラス分類で間違った列を選択

✅ **AUC=0の場合の対処法**：

**予測を反転すれば完璧なモデルになる**：
- AUC=0のモデルは、予測を反転すれば **AUC=1.0** になる
- 新しい確率 = 1 - 元の確率
- つまり、ラベルを逆にするか、確率を反転すれば完全分類器

**具体例**：
```
【AUC=0のモデル】
陽性サンプル：予測確率 [0.1, 0.2, 0.05, 0.15]  ← すべて低い
陰性サンプル：予測確率 [0.9, 0.8, 0.95, 0.85]  ← すべて高い

→ 完全に逆予測（AUC=0）

【予測を反転】
陽性サンプル：新確率 [0.9, 0.8, 0.95, 0.85]  ← 高い
陰性サンプル：新確率 [0.1, 0.2, 0.05, 0.15]  ← 低い

→ 完全分類（AUC=1.0）
```

**重要なポイント**：
- **AUC=0は理論上可能だが、実務では実装ミスを示唆**
- 発見したら、ラベルや予測の定義を確認すべき
- 予測を反転すれば完璧なモデルになる（情報自体は完全）
- **AUC=0.5付近が最悪**（情報がない）、AUC=0は「逆の情報がある」

**試験での問われ方**：

**典型問題**：「AUC=0のモデルについて、最も適切な説明を選べ。」

**選択肢例**：
- A. 完全にランダムな予測をしている → ❌ ランダムはAUC=0.5
- B. **予測を反転すればAUC=1.0になる** → ✅ **正解**
- C. 最も性能が悪く修正不可能 → ❌ 反転すれば完璧
- D. 実務で目指すべき理想的な値 → ❌ 目指すのはAUC=1.0

**正解**: **B**

**引っ掛けポイント**：

| ひっかけ | 正しい理解 |
|----------|------------|
| ❌ AUC=0は最悪で使えない | ✅ 予測反転で**AUC=1.0**（情報は完全） |
| ❌ AUC=0はランダム | ✅ ランダムは**AUC=0.5** |
| ❌ AUC=0は修正不可能 | ✅ ラベルか予測を反転すれば解決 |
| ❌ 最悪はAUC=0 | ✅ 最悪は**AUC=0.5**（情報なし） |

**AUCの性能順序**：
1. **AUC = 1.0**: 完全分類（最良）
2. **AUC = 0.7～0.9**: 良好なモデル
3. **AUC = 0.5**: ランダム予測（**最悪** - 情報なし）
4. **AUC = 0.0**: 完全逆予測（反転すれば最良）

→ **AUC=0.5が実質的に最悪**（改善不可能）

**確率的解釈**:
AUCは「ランダムに選んだ陽性サンプルの予測確率が、ランダムに選んだ陰性サンプルの予測確率より高い確率」を表す。

##### ROC曲線・AUCの利点

1. **閾値非依存**: 特定の閾値に依存せず、全体的な性能を評価
2. **不均衡データに適用可能**: クラス比に影響されにくい
3. **視覚的理解**: 曲線の形で性能を直感的に把握
4. **モデル比較**: 複数モデルのAUCを比較

##### ROC曲線・AUCについて最も不適切な選択肢（試験頻出）

✅ **ROC曲線・AUCについて適切な説明**:

1. **「横軸に偽陽性率（FPR）、縦軸に真陽性率（TPR）をプロットした曲線である」** → ✅ 適切
2. **「AUCは0.5から1.0の範囲で、1.0に近いほど性能が良い」** → ✅ 適切
3. **「閾値に依存せず、分類器の全体的な性能を評価できる」** → ✅ 適切
4. **「不均衡データにも適用可能である」** → ✅ 適切
5. **「二値分類タスクの評価指標である」** → ✅ 適切
6. **「AUC=0.5はランダム予測を意味する」** → ✅ 適切
7. **「真陽性率（TPR）は再現率（Recall）と同じである」** → ✅ 適切

❌ **ROC曲線・AUCについて最も不適切な説明**:

1. **「横軸に再現率、縦軸に適合率をプロットした曲線である」** → ❌ **最も不適切**
   - これは**PR曲線（Precision-Recall Curve）**の説明
   - ROC曲線は横軸FPR、縦軸TPR

2. **「回帰タスクの評価指標である」** → ❌ 不適切
   - 分類タスク用、回帰はRMSE/MAE等を使用

3. **「AUCが0.5未満であれば良いモデルである」** → ❌ 不適切
   - 0.5未満は悪いモデル（予測を反転すべき）

4. **「特定の閾値を1つに固定して評価する指標である」** → ❌ 不適切
   - 閾値を変化させて全体性能を評価（閾値非依存）

5. **「正例と負例のバランスが取れているデータ専用の指標である」** → ❌ 不適切
   - 不均衡データにも適用可能

6. **「AUCの値が小さいほど性能が良い」** → ❌ 不適切
   - 大きいほど良い（1.0が最良）

##### 典型問題：「ROC曲線、AUCについての説明として、最も不適切な選択肢を1つ選べ」

**選択肢例**：
- A. 横軸に偽陽性率（FPR）、縦軸に真陽性率（TPR）をプロットした曲線である → ✅ 適切
- B. AUCは0.5から1.0の範囲で、1.0に近いほど性能が良い → ✅ 適切
- C. **横軸に再現率、縦軸に適合率をプロットした曲線である** → ❌ **最も不適切**
- D. 閾値に依存せず、分類器の全体的な性能を評価できる → ✅ 適切

**正解**: **C（最も不適切）**

**理由**：
- **ROC曲線**：横軸FPR、縦軸TPR
- **PR曲線**：横軸再現率、縦軸適合率
- 選択肢CはPR曲線の説明であり、ROC曲線の説明として不適切

##### 引っ掛けポイント

| ひっかけ | 正しい理解 | 混同される概念 |
|----------|------------|---------------|
| ❌ 横軸：再現率、縦軸：適合率 | ✅ 横軸：FPR、縦軸：TPR | **PR曲線** |
| ❌ 回帰タスク用 | ✅ 分類タスク用 | RMSE/MAE |
| ❌ AUC小さいほど良い | ✅ AUC大きいほど良い | — |
| ❌ 閾値固定で評価 | ✅ 閾値を変化させて評価 | 単一閾値の精度 |
| ❌ 均衡データ専用 | ✅ 不均衡データにも適用可 | 精度（Accuracy） |
| ❌ TPR = 適合率 | ✅ TPR = 再現率 | 適合率 vs 再現率 |

**キーポイント**：
- **ROC曲線**：横軸**FPR**、縦軸**TPR（再現率）**
- **PR曲線**：横軸**再現率**、縦軸**適合率**
- **AUC**：**大きいほど良い**、0.5がランダム、1.0が完全
- **閾値非依存**：全閾値での性能を総合評価

##### ROC曲線 vs PR曲線の比較

| 項目 | ROC曲線 | PR曲線 |
|------|---------|--------|
| **横軸** | 偽陽性率（FPR） | 再現率（Recall） |
| **縦軸** | 真陽性率（TPR） | 適合率（Precision） |
| **適用場面** | 一般的な二値分類 | 不均衡データ |
| **ランダム予測** | 対角線（AUC=0.5） | 正例率の水平線 |
| **利点** | クラス比に頑健 | 少数クラスに焦点 |
| **欠点** | 極端な不均衡で楽観的 | クラス比に敏感 |

**使い分け**：
- **正例と負例が同程度**：ROC曲線
- **正例が極端に少ない**（例：異常検知、希少疾患）：PR曲線

#### 実例

##### 例1：閾値を変えたときの変化

**分類器の出力**（陽性確率）と正解ラベル：

| サンプル | 予測確率 | 正解 |
|---------|---------|------|
| 1 | 0.9 | 陽性 |
| 2 | 0.8 | 陽性 |
| 3 | 0.7 | 陰性 |
| 4 | 0.6 | 陽性 |
| 5 | 0.4 | 陰性 |
| 6 | 0.2 | 陰性 |

**閾値=0.5の場合**:
- 予測陽性：サンプル1, 2, 3, 4
- TP=3, FP=1, FN=0, TN=2
- TPR = 3/3 = 1.0
- FPR = 1/3 = 0.33

**閾値=0.7の場合**:
- 予測陽性：サンプル1, 2, 3
- TP=2, FP=1, FN=1, TN=2
- TPR = 2/3 = 0.67
- FPR = 1/3 = 0.33

→ 閾値を変えながら(FPR, TPR)をプロット → ROC曲線

##### 例2：AUCの計算（簡易版）

**ROC曲線の点**：
- (0.0, 0.0) → 閾値=1.0（全て陰性と予測）
- (0.2, 0.6)
- (0.4, 0.8)
- (0.6, 0.9)
- (1.0, 1.0) → 閾値=0.0（全て陽性と予測）

台形公式でAUCを計算：
AUC ≈ 0.85（この分類器は良好）

##### 例3：モデル比較

| モデル | AUC | 評価 |
|--------|-----|------|
| ロジスティック回帰 | 0.78 | 許容範囲 |
| ランダムフォレスト | 0.89 | 良好 |
| 勾配ブースティング | 0.92 | 優秀 |

→ 勾配ブースティングが最も優れている

#### 補足

##### 実務上の注意点
1. **不均衡データでの制限**: 正例が極端に少ない場合、ROC曲線は楽観的になる（PR曲線を併用）
2. **閾値の選定**: AUCは全体性能だが、実務では特定の閾値を選ぶ必要がある
3. **コスト考慮**: FPとFNのコストが異なる場合、適切な閾値を選択
4. **多クラス分類**: One-vs-Rest等で拡張可能

##### 関連概念
- **PR曲線**: 不均衡データ向け、横軸再現率・縦軸適合率
- **F値**: 適合率と再現率の調和平均
- **Confusion Matrix**: ROC曲線の各点での詳細な内訳

### AUC-ROC（まとめ）
- **ROC曲線**: 横軸にFPR（偽陽性率）、縦軸にTPR（真陽性率=再現率）をプロット
- **AUC**: ROC曲線の下側面積（0.5〜1.0、0.5はランダム予測、1.0は完全分類）
- **利点**: 閾値に依存せず全体性能を評価

---

## 回帰タスクの評価指標

### RMSE（Root Mean Squared Error / 平方根平均二乗誤差）
- **定義**: $\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}$
- **特徴**:
  - 誤差を2乗するため**外れ値に敏感**（大きな誤差を強調）
  - 元データと同じ単位
  - 微分可能→勾配降下法で最適化しやすい
- **適用**: 大きな誤差を避けたい場面（予測精度重視）

### MAE（Mean Absolute Error / 平均絶対誤差）
- **定義**: $\text{MAE} = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$
- **特徴**:
  - 誤差の絶対値の平均→**外れ値に頑健**（全誤差を均等に扱う）
  - 解釈が直感的（平均的な誤差量）
  - 原点（誤差=0）で微分不可能だが部分微分可能
- **適用**: 安定した評価が必要、外れ値が多い場合

### RMSE vs MAE の主な違い（重要）

| 観点 | RMSE | MAE |
|------|------|-----|
| **外れ値への感度** | **高い**（2乗で増幅） | **低い**（線形） |
| **解釈性** | やや複雑 | **直感的** |
| **最適化** | 微分可能（勾配法◎） | 原点で微分不可 |
| **適用場面** | 大誤差を避けたい | 安定評価が必要 |
| **単位** | 元データと同単位 | 元データと同単位 |

### R²（決定係数 / Coefficient of Determination）
- **定義**: $R^2 = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$
- **意味**: モデルがデータの分散をどれだけ説明できるか（0〜1、1に近いほど良い）
- **注意**: 説明変数を増やすと自動的に上がる→調整済みR²を使用

---

## 重要キーワード
- **混同行列（Confusion Matrix）**: TP/TN/FP/FNの集計表
- **精度（Accuracy）**: 全体の正解率。不均衡データでは注意
- **適合率（Precision）**: 予測陽性の精度（誤検知を減らす）
- **再現率（Recall）**: 実際の陽性の検出率（見逃しを減らす）
- **F1スコア**: 適合率と再現率の調和平均
- **AUC-ROC**: 閾値非依存の分類性能指標
- **RMSE**: 外れ値に敏感な回帰誤差（2乗平均の平方根）
- **MAE**: 外れ値に頑健な回帰誤差（絶対値の平均）
- **R²**: モデルの説明力（分散の説明割合）

## 詳細

### 分類と回帰での指標の使い分け
機械学習タスクは大きく**分類**（離散ラベル）と**回帰**（連続値）に分かれ、それぞれ適切な評価指標が異なる。

**分類**: クラス間の境界を学習。精度だけでなく、誤分類の種類（FP/FN）を区別する指標が重要。医療診断では再現率、スパムフィルタでは適合率を優先。

**回帰**: 連続値の予測誤差を評価。RMSEは大きな誤差にペナルティを与え、MAEは誤差を均等に扱う。予測の安定性とビジネス要件で選択。

### 不均衡データでの注意
クラス比が極端（例：陽性1%、陰性99%）な場合、常に陰性と予測しても精度99%を達成。このため**F値**や**AUC-ROC**を併用し、少数クラスの検出性能も評価する。

### 評価指標とモデル選択
指標はビジネス目標と直結：
- がん検診：再現率を最重視（見逃しのコストが高い）
- 広告配信：適合率を優先（無駄な配信を減らす）
- 需要予測：RMSEで大幅なずれを抑制

## 試験での問われ方

### パターン1：穴埋め問題（定義の暗記）★頻出

**典型問題**：
> 「機械学習の評価指標として、（A）は全ての予測のうち、正解した予測の割合のことである。また、（B）は正解と予測したもののうち、実際に正解であるものの割合、（C）は実際に正解であるもののうち、正しく正解と予測できたものの割合を指す。」

✅ **正解**：
- **(A) = 精度（Accuracy）**
- **(B) = 適合率（Precision）**
- **(C) = 再現率（Recall）**

---

**定義の暗記ポイント**：

| 指標 | 定義（覚え方） | 式 | キーワード |
|------|--------------|-----|-----------|
| **精度（Accuracy）** | **全体のうち**正解した割合 | $\frac{TP + TN}{全体}$ | 「全ての予測」 |
| **適合率（Precision）** | **予測陽性のうち**実際に陽性の割合 | $\frac{TP}{TP + FP}$ | 「正解と予測したもののうち」 |
| **再現率（Recall）** | **実際の陽性のうち**正しく検出した割合 | $\frac{TP}{TP + FN}$ | 「実際に正解であるもののうち」 |

**混同しやすいポイント**：
- 適合率：分母は「**予測陽性**（TP + FP）」
- 再現率：分母は「**実際の陽性**（TP + FN）」

**覚え方**：
- **精度**：「全部の中で何個正解？」→ 全体
- **適合率**：「私が『陽性』と言ったら本当に陽性？」→ 予測から見る
- **再現率**：「本当の陽性を何個見つけられた？」→ 実際から見る

---

### 混同行列で理解する

```
                予測
          陽性(P)  陰性(N)
実 陽性  |  100  |  20  | = 120 (実際の陽性)
際 陰性  |  30   | 850  | = 880 (実際の陰性)
          ↓       ↓
         130    870    = 1000 (全体)
        (予測陽性)(予測陰性)
```

**計算例**：
- **(A) 精度** = $\frac{100 + 850}{1000}$ = **0.95（95%）** ← 全体のうち正解
- **(B) 適合率** = $\frac{100}{130}$ = **0.77（77%）** ← 予測陽性のうち実際に陽性
- **(C) 再現率** = $\frac{100}{120}$ = **0.83（83%）** ← 実際の陽性のうち検出

---

### パターン2：選択肢問題

**典型的な出題パターン**
1. **RMSE vs MAE**: 「外れ値に敏感なのはどちらか」→ **RMSE**（2乗で増幅）
2. **精度の落とし穴**: 「不均衡データで精度90%は良いモデルか」→ 不十分（F値やAUCで評価）
3. **適合率 vs 再現率**: 「医療診断で重視すべきは」→ **再現率**（見逃し回避）
4. **F1スコアの意味**: 「調和平均を使う理由」→ 適合率・再現率のバランス（片方だけ高くても低評価）
5. **R²の解釈**: 「R²=0.9の意味」→ モデルがデータ分散の90%を説明

### 引っ掛けポイント
❌ 「MAEの方が外れ値に敏感」→ **逆**（RMSEが敏感）  
❌ 「RMSEは単位が異なる」→ 同単位（平方根で元に戻る）  
❌ 「精度が高ければ良いモデル」→ データ分布による（不均衡時は不十分）  
❌ 「再現率を上げれば適合率も上がる」→ **トレードオフ**関係（一方を上げると他方が下がる傾向）  
✅ 「RMSEは大きな誤差を重視する」→ **正解**

### 比較されやすい概念
- **精度 vs F値**: 不均衡データではF値が適切
- **適合率 vs 再現率**: タスク要件で優先度が変わる
- **RMSE vs MAE**: 外れ値の扱いが正反対
- **ROC vs PR曲線**: 不均衡データではPR曲線が有用

## 補足
- **実務では複数指標を併用**: 単一指標だけでは不十分。ビジネスKPIと紐付けて評価
- **クロスバリデーション**: 評価指標を複数分割で平均し、過学習を検証
- **閾値調整**: 分類では閾値（確率カットオフ）によって適合率・再現率が変動。ROC/PR曲線で最適点を探索
- **コスト考慮**: 誤分類のコスト（FPとFNの重み）が異なる場合、カスタム損失関数を設計
