# 転移学習（Transfer Learning）

## 要点
- 転移学習は**別タスクで学習した知識を新しいタスクに活用**する手法。少ないデータで高精度を実現。
- **ファインチューニング**が代表的手法で、事前学習済みモデルの重みを新タスクで微調整。
- ImageNet等の大規模データで事前学習したモデルを活用し、学習時間・データ量を大幅削減。

## 定義
**転移学習（Transfer Learning）**とは、あるタスク（ソースタスク）で学習した知識を、別のタスク（ターゲットタスク）の学習に活用する機械学習の手法。特に深層学習では、大規模データセットで事前学習（Pre-training）したモデルを、目的のタスクで微調整（Fine-tuning）することで、少ないデータでも高精度を実現できる。

**典型例**:
```
ImageNet（1000クラス、120万枚）で事前学習
         ↓
医療画像診断（100枚）でファインチューニング
         ↓
高精度な診断モデルを短期間で構築
```

## 重要キーワード
- **転移学習（Transfer Learning）**: 別タスクの知識を新タスクに活用
- **事前学習（Pre-training）**: 大規模データで汎用的な特徴を学習
- **ファインチューニング（Fine-tuning）**: 事前学習モデルを新タスクで微調整
- **ソースタスク**: 知識を提供する元のタスク（例: ImageNet分類）
- **ターゲットタスク**: 知識を適用する目的のタスク（例: 医療画像診断）
- **ドメイン適応（Domain Adaptation）**: ソースとターゲットのデータ分布の違いに対処
- **特徴抽出器（Feature Extractor）**: 事前学習モデルの中間層を固定して特徴量として利用
- **学習済みモデル（Pre-trained Model）**: ImageNet等で事前学習された公開モデル（ResNet、VGG等）
- **データ効率**: 少ないデータで学習可能になる転移学習の利点

## 詳細

### 背景と直観

**なぜ転移学習が有効か**:
- 深層学習は大量のデータと計算資源が必要
- 多くの実務タスクではデータが少ない（医療画像、製造業の欠陥検査等）
- 低レイヤーの特徴（エッジ、テクスチャ）はタスク間で共通

**人間の学習との類似**:
```
人間の例:
自転車に乗れる → オートバイも早く習得
英語を学んだ → フランス語の習得が容易

AI:
ImageNetで物体認識 → 医療画像診断も早く学習
一般文書で言語学習 → 法律文書の処理も容易
```

### 転移学習の基本プロセス

#### ステップ1: 事前学習（Pre-training）

**大規模データセットで学習**:
```
【ImageNet】
- 1000クラス
- 120万枚の画像
- 数週間の学習

↓

汎用的な視覚特徴を獲得:
- 低レイヤー: エッジ、色、テクスチャ
- 中レイヤー: パターン、形状
- 高レイヤー: 物体の部品（目、耳等）
```

**代表的な事前学習モデル**:
- **画像**: ResNet、VGG、EfficientNet、Vision Transformer
- **言語**: BERT、GPT、T5、LLaMA
- **音声**: Wav2Vec、Whisper

#### ステップ2: ファインチューニング（Fine-tuning）

**新タスクで微調整**:
```
事前学習モデル（ImageNet）
    ↓ 最終層を置換
医療画像用の出力層（3クラス: 正常/良性/悪性）
    ↓ 少量データで学習
ターゲットタスクのモデル
```

**調整の度合い**:
1. **全層ファインチューニング**: すべての層の重みを更新
2. **部分ファインチューニング**: 上位層のみ更新、下位層は固定
3. **特徴抽出**: 事前学習モデルを固定、最終層のみ学習

### 転移学習の戦略

#### 戦略1: ファインチューニング（Fine-tuning）★最も一般的

**手順**:
```python
# 疑似コード
model = load_pretrained_model("ResNet50", weights="ImageNet")
model.final_layer = new_layer(num_classes=3)  # 最終層を置換

# 低レイヤーは小さな学習率、高レイヤーは大きな学習率
for layer in model.low_layers:
    layer.learning_rate = 0.0001  # 小さく
for layer in model.high_layers:
    layer.learning_rate = 0.01    # 大きく

model.train(new_dataset)
```

**適用場面**:
- ソースとターゲットが類似（画像→画像、言語→言語）
- ターゲットデータが中程度（数百〜数千サンプル）

#### 戦略2: 特徴抽出（Feature Extraction）

**手順**:
```
事前学習モデル（固定）
    ↓ 中間層の出力を特徴量として抽出
[特徴ベクトル]
    ↓ 新しい分類器（SVM、ロジスティック回帰等）
ターゲットタスクの予測
```

**適用場面**:
- ターゲットデータが非常に少ない（数十〜数百サンプル）
- 計算資源が限られている
- ソースとターゲットが類似

#### 戦略3: 段階的ファインチューニング

**手順**:
```
1. 最終層のみ学習（他は固定）
2. 上位数層を解凍して学習
3. 全層を解凍して学習（学習率を徐々に小さく）
```

**利点**:
- 過学習を防ぐ
- 安定した学習

### データ量と転移学習戦略の関係

| ターゲットデータ量 | ソース類似度 | 推奨戦略 |
|-----------------|------------|---------|
| **少量（<100）** | 高い | 特徴抽出のみ |
| **少量（<100）** | 低い | 軽いファインチューニング |
| **中量（数百〜数千）** | 高い | 上位層ファインチューニング |
| **中量（数百〜数千）** | 低い | 全層ファインチューニング |
| **大量（>1万）** | 高い | 全層ファインチューニング |
| **大量（>1万）** | 低い | スクラッチ学習も検討 |

### 転移学習の効果

#### 1. データ効率の向上

**比較例（画像分類）**:
```
【スクラッチ学習】
- 必要データ: 10,000枚以上
- 学習時間: 数日〜数週間
- 精度: 85%

【転移学習】
- 必要データ: 100〜1,000枚
- 学習時間: 数時間
- 精度: 90%以上
```

#### 2. 学習の高速化

**学習エポック数の削減**:
- スクラッチ: 100〜200エポック
- 転移学習: 10〜50エポック

#### 3. 汎化性能の向上

- 事前学習で獲得した頑健な特徴
- 過学習のリスク低減

### ドメイン適応（Domain Adaptation）

**問題**:
ソースとターゲットのデータ分布が異なる場合、性能が低下

**例**:
```
ソース: 自然画像（ImageNet）
ターゲット: 医療画像（X線、MRI）
→ 分布が大きく異なる
```

**対策**:
1. **ドメイン固有の前処理**: ターゲットドメインに合わせた正規化
2. **段階的ファインチューニング**: 慎重に学習率を調整
3. **ドメイン適応手法**: 敵対的学習でドメイン不変な特徴を学習
4. **中間ドメインの活用**: 類似度の高いドメインで中間学習

### 実例

#### 1. 画像認識

**医療画像診断**:
```
ImageNetで事前学習（ResNet50）
    ↓
胸部X線データ（1,000枚）でファインチューニング
    ↓
肺炎検出精度: 95%（スクラッチでは80%）
```

**製造業の欠陥検査**:
```
ImageNetで事前学習
    ↓
製品画像（200枚）でファインチューニング
    ↓
欠陥検出精度: 98%
```

#### 2. 自然言語処理

**法律文書の分類**:
```
BERT（Wikipedia等で事前学習）
    ↓
法律文書（5,000件）でファインチューニング
    ↓
契約書分類精度: 92%
```

**感情分析**:
```
GPT-3（大規模テキストで事前学習）
    ↓
商品レビュー（1,000件）でファインチューニング
    ↓
感情分類精度: 94%
```

#### 3. 音声認識

**方言認識**:
```
英語音声認識モデル（事前学習）
    ↓
日本の方言音声（500発話）でファインチューニング
    ↓
方言認識精度: 85%
```

### 転移学習の限界

**失敗するケース**:
1. **ソースとターゲットが全く異なる**: 画像→テキスト等
2. **負の転移（Negative Transfer）**: 誤った知識が転移し性能低下
3. **データ品質の問題**: ターゲットデータにノイズや偏りが多い

**対策**:
- ソースタスクの慎重な選択
- 転移学習とスクラッチ学習の比較実験
- 段階的ファインチューニング

## 試験での問われ方

### 典型的な穴埋め問題

**問題**:
> 「別のタスクで学習した知識を、新しいタスクの識別に活用する手法を（　）という。」

**正解**: **転移学習（Transfer Learning）**

**問題**:
> 「大規模データセットで学習したモデルを、目的のタスクで微調整することを（　）という。」

**正解**: **ファインチューニング（Fine-tuning）**

### 選択肢問題のパターン

#### Q. 転移学習の説明として最も適切なものを選べ

**✅ 適切な選択肢**:
- 「ImageNetで学習したモデルを医療画像診断に活用する」
- 「少量のデータでも高精度なモデルを構築できる」
- 「事前学習済みモデルの重みを新タスクで微調整する」
- 「学習時間とデータ量を削減できる」

**❌ 不適切な選択肢**:
- 「複数タスクを同時に学習する」→ **マルチタスク学習**
- 「ラベルなしデータで学習する」→ **教師なし学習**
- 「強化学習で方策を学習する」→ **強化学習**
- 「データを増やして学習する」→ **データ拡張**

### 比較されやすい概念

| 概念 | 定義 | データ | タイミング |
|------|------|--------|-----------|
| **転移学習** | 別タスクの知識を活用 | ソース＋ターゲット | 逐次学習 |
| **マルチタスク学習** | 複数タスクを同時学習 | 複数タスク同時 | 並行学習 |
| **メタ学習** | 学習方法自体を学習 | 多数の小タスク | エピソード学習 |
| **ドメイン適応** | 分布の違いに対処 | ソース＋ターゲット | 分布整合 |
| **Few-shot学習** | 少数例から学習 | 極小データ | サポート集合 |

### 引っ掛けポイント

**1. マルチタスク学習との混同**
- ✗ 誤解: 「転移学習は複数タスクを同時に学習」
- ○ 事実: **逐次学習**（まずソース、次にターゲット）
- マルチタスクは同時学習

**2. ファインチューニングとの関係**
- 転移学習: 概念（別タスクの知識活用）
- ファインチューニング: 具体的手法（重みの微調整）

**3. データ量の誤解**
- ✗ 誤解: 「転移学習には大量のデータが必要」
- ○ 事実: ターゲットタスクは**少量で可能**（事前学習は大量必要）

**4. 適用範囲の誤解**
- ✗ 誤解: 「あらゆるタスクに適用可能」
- ○ 事実: ソースとターゲットの**類似度が重要**

### 頻出キーワードと判定

| キーワード | 関連概念 |
|----------|---------|
| 「別タスクの知識」「事前学習」 | 転移学習 |
| 「微調整」「重みの再利用」 | ファインチューニング |
| 「ImageNet」「BERT」 | 事前学習モデル |
| 「少量データで高精度」 | 転移学習の利点 |
| 「複数タスクを同時」 | マルチタスク学習（別概念） |

## 補足

### 実務的観点

**1. 事前学習モデルの選択**:
- **タスクの類似性**: 画像なら画像モデル、言語なら言語モデル
- **モデルサイズ**: 計算資源に応じて選択（ResNet50 vs EfficientNet-B7）
- **ライセンス**: 商用利用の可否を確認

**2. ファインチューニングの設定**:
- **学習率**: 事前学習時の1/10〜1/100を推奨
- **層別学習率**: 低レイヤーは小さく、高レイヤーは大きく
- **正則化**: Dropout、Early Stopping、Data Augmentation

**3. データ準備**:
- **前処理の統一**: 事前学習時と同じ正規化を適用
- **クラスバランス**: 不均衡データの場合は重み調整
- **データ拡張**: 回転、反転、色調変更等

**4. 評価**:
- **ベースライン比較**: スクラッチ学習との性能比較
- **ドメインシフト確認**: 訓練と本番環境の分布差
- **汎化性能**: 未知データでの精度検証

### 関連トピック
- [マルチタスク学習](multi_task_learning.md) - 複数タスクの同時学習
- [過学習対策](overfitting_underfitting.md) - 転移学習での過学習防止
- [教師あり学習](supervised_learning.md) - 基本的な学習パラダイム
- [ニューラルネットワーク基礎](../06_deep_learning/neural_network_basics.md) - 深層学習の基礎

### 発展トピック
- **Few-shot Learning**: 数例からの学習
- **Zero-shot Learning**: 未見クラスの認識
- **Meta-Learning**: 学習方法の学習（MAML等）
- **Self-supervised Learning**: ラベルなしでの事前学習
- **Domain Adaptation**: 敵対的学習によるドメイン適応
- **Curriculum Learning**: 易しいタスクから難しいタスクへ段階的学習
