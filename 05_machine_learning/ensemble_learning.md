# アンサンブル学習（Ensemble Learning）

## 要点
- 複数のモデル（弱学習器）を組み合わせて、単一モデルより高精度を実現する手法
- 主要手法：**バギング**（並列、分散低減）、**ブースティング**（逐次、バイアス低減）、**スタッキング**（異種モデル組合せ）
- 代表例：ランダムフォレスト（バギング）、XGBoost・AdaBoost（ブースティング）、Kaggleの上位手法

## 定義
アンサンブル学習は、複数の学習器（モデル）を訓練し、その予測を統合することで、単一モデルより高い性能を得る機械学習手法。「三人寄れば文殊の知恵」の原理を機械学習に適用。

---

## アンサンブル学習の基本原理

### なぜ複数モデルで性能が向上するか
1. **分散の低減**: 異なるデータで訓練した複数モデルの平均は、個別モデルより安定
2. **バイアスの低減**: 弱い学習器を逐次改善することで、全体として複雑なパターンを学習
3. **多様性の活用**: 異なるアルゴリズムや特徴量を組み合わせ、各モデルの長所を活用

### 前提条件
アンサンブルが有効に機能するには：
- 各モデルが**ある程度の精度**を持つ（ランダムより良い）
- モデル間に**多様性**がある（異なる誤りをする）

---

## 主要なアンサンブル手法

### 1. バギング（Bagging: Bootstrap Aggregating）

#### 概要
- **手法**: データをブートストラップサンプリング（復元抽出）し、複数モデルを**並列**に訓練
- **統合**: 回帰は平均、分類は多数決
- **効果**: **分散（過学習）を低減**

#### アルゴリズム
```
1. 元データからN個のブートストラップサンプルを生成（重複を許す）
2. 各サンプルで1つずつモデルを訓練（並列）
3. 予測時は全モデルの結果を平均（回帰）または多数決（分類）
```

#### 代表例：ランダムフォレスト
- 複数の決定木をバギングで組み合わせ
- 各決定木で**ランダムに特徴量を選択**し、多様性を確保
- 過学習しにくく、高精度、解釈性も比較的高い
- **最も有名なアンサンブル手法**

#### 特徴
**利点**：
- 並列化可能（高速）
- 過学習を抑制
- 実装が簡単

**欠点**：
- バイアスは低減しない（個別モデルがバイアスを持つと全体も持つ）
- 解釈性が低下（複数モデルの組合せ）

---

### 2. ブースティング（Boosting）

#### 概要
- **手法**: 弱学習器を**逐次的**に訓練し、前のモデルの誤りを重点的に学習
- **統合**: 重み付き多数決または加重和
- **効果**: **バイアス（学習不足）を低減**

#### アルゴリズム（AdaBoostの例）
```
1. 全サンプルに同じ重みを設定
2. 弱学習器を訓練
3. 誤分類したサンプルの重みを増加
4. 2-3を繰り返す（T回）
5. 各学習器の精度に応じた重みで最終予測
```

#### 代表例

##### AdaBoost（Adaptive Boosting）
- **最初のブースティング手法**（1996年、Freund & Schapire）
- 誤分類サンプルの重みを増加
- 弱学習器には決定木（深さ1〜2のstump）を使用
- 理論的に誤差率を指数的に減少

##### Gradient Boosting
- 損失関数の勾配方向に学習器を追加
- 残差（誤差）を次の学習器で予測
- AdaBoostより柔軟（任意の損失関数に対応）

##### XGBoost（eXtreme Gradient Boosting）
- Gradient Boostingの高速実装
- 正則化項で過学習抑制
- 並列処理、欠損値処理、枝刈りなど最適化
- **Kaggleで最も使われる手法**（2016年頃から）

##### LightGBM
- Microsoftが開発
- XGBoostより高速・省メモリ
- ヒストグラムベースの分割

##### CatBoost
- Yandexが開発
- カテゴリ変数の扱いに優れる
- 順序付きブースティング

#### 特徴
**利点**：
- 高精度（バイアスを効果的に低減）
- 弱学習器でも強力なモデルを構築

**欠点**：
- 逐次処理で時間がかかる（並列化困難）
- 過学習しやすい（正則化が必要）
- ノイズに敏感

---

### 3. スタッキング（Stacking）

#### 概要
- **手法**: 異なる種類のモデル（レベル0）を訓練し、その予測を入力として上位モデル（メタ学習器、レベル1）を訓練
- **統合**: メタ学習器が最終予測
- **効果**: 異なるアルゴリズムの長所を統合

#### アルゴリズム
```
1. レベル0モデル（SVM、ランダムフォレスト、ニューラルネット等）を訓練
2. 各モデルの予測を新しい特徴量として抽出
3. レベル1モデル（メタ学習器）を訓練
4. 予測時は同じ流れで統合
```

#### 特徴
**利点**：
- 異なるアルゴリズムの長所を活用
- 高精度（Kaggle上位で頻用）

**欠点**：
- 計算コスト大
- 過学習リスク（クロスバリデーションで対策）
- 実装が複雑

---

### 4. その他の手法

#### ブレンディング（Blending）
- スタッキングの簡易版
- ホールドアウト検証用データでメタ学習器を訓練

#### バランス型アンサンブル
- 不均衡データに対応
- 各学習器を異なるサンプリングで訓練

---

## バギング vs ブースティング 比較表

| 項目 | バギング | ブースティング |
|------|---------|--------------|
| **訓練方法** | 並列 | 逐次 |
| **サンプリング** | ブートストラップ（重複あり） | 誤分類重視（重み付き） |
| **主な効果** | **分散低減**（過学習抑制） | **バイアス低減**（学習不足改善） |
| **代表例** | ランダムフォレスト | AdaBoost、XGBoost |
| **速度** | 速い（並列化可） | 遅い（逐次） |
| **過学習** | しにくい | しやすい |
| **ノイズ** | 頑健 | 敏感 |

---

## アンサンブル学習の活用例（試験頻出）

### 適切な例（試験で正解となる選択肢）

| 手法 | 説明 | 適用例 |
|------|------|--------|
| **ランダムフォレスト** | 複数の決定木をバギングで組み合わせ | **最頻出**、分類・回帰全般 |
| **XGBoost** | 勾配ブースティングの実装 | Kaggle、テーブルデータ |
| **AdaBoost** | 弱学習器を逐次改善 | 分類問題 |
| **複数モデルの多数決** | 異なるモデルの予測を統合 | 分類タスク |
| **複数モデルの平均** | 異なるモデルの予測の平均 | 回帰タスク |
| **スタッキング** | 異種モデルをメタ学習器で統合 | Kaggle上位 |

### 不適切な例（試験でひっかけとなる選択肢）

| 選択肢 | 判定 | 理由 |
|-------|------|------|
| 単一の深層学習モデル | ❌ | アンサンブルではない |
| 単一の決定木 | ❌ | アンサンブルではない |
| データ正規化 | ❌ | 前処理、アンサンブル学習ではない |
| 特徴量エンジニアリング | ❌ | データ加工、アンサンブル学習ではない |
| クロスバリデーション | ❌ | 評価手法、アンサンブルではない |
| ドロップアウト | ❌ | 正則化手法、厳密にはアンサンブルでない |

**注意**: ドロップアウト（ニューラルネット）は「暗黙的なアンサンブル」と解釈されることもあるが、試験では通常、明示的なアンサンブル手法（バギング・ブースティング等）を問う。

---

## 重要キーワード
- **アンサンブル学習（Ensemble Learning）**: 複数モデルを組み合わせる手法
- **弱学習器（Weak Learner）**: 単独では精度が低いモデル（ランダムよりは良い）
- **バギング（Bagging）**: 並列訓練で分散低減
- **ブースティング（Boosting）**: 逐次訓練でバイアス低減
- **スタッキング（Stacking）**: 異種モデルをメタ学習器で統合
- **ランダムフォレスト**: 決定木のバギング、最も有名なアンサンブル
- **XGBoost**: 勾配ブースティングの高速実装
- **AdaBoost**: 最初のブースティング手法
- **多数決（Voting）**: 分類での統合方法
- **平均（Averaging）**: 回帰での統合方法

---

## 実例

### Kaggle（データサイエンスコンペ）での活用
- **上位入賞の必須技術**: ほぼ全ての上位ソリューションがアンサンブルを使用
- **典型的構成**: XGBoost + LightGBM + ニューラルネット をスタッキング
- **効果**: 単一モデルより数%の精度向上（順位に大きく影響）

### 実務での活用例
- **クレジットスコアリング**: XGBoostで貸倒リスク予測
- **広告クリック率予測**: LightGBMで高速処理
- **医療診断支援**: ランダムフォレストで疾患分類（解釈性と精度の両立）
- **不正検知**: AdaBoostで異常パターン検出

### 具体的な性能向上例
- **単一決定木**: 精度75%
- **ランダムフォレスト（100木）**: 精度85%（+10%向上）
- **XGBoost**: 精度88%（+3%向上）
- **スタッキング**: 精度89%（+1%向上）

---

## 試験での問われ方

### 典型的な出題パターン
**問：アンサンブル学習を活用した例として、最も適切な選択肢を1つ選べ**

✅ **適切な選択肢**：
- **「ランダムフォレストで複数の決定木を組み合わせる」** → ✅ **最も適切**（最頻出）
- **「XGBoostで弱学習器を逐次改善する」** → ✅ 適切（ブースティング）
- **「複数のモデルの予測を多数決で決定する」** → ✅ 適切（バギング）
- 「AdaBoostで誤分類を重点的に学習する」→ ✅ 適切
- 「異なるアルゴリズムをスタッキングで統合する」→ ✅ 適切

❌ **不適切な選択肢（試験で狙われる）**：
- **「単一の深層学習モデルで画像認識を行う」** → ❌ アンサンブルでない
- **「データを正規化して学習精度を向上させる」** → ❌ 前処理、アンサンブルでない
- **「単一の決定木でルールベースの分類を行う」** → ❌ アンサンブルでない
- **「特徴量エンジニアリングで新しい変数を作成する」** → ❌ データ加工、アンサンブルでない
- **「クロスバリデーションで汎化性能を評価する」** → ❌ 評価手法、アンサンブルでない

### 引っ掛けポイント
❌ 「深層学習もアンサンブルに含まれる」→ 単一モデルは不可（複数のニューラルネットを組み合わせれば可）  
❌ 「データ前処理もアンサンブルの一種」→ 前処理は別概念  
❌ 「単一モデルの精度が高ければアンサンブル不要」→ 実用的には有効だが、定義上はアンサンブルでない  
❌ 「ドロップアウトはアンサンブル」→ 研究上の解釈はあるが、試験では通常明示的手法を問う  
✅ 「ランダムフォレストは代表的なアンサンブル手法」→ **正解**  
✅ 「バギングとブースティングは異なる統合方法」→ **正解**  
✅ 「複数モデルを組み合わせることがアンサンブルの本質」→ **正解**

### 比較されやすい概念
- **バギング vs ブースティング**: 並列・分散低減 vs 逐次・バイアス低減
- **ランダムフォレスト vs XGBoost**: バギング vs ブースティング
- **単一モデル vs アンサンブル**: 解釈性・速度 vs 精度・頑健性
- **アンサンブル vs 前処理**: モデル統合 vs データ加工
- **アンサンブル vs 正則化**: モデル組合せ vs 過学習抑制手法

---

## 補足

### 実務での選択基準
- **精度重視**: XGBoost、スタッキング
- **速度重視**: LightGBM、並列化可能なバギング
- **解釈性重視**: ランダムフォレスト（特徴量重要度が明確）
- **安定性重視**: バギング（ブースティングよりノイズに頑健）

### 計算コストとトレードオフ
- **バギング**: 並列化で高速化可能
- **ブースティング**: 逐次処理で時間かかる
- **スタッキング**: 複数モデル訓練で最もコスト大

### 過学習への対策
- **バギング**: 元々過学習しにくい
- **ブースティング**: 正則化（木の深さ制限、学習率低減）、early stopping
- **スタッキング**: クロスバリデーション必須

### 理論的背景
- **バイアス-バリアンストレードオフ**: アンサンブルはバリアンス（またはバイアス）を低減
- **多様性の重要性**: 各モデルが異なる誤りをすることが前提
- **弱学習器の組合せ**: PAC学習理論でブースティングの有効性を証明

### 発展的トピック
- **ニューラルネットのアンサンブル**: Snapshot Ensemble、知識蒸留
- **ベイズモデル平均**: 確率的なモデル統合
- **AutoML**: アンサンブル手法の自動選択・最適化
