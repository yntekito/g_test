# 過学習と汎化（Overfitting and Generalization）

## 要点
- **過学習（Overfitting）**: 訓練データに過度に適合し、未知データへの性能が低下。対策：正則化、Dropout、Early Stopping。
- **汎化（Generalization）**: 未知データに対する予測性能。訓練誤差と汎化誤差の差が汎化ギャップ。
- **ダブルディセント**: モデルサイズ増加で性能が一度悪化後、再び向上する現象。深層学習で観測される。

## 定義
**過学習（Overfitting）**は、モデルが訓練データの細かいノイズやパターンまで学習してしまい、未知のテストデータに対する予測性能（汎化性能）が低下する現象。**汎化（Generalization）**は、訓練データ以外の未知データに対してもモデルが正確に予測できる能力。

## 重要キーワード
- **過学習（Overfitting）**: 訓練データに過度に適合し汎化性能が低下
- **未学習（Underfitting）**: モデルが単純すぎて訓練データすら学習できない
- **汎化（Generalization）**: 未知データへの予測性能
- **汎化誤差（Generalization Error）**: テストデータでの誤差
- **汎化ギャップ（Generalization Gap）**: 訓練誤差とテスト誤差の差
- **バイアス-バリアンストレードオフ**: モデルの複雑さと汎化性能のバランス
- **正則化（Regularization）**: 過学習を防ぐための制約
- **Early Stopping**: 検証誤差が悪化し始めたら学習を停止
- **ダブルディセント（Double Descent）**: モデルサイズ増加で性能が一度悪化後に再向上する現象

## 詳細

### 過学習（Overfitting）

#### 定義と症状
モデルが訓練データの**ノイズや特異なパターン**まで記憶してしまい、本質的なパターンの学習が不十分な状態。

**典型的な症状**：
- 訓練誤差は非常に小さい（ほぼゼロ）
- テスト誤差は大きい（汎化性能が低い）
- 訓練誤差とテスト誤差の差が大きい

#### 図解：学習曲線
```
誤差
 ↑
 │         ┌─ テスト誤差（上昇）
 │        ╱
 │       ╱
 │      ╱    ← 過学習開始
 │     ╱
 │    ╱
 │   ╱  ＼─ 訓練誤差（減少）
 │  ╱     ＼_____
 │ ╱            ＼_____
 └─────────────────────→ エポック数
   最適な停止点↑
```

#### 過学習の原因
1. **モデルが複雑すぎる**: パラメータ数 >> データ数
2. **訓練データが少ない**: 十分な例がない
3. **訓練時間が長すぎる**: 過度に学習
4. **ノイズが多い**: データに誤りや外れ値が多い
5. **正則化不足**: 制約がない

### 未学習（Underfitting）

#### 定義
モデルが単純すぎて、訓練データのパターンすら十分に学習できない状態。

**典型的な症状**：
- 訓練誤差が大きい
- テスト誤差も大きい
- 両方の誤差が近い値

#### 未学習の原因
1. **モデルが単純すぎる**: 線形モデルで非線形データを学習
2. **特徴量が不足**: 重要な情報が欠けている
3. **訓練時間が短すぎる**: 学習が不十分
4. **正則化が強すぎる**: 過度な制約

### 汎化（Generalization）

#### 定義
訓練データで学習したモデルが、**未知のデータに対してもうまく機能する**能力。

#### 汎化性能の評価
- **訓練誤差**: 訓練データでの誤差
- **検証誤差**: ハイパーパラメータ調整用の誤差
- **テスト誤差**: 最終評価用の誤差（汎化誤差）

$$\text{汎化ギャップ} = \text{テスト誤差} - \text{訓練誤差}$$

汎化ギャップが小さいほど、モデルは汎化している。

### バイアス-バリアンストレードオフ

#### 定義
モデルの**バイアス（偏り）**と**バリアンス（分散）**のバランス。

**バイアス（Bias）**：
- モデルの単純さによる誤差
- 高バイアス → 未学習

**バリアンス（Variance）**：
- 訓練データへの過敏性による誤差
- 高バリアンス → 過学習

#### トレードオフの図
```
誤差
 ↑
 │  ＼              ╱
 │   ＼  総誤差   ╱
 │    ＼        ╱
 │     ＼      ╱
 │      ＼    ╱
 │       ＼  ╱  ← 最適な複雑さ
 │    バイアス
 │         ╲ ╱
 │          X
 │         ╱ ╲
 │   バリアンス
 │       ╱     ╲____
 └─────────────────────→ モデルの複雑さ
   単純    最適    複雑
 （未学習）      （過学習）
```

### ダブルディセント現象（Double Descent）

#### 定義（★試験重要）
**ダブルディセント**は、モデルサイズ（パラメータ数）や訓練時間を増やしていくと：
1. **最初は性能が向上**（未学習から脱却）
2. **ある点で性能が最悪化**（補間閾値：interpolation threshold）
3. **さらに増やすと再び性能が向上**（過パラメータ化領域）

という**U字型の性能変化が2回起こる**現象。

#### 典型的な質問形式
> 「学習初期は性能が上がるものの、次第に性能が悪化し、モデルサイズや訓練時間を増やすと再び性能が上がる現象のことを何と呼ぶか。」

✅ **正解**: **ダブルディセント（Double Descent）**

#### 3つの領域

```
テスト誤差
 ↑
 │ ＼                    ╱
 │  ＼     伝統的      ╱  近代的
 │   ＼    リスク曲線 ╱   リスク曲線
 │    ＼            ╱
 │     ＼          ╱
 │      ＼      ╱╲
 │       ＼    ╱  ╲
 │        ＼  ╱    ╲___
 │         ╲╱         ＼___
 └─────────────────────────→ モデルサイズ
   未学習   最適  臨界点  過パラメータ化
     ①      ②     ③        ④
```

**①未学習領域（Underparameterized）**：
- モデルが単純すぎて訓練データを学習できない
- 性能が低い

**②古典的な最適点**：
- バイアス-バリアンストレードオフの最適点
- 従来の機械学習理論で推奨される複雑さ

**③補間閾値（Interpolation Threshold）**：
- モデルが訓練データを完全に記憶できる境界
- **性能が最悪**になる点（過学習のピーク）
- パラメータ数 ≈ データ数

**④過パラメータ化領域（Overparameterized）**：
- モデルが非常に大きい（パラメータ数 >> データ数）
- 深層学習の実用領域
- **性能が再び向上**（暗黙の正則化が働く）

#### なぜダブルディセントが起こるのか

**従来の理論との矛盾**：
- 古典的理論：モデルが大きすぎると必ず過学習
- 現実：巨大な深層学習モデルは汎化性能が高い

**現在の理解**：
1. **補間閾値付近**：訓練データにぎりぎり適合しようとして不安定
2. **過パラメータ化領域**：
   - 解の空間が広く、「滑らかな解」を見つけやすい
   - 暗黙の正則化（Implicit Regularization）が働く
   - SGDが滑らかな最小値を選好

#### 訓練時間によるダブルディセント

モデルサイズだけでなく、**訓練時間（エポック数）**でも同様の現象が観測される：

```
テスト誤差
 ↑
 │                     ╱
 │     Early        ╱
 │     Stopping   ╱
 │      ↓       ╱
 │      ＼    ╱╲
 │       ＼  ╱  ╲
 │        ╲╱    ╲____
 └─────────────────────→ 訓練時間
   未学習   過学習  再向上
```

1. 初期：未学習
2. 中期：過学習（Early Stoppingで停止すべき点）
3. 後期：性能が再び向上

#### 実例

**深層学習での観測**：
- ResNet等の巨大モデルは訓練データを完全に記憶しても汎化性能が高い
- GPT-3（1750億パラメータ）は過学習せず高性能

**ランダムフォレスト**：
- 木の数を増やすと一度性能が落ちた後、再び向上

#### ダブルディセントの意味

✅ **正しい理解**：
- 深層学習では「大きいモデル + 十分な訓練」が有効
- Early Stoppingが常に最適とは限らない
- 過パラメータ化は悪ではない

❌ **誤解**：
- 「モデルは常に小さい方が良い」→ 状況による
- 「過学習は必ず避けるべき」→ 補間閾値を超えれば再向上

### 過学習の対策

#### 1. 正則化（Regularization）

**L1正則化（Lasso）**：
$$\text{Loss} = \text{誤差} + \lambda \sum |w_i|$$
- スパース性を促進（一部の重みがゼロに）
- 特徴選択の効果

**L2正則化（Ridge）**：
$$\text{Loss} = \text{誤差} + \lambda \sum w_i^2$$
- 重みを小さく保つ
- 最も一般的

**Elastic Net**：
$$\text{Loss} = \text{誤差} + \lambda_1 \sum |w_i| + \lambda_2 \sum w_i^2$$
- L1とL2の組み合わせ

#### 2. Dropout

ニューラルネットワークで、訓練時に**ランダムにニューロンを無効化**：
```
通常:  ●─●─●─●
       ↓ ↓ ↓ ↓
Dropout: ●─×─●─×  (50%をランダムに無効化)
```

**効果**：
- アンサンブル学習に近い効果
- ニューロン間の過度な依存を防ぐ

#### 3. Early Stopping

検証誤差が悪化し始めたら学習を停止：
```
誤差
 ↑    ┌─ 検証誤差
 │   ╱
 │  ╱  ←ここで停止
 │ ╱
 │╱＼─ 訓練誤差
 └──────→ エポック
```

**注意**：ダブルディセント現象がある場合、早期停止が最適とは限らない。

#### 4. データ拡張（Data Augmentation）

訓練データを人工的に増やす：
- 画像：回転、反転、クロッピング、色調変換
- テキスト：同義語置換、バックトランスレーション
- 音声：ノイズ追加、ピッチ変換

#### 5. その他の手法

**Batch Normalization**：
- 各層の入力を正規化して学習安定化

**Weight Decay**：
- 重みを小さく保つ（L2正則化と同等）

**モデル簡略化**：
- 層数削減、パラメータ数削減

**交差検証**：
- データを分割して汎化性能を正確に評価

### 汎化性能の評価方法

#### ホールドアウト法
データを訓練・検証・テストに分割：
```
全データ
├─ 訓練（60%）: モデル学習
├─ 検証（20%）: ハイパーパラメータ調整
└─ テスト（20%）: 最終評価（一度だけ使用）
```

#### k-分割交差検証
データをk個に分割し、k回学習・評価：
- 詳細は [cross_validation.md](cross_validation.md) を参照

## 試験での問われ方
- **典型設問**：
  - 「訓練誤差は小さいがテスト誤差が大きい状態は？」→ **過学習（Overfitting）**
  - 「訓練誤差もテスト誤差も大きい状態は？」→ **未学習（Underfitting）**
  - 「過学習の対策として適切なものは？」→ 正則化、Dropout、Early Stopping、データ拡張
  - 「モデルサイズ増加で性能が一度悪化後に再向上する現象は？」→ **ダブルディセント（Double Descent）**
  - 「ダブルディセントで性能が最悪になる点は？」→ **補間閾値（Interpolation Threshold）**
- **比較されやすい概念**：
  - **過学習** vs **未学習**: 訓練誤差が小さい vs 大きい
  - **正則化** vs **Dropout**: 重みへの制約 vs ニューロンの無効化
  - **L1正則化** vs **L2正則化**: スパース性 vs 重みの縮小
  - **Early Stopping** vs **ダブルディセント**: 早期停止 vs 訓練継続で再向上
  - **バイアス** vs **バリアンス**: 単純さの誤差 vs 複雑さの誤差
- **引っ掛けポイント**：
  - ✅ 「過学習は訓練誤差が小さくテスト誤差が大きい」= **正しい**
  - ❌ 「過学習は訓練誤差が大きい」→ 誤り（未学習の症状）
  - ✅ 「L2正則化は重みを小さく保つ」= **正しい**
  - ❌ 「Dropoutは推論時も使用する」→ 誤り（訓練時のみ）
  - ✅ 「ダブルディセントでは巨大モデルが高性能」= **正しい**
  - ❌ 「ダブルディセントは常に起こる」→ 誤り（特定の条件下）
  - ❌ 「Early Stoppingが常に最適」→ 誤り（ダブルディセントでは継続が有効な場合も）
  - ❌ 「過学習は必ず避けるべき」→ 誤り（補間閾値を超えれば再向上）
- **頻出パターン**：
  - 過学習と未学習の見分け方（訓練誤差とテスト誤差の関係）
  - 過学習対策の列挙（正則化、Dropout、Early Stopping等）
  - 正則化の種類と効果（L1、L2、Elastic Net）
  - ダブルディセント現象の説明（3つの領域）
  - バイアス-バリアンストレードオフの理解

## 補足
- **実務的観点**：
  - 深層学習では過パラメータ化（巨大モデル）が標準的
  - Early Stoppingは一般的だが、ダブルディセントを考慮すると訓練継続が有効な場合も
  - 正則化とDropoutは併用可能（相乗効果）
  - データ拡張は最も効果的な過学習対策の一つ
  - 転移学習で事前学習済みモデルを使うと過学習リスク低減
  - ダブルディセントは2019年頃から注目された比較的新しい知見
- **関連トピック**：
  - [交差検証](cross_validation.md) - 汎化性能の評価手法
  - [評価指標](evaluation_metrics.md) - 性能評価の指標
  - [ニューラルネットワーク基礎](../06_deep_learning/neural_network_basics.md) - Dropout、Batch Normalization
  - [アンサンブル学習](ensemble_learning.md) - バイアス-バリアンストレードオフ
- **発展**：
  - Implicit Regularization（暗黙の正則化）
  - Neural Tangent Kernel（深層学習の理論的理解）
  - Lottery Ticket Hypothesis（スパースネットワークの理論）
  - Mixup、CutMix（データ拡張の発展形）
