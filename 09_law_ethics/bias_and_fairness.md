# バイアスと公平性（Bias and Fairness）

## 要点
- AIシステムの偏り（バイアス）は**訓練データ**に含まれる社会的偏見・歴史的差別が主原因。
- 技術的対策（データ多様化、公平性指標）と社会的配慮（倫理審査、透明性）の両面が必要。
- 完全な公平性は困難だが、バイアスの認識・測定・緩和努力が求められる。

## 定義
**アルゴリズムバイアス（Algorithmic Bias）**とは、AIシステムが特定の属性（人種、性別、年齢等）に基づいて不公平な判断を行う現象。訓練データの偏り、ラベル付けの偏見、アルゴリズム設計の問題等が原因で発生し、社会的不平等を再生産・増幅するリスクがある。

**公平性（Fairness）**には複数の定義があり、状況に応じて適切な基準を選択する必要がある（統計的公平性、個人的公平性、機会の平等等）。

## 重要キーワード
- **アルゴリズムバイアス**: AIの判断に含まれる系統的な偏り
- **訓練データのバイアス**: 過去の差別的慣行や偏った収集方法がデータに反映
- **サンプリングバイアス**: データ収集時の偏り（特定集団の過少/過剰代表）
- **ラベルバイアス**: 人間のアノテーション時の偏見が混入
- **プロキシ変数**: 保護属性と相関する変数（郵便番号→人種等）
- **統計的公平性**: 各グループで予測精度が等しい
- **個人的公平性**: 類似個人に類似判断
- **機会の平等**: 同じ能力なら同じ機会
- **COMPAS問題**: 再犯予測システムが黒人に不利な判定（ProPublica調査）
- **公平性のトレードオフ**: 異なる公平性基準を同時に満たすのは困難

## 詳細

### 背景と問題の所在

**AIバイアスの社会問題化**:
- **採用AI**: Amazon（2018年）の採用AIが女性を不利に評価（過去の男性中心採用データを学習）
- **顔認識**: 有色人種・女性の認識精度が低い（MIT Media Lab調査）
- **再犯予測**: COMPAS（米国）が黒人の再犯率を過大評価
- **ローン審査**: Apple Card（2019年）が女性に低い与信枠（性別バイアス疑惑）

**なぜ問題か**:
- 差別の自動化・大規模化
- 透明性の欠如（ブラックボックス）
- 説明責任の曖昧化
- 社会的不平等の固定化

### アルゴリズムバイアスの原因

#### 1. 訓練データのバイアス ★最重要

**歴史的バイアスの反映**:
```
過去の人事データ → 男性が多い → AIが「男性」を好条件と学習
過去の逮捕データ → 特定地域の過剰取締 → AIが地域で差別
```

**サンプリングバイアス**:
- データ収集時の偏り（インターネット調査→高齢者が少ない）
- センサーの限界（安価カメラ→暗い肌の認識精度低下）

**ラベルバイアス**:
- 人間のアノテーション時の先入観（「攻撃的」の判断に偏見混入）
- 歴史的判断の正当化（過去の裁判官の判決を学習→過去の差別を継承）

#### 2. プロキシ変数（代理変数）

**保護属性と相関する変数**:
| 直接利用禁止 | プロキシ変数（相関） |
|------------|------------------|
| 人種 | 郵便番号、名前 |
| 性別 | 職種、趣味 |
| 年齢 | クレジットヒストリーの長さ |

- 人種を入力に使わなくても、郵便番号から間接的に差別可能
- 完全な除去は困難（相関の連鎖）

#### 3. アルゴリズム設計の問題

- **評価指標の選択**: 精度重視→多数派に有利、少数派を無視
- **最適化目標**: 全体の利益最大化→マイノリティ犠牲
- **閾値設定**: グループごとの特性無視→一律適用で不公平

#### 4. フィードバックループ（バイアスの増幅）

```
バイアスある予測 → 偏った介入 → 偏ったデータ蓄積 → バイアス増幅
↑___________________________________________________________________|

例: 予測的ポリシング
「犯罪多発地域」予測 → 警察配置増 → 逮捕増 → データ上「犯罪多い」→ 予測強化
```

### 公平性の定義（複数の基準）

#### 1. 統計的公平性（Demographic Parity）
各グループで**陽性予測率が等しい**
$$P(\hat{Y}=1 | A=0) = P(\hat{Y}=1 | A=1)$$

例: 採用率が男女で等しい

#### 2. 機会の平等（Equal Opportunity）
**真陽性率**（適格者の採用率）が各グループで等しい
$$P(\hat{Y}=1 | Y=1, A=0) = P(\hat{Y}=1 | Y=1, A=1)$$

例: 能力ある人の採用率が男女で等しい

#### 3. 個人的公平性（Individual Fairness）
**類似個人に類似判断**
類似度が高い2人には類似した予測

#### 4. 反事実的公平性（Counterfactual Fairness）
保護属性が異なっても（他が同じなら）予測は同じ

**公平性のトレードオフ**:
- 統計的公平性と機会の平等を同時に満たすのは困難（Impossibility Theorem）
- ベースレート（実際の比率）が異なる場合は矛盾

### バイアス緩和の手法

#### 技術的アプローチ

**1. 前処理（Pre-processing）**:
- データの再サンプリング（過小代表グループを増やす）
- データ拡張（合成データ生成）
- 保護属性の除去（ただしプロキシ問題あり）

**2. インプロセス（In-processing）**:
- 公平性制約付き学習（最適化問題に公平性項を追加）
- 敵対的学習（保護属性予測を困難にする）

**3. 後処理（Post-processing）**:
- 閾値調整（グループごとに異なる閾値）
- 予測の校正（各グループで予測精度を等しくする）

#### 組織的・社会的アプローチ

**1. 多様なチーム構成**:
- 開発者の多様性確保
- バイアス発見能力の向上

**2. 倫理審査**:
- AIシステム導入前の影響評価
- 継続的なモニタリング

**3. 透明性と説明可能性**:
- 判断根拠の開示
- アルゴリズム監査の実施

**4. ステークホルダー参加**:
- 影響を受けるコミュニティの意見反映
- 参加型設計

### 実例と事件

**1. Amazonの採用AI（2018年）**:
- 過去10年の履歴書（男性中心）を学習
- 「女性」「女子大学」を含む履歴書を減点
- 開発中止

**2. COMPAS再犯予測（ProPublica, 2016年）**:
- 黒人被告を「高リスク」と過大評価（45% vs 白人23%）
- 偽陽性率に人種差

**3. 顔認識の精度差（MIT, 2018年）**:
- 白人男性: 誤認識率 < 1%
- 黒人女性: 誤認識率 35%
- 訓練データの偏り（白人画像が多い）

**4. ヘルスケアアルゴリズム（Science, 2019年）**:
- 医療費を基準に重症度予測
- 黒人は同じ健康状態でも医療費が低い（アクセス障壁）
- → 黒人患者を過少評価

## 試験での問われ方

### G検定選択肢問題：不適切な選択肢の見極め

#### 典型的な「不適切な選択肢」（誤解）

**❌ 誤答パターン1: AIの中立性神話**
- 「AIは客観的データに基づくため、人間のような差別をしない」
  - **誤り**: データ自体に偏見が含まれる
  
**❌ 誤答パターン2: 技術万能主義**
- 「バイアスは技術的手法だけで完全に除去できる」
  - **誤り**: 社会的配慮も必要、完全除去は困難

**❌ 誤答パターン3: 保護属性除去で解決**
- 「性別・人種を入力に使わなければバイアスは発生しない」
  - **誤り**: プロキシ変数（郵便番号等）で間接的に差別可能

**❌ 誤答パターン4: 精度優先**
- 「全体の精度が高ければ公平性は問題ない」
  - **誤り**: 多数派に有利、少数派を犠牲にする可能性

**❌ 誤答パターン5: 単一指標で解決**
- 「一つの公平性指標を満たせば十分」
  - **誤り**: 複数の公平性基準は両立困難、状況に応じた選択必要

**❌ 誤答パターン6: アルゴリズムのみが原因**
- 「バイアスはアルゴリズム自体の欠陥である」
  - **誤り**: 主原因は訓練データの偏り

#### 適切な選択肢（正しい理解）

**✅ 正解パターン1: データの偏りが原因**
- 「訓練データに含まれる社会的偏見や歴史的差別がモデルに反映される」

**✅ 正解パターン2: プロキシ変数の問題**
- 「保護属性を除いても、相関する変数（郵便番号等）で間接差別が起こり得る」

**✅ 正解パターン3: 多面的アプローチ**
- 「技術的対策と組織的・倫理的配慮の両方が必要」

**✅ 正解パターン4: 継続的監視**
- 「導入後も継続的なモニタリングとフィードバックループの監視が重要」

**✅ 正解パターン5: 透明性の重要性**
- 「判断根拠の説明可能性と外部監査が公平性確保に不可欠」

**✅ 正解パターン6: 公平性のトレードオフ**
- 「異なる公平性基準を同時に満たすのは困難で、状況に応じた選択が必要」

### 選択肢判定フローチャート

```
不適切な選択肢を選ぶ問題の場合：

1. 「AIは中立」「差別しない」→ ❌ 不適切（誤解）
2. 「完全に除去できる」→ ❌ 不適切（過度に楽観的）
3. 「保護属性を使わなければOK」→ ❌ 不適切（プロキシ無視）
4. 「精度が高ければ公平」→ ❌ 不適切（多数派優遇無視）
5. 「アルゴリズムの欠陥」→ ❌ 不適切（データの偏り無視）
6. 「技術だけで解決」→ ❌ 不適切（社会的側面無視）

適切な選択肢の場合：
1. 「データの偏りが原因」→ ✅ 適切
2. 「プロキシ変数で間接差別」→ ✅ 適切
3. 「技術と倫理の両面」→ ✅ 適切
4. 「継続的監視が必要」→ ✅ 適切
5. 「透明性・説明可能性」→ ✅ 適切
```

### 比較されやすい概念

| 概念 | 説明 | バイアスとの関係 |
|------|------|----------------|
| **アルゴリズムバイアス** | AIの系統的な偏り | 本テーマ |
| **オーバーフィッティング** | 訓練データへの過適合 | 技術的問題、バイアスとは別 |
| **サンプリングバイアス** | データ収集時の偏り | バイアスの**原因** |
| **確証バイアス** | 人間の認知バイアス | ラベル付け時に影響 |
| **公平性（Fairness）** | 偏りのない扱い | バイアスの**対極概念** |
| **説明可能性（XAI）** | 判断根拠の開示 | バイアス発見の**手段** |
| **プライバシー** | 個人情報保護 | 別の倫理問題、関連あり |

### 引っ掛けポイント（混同注意）

**1. AIは客観的？**
- ✗ 誤解: 「データに基づくから客観的」
- ○ 事実: データ自体が主観的・歴史的偏見を含む

**2. 保護属性を除けば解決？**
- ✗ 誤解: 「性別・人種を使わなければ差別しない」
- ○ 事実: プロキシ変数で間接差別（郵便番号→人種）

**3. 精度と公平性**
- ✗ 誤解: 「精度が高ければ公平」
- ○ 事実: 精度は多数派に有利、公平性は別指標

**4. 単一解決策**
- ✗ 誤解: 「技術的修正だけで解決」
- ○ 事実: 組織・社会・倫理的側面も必要

**5. 完全な公平性**
- ✗ 誤解: 「バイアスゼロは可能」
- ○ 事実: 完全除去は困難、緩和と監視が現実的

### 頻出キーワードと判定

| キーワード | 正誤判定 | 補足 |
|----------|---------|------|
| 「訓練データの偏り」 | ✅ 適切 | 主原因 |
| 「AIは中立的」 | ❌ 不適切 | 神話 |
| 「プロキシ変数」 | ✅ 適切 | 間接差別 |
| 「完全除去可能」 | ❌ 不適切 | 過度に楽観的 |
| 「継続的監視」 | ✅ 適切 | 必須プロセス |
| 「技術のみで解決」 | ❌ 不適切 | 社会的側面無視 |
| 「透明性・説明可能性」 | ✅ 適切 | バイアス発見手段 |
| 「精度優先」 | ❌ 不適切 | 公平性軽視 |

## 補足

### 実務的観点

**1. 開発フェーズ別の対策**:
- **企画**: 影響評価、ステークホルダー参加
- **データ収集**: 多様性確保、サンプリング偏り回避
- **モデル開発**: 公平性指標の監視、複数モデル比較
- **運用**: 継続的監視、フィードバック収集、定期監査

**2. 組織的対応**:
- AI倫理委員会の設置
- ダイバーシティ推進
- 外部監査の受入れ
- インシデント対応体制

**3. 法的リスク**:
- 差別禁止法違反の可能性
- レピュテーションリスク
- 訴訟リスク（集団訴訟の可能性）

**4. 国際的ガイドライン**:
- OECD AI原則（包摂性、公平性）
- EU倫理ガイドライン（多様性、差別排除）
- IEEE Ethically Aligned Design

### 関連トピック
- [AI倫理原則](ai_ethics_principles.md) - 全体的な倫理フレームワーク
- [GDPR](gdpr.md) - プライバシーと差別禁止
- [個人情報保護](personal_data_protection.md) - データ利用の制約
- [AIの限界](../08_ai_society/ai_limitations.md) - 技術的限界の理解
- [説明可能AI](../07_ai_applications/explainable_ai.md) - バイアス発見手段

### 発展トピック
- 因果推論によるバイアス分析
- フェアネス制約付き最適化
- 多目的最適化（精度と公平性の両立）
- アルゴリズム監査の標準化
