# 音声処理（Speech Processing）

## 要点（試験用）
- 音声認識・音声合成・話者認識等を含む時系列データ処理。RNN/LSTM、Transformer、1D-CNNが適切
- 音響特徴量（MFCC、メル周波数等）を抽出し、深層学習で処理するのが現代の主流
- 2D-CNN（画像向け）、探索アルゴリズム（記号的AI）は音声の時系列構造に不適切

## 定義
音声処理は、人間の音声信号をコンピュータで解析・生成・変換する技術分野。主要タスクに音声認識（Speech Recognition）、音声合成（Speech Synthesis / TTS）、話者認識（Speaker Recognition）がある。

---

## 音声データの特性

### 時系列性
- **連続的な時間変化**: 音声は時間軸に沿って連続的に変化する波形
- **前後の依存関係**: 音素・単語の認識には前後の文脈が重要
- **可変長**: 発話速度・文章長により系列長が変動

### 音響的特性
- **周波数成分**: 音声は複数の周波数の重ね合わせ
- **時間-周波数表現**: スペクトログラム（縦軸:周波数、横軸:時間、色:強度）で可視化
- **音響特徴量**: MFCC（メル周波数ケプストラム係数）、メルスペクトログラム等

---

## 主要タスク

### 1. 音声認識（Speech Recognition / ASR）
**目的**: 音声波形→テキスト変換

**処理フロー**:
```
音声波形 → 音響特徴抽出 → 音響モデル → 言語モデル → テキスト出力
         (MFCC等)      (RNN/Transformer) (確率的補正)
```

**主要技術**:
- **音響モデル**: RNN/LSTM/GRU、Transformer（現在の主流）
- **言語モデル**: N-gram、RNN言語モデル、Transformer
- **CTC（Connectionist Temporal Classification）**: 音声とテキストの時間的対応付け
- **Attention機構**: Decoderが音声の重要部分に注目

**代表的モデル**:
- **DeepSpeech**: RNN + CTC（Baidu、2014）
- **Listen, Attend and Spell**: Encoder-Decoder + Attention（2015）
- **Wav2Vec 2.0**: 自己教師あり学習（Facebook/Meta、2020）
- **Whisper**: Transformer、多言語対応（OpenAI、2022）

### 2. 音声合成（Speech Synthesis / TTS: Text-to-Speech）
**目的**: テキスト→音声波形生成

**処理フロー**:
```
テキスト → 音素列変換 → 韻律予測 → 音響特徴生成 → 音声波形生成
         (前処理)     (アクセント等)  (メルスペクトログラム) (Vocoder)
```

**主要技術**:
- **WaveNet**: 1D-CNN（畳み込み）で波形を直接生成（Google、2016）
- **Tacotron**: Encoder-Decoder + Attention（2017）
- **FastSpeech**: 並列生成で高速化（2019）
- **VITS**: End-to-Endの高品質合成（2021）

### 3. 話者認識（Speaker Recognition）
**目的**: 話者の識別・照合

- **話者識別**: 誰が話しているかを特定
- **話者照合**: 本人かどうかを確認（生体認証）

**技術**: 話者埋め込み（Speaker Embedding）、Deep Speaker等

### 4. 音声強調（Speech Enhancement）
**目的**: ノイズ除去、音質向上

**技術**: U-Net、Denoising Autoencoder等

---

## 音声の時系列データを扱う手法

### 適切な手法

| 手法 | 説明 | 適用例 |
|------|------|--------|
| **RNN/LSTM/GRU** | 時系列データ処理の標準手法 | 音声認識、音声合成 |
| **Transformer** | 並列処理可能、現在の主流 | Whisper、Wav2Vec 2.0 |
| **1D-CNN** | 時間方向の畳み込み | WaveNet、音響特徴抽出 |
| **Encoder-Decoder** | 系列変換 | 音声認識、音声合成 |
| **CTC** | 時間的対応付け | 音声認識（alignment不要） |
| **Attention機構** | 重要部分への注目 | 音声認識、音声合成 |

### 不適切な手法（試験頻出）

| 手法 | 判定 | 理由 |
|------|------|------|
| **探索アルゴリズム（BFS/DFS/A*）** | ❌ **最も不適切** | 記号的AI、離散状態空間向け。連続的な音声データには不適 |
| **通常のCNN（2D畳み込み）** | ❌ 不適切 | 2次元画像向け。時系列の時間構造を直接扱わない |
| **単純な決定木** | ❌ 不適切 | 時系列構造を考慮しない。フレーム単位の分類には使えるが系列全体の処理は困難 |
| **単純な全結合NN** | ❌ 不適切 | 固定長入力のみ、時系列構造を無視 |
| **K-means等のクラスタリング** | ❌ 不適切 | 教師なし学習、時系列構造なし（特徴量抽出後の補助的利用は可） |

**注意**: CNNでも**1D-CNN**（時間方向の畳み込み）は音声処理に有効。通常の2D-CNNが不適切。

---

## 音響特徴量

音声波形を直接扱うのは困難なため、特徴量に変換：

### MFCC（Mel-Frequency Cepstral Coefficients）
- **最も一般的な音響特徴量**
- 人間の聴覚特性（メル尺度）を模倣
- 次元: 通常13次元（+ デルタ・デルタデルタで39次元）
- 音声認識の前処理として標準的

### メルスペクトログラム
- 周波数軸をメル尺度に変換したスペクトログラム
- 深層学習では直接入力として使用されることが多い

### その他
- **フィルタバンク特徴量**: メルフィルタの出力
- **ピッチ**: 基本周波数（話者特性）
- **音声パワー**: 発話区間検出に利用

---

## 重要キーワード
- **音声認識（ASR: Automatic Speech Recognition）**: 音声→テキスト変換
- **音声合成（TTS: Text-to-Speech）**: テキスト→音声生成
- **話者認識（Speaker Recognition）**: 話者の識別・照合
- **MFCC（Mel-Frequency Cepstral Coefficients）**: 最も一般的な音響特徴量
- **スペクトログラム**: 時間-周波数表現（横軸:時間、縦軸:周波数）
- **CTC（Connectionist Temporal Classification）**: 音声とテキストの時間的対応付け
- **Encoder-Decoder**: 系列変換アーキテクチャ
- **WaveNet**: 1D-CNNによる音声波形生成
- **Transformer**: 現在の音声認識の主流（Whisper等）
- **1D-CNN**: 時間方向の畳み込み、音声処理に有効
- **Attention機構**: 重要部分への注目

---

## 実例

### 音声認識の実用例
- **スマートスピーカー**: Amazon Alexa、Google Assistant、Siri
- **文字起こし**: 会議録、字幕生成
- **音声検索**: YouTubeの音声検索
- **リアルタイム翻訳**: 多言語コミュニケーション

### 音声合成の実用例
- **ナビゲーション**: カーナビ、地図アプリの音声案内
- **スクリーンリーダー**: 視覚障害者支援
- **音声アシスタント**: AIアシスタントの応答
- **コンテンツ制作**: オーディオブック、ニュース読み上げ

### 性能の進化
- **2010年代初頭**: 音声認識の単語誤り率（WER）20-30%
- **2017年（深層学習の普及）**: WER 5-10%
- **2020年代（Transformer）**: WER 3-5%（人間レベルに接近）

---

## 試験での問われ方

### 典型的な出題パターン
**問：音声の時系列データを扱う手法として、最も不適切な選択肢を1つ選べ**

✅ **適切な選択肢**：
- **「RNN/LSTM/GRUで時系列パターンを学習する」** → ✅ 適切
- **「Transformerで音声認識を行う」** → ✅ 適切（現在の主流）
- **「1D-CNNで音声波形を処理する」** → ✅ 適切（WaveNet等）
- 「CTCで音声とテキストの対応付けを行う」→ ✅ 適切
- 「Encoder-Decoderで音声認識を行う」→ ✅ 適切
- 「MFCCを抽出して深層学習で処理する」→ ✅ 適切

❌ **不適切な選択肢（試験で狙われる）**：
- **「探索アルゴリズム（BFS/DFS）で音声を処理する」** → ❌ **最も不適切**（記号的AI、連続データに不適）
- **「通常のCNN（2D畳み込み）で音声波形を直接処理する」** → ❌ 不適切（2次元画像向け）
- **「決定木で音声認識を行う」** → ❌ 不適切（時系列構造を考慮しない）
- **「単純な全結合NNで可変長音声を処理する」** → ❌ 不適切（固定長入力のみ）
- **「A*探索で音声認識を行う」** → ❌ 不適切（探索手法は不適）

### 引っ掛けポイント
❌ 「CNNは音声処理に不適」→ 2D-CNNは不適だが、1D-CNNは適切（WaveNet等）  
❌ 「RNNは遅いので音声処理に不向き」→ Transformerが主流だがRNNも有効  
❌ 「探索アルゴリズムで音素系列を探索」→ 記号的な音素は扱えるが、連続的な音響信号には不適  
❌ 「決定木でフレーム分類すれば音声認識可能」→ フレーム単位は可能だが系列全体の時間構造を扱えない  
✅ 「RNN/LSTM/Transformerが適切」→ **正解**  
✅ 「1D-CNNは時間方向の畳み込みで有効」→ **正解**  
✅ 「探索アルゴリズムは音声の時系列処理に不適切」→ **正解**

### 比較されやすい概念
- **RNN vs Transformer**: 逐次処理 vs 並列処理、現在はTransformer主流
- **1D-CNN vs 2D-CNN**: 時間方向の畳み込み vs 画像の2次元畳み込み
- **音声認識 vs 画像認識**: 時系列データ vs 空間データ
- **適切な手法（RNN/Transformer） vs 不適切な手法（探索アルゴリズム）**: 時系列処理 vs 記号的AI
- **連続的データ vs 離散的データ**: 音声波形 vs 記号・状態空間

---

## 補足

### 実務での技術選択
- **リアルタイム性重視**: 軽量なRNN、ストリーミング処理
- **精度重視**: Transformer（Whisper等）
- **オフライン処理**: 大規模モデル可能
- **エッジデバイス**: モデル圧縮、量子化

### 音声処理の課題
- **雑音環境**: ノイズ除去、音声強調が必要
- **話者適応**: 多様な話者への対応
- **方言・訛り**: データ多様性の確保
- **リアルタイム性**: 低遅延処理の実現

### 技術の進化
- **従来（〜2010年）**: HMM（隠れマルコフモデル）+ GMM（混合ガウスモデル）
- **深層学習初期（2012〜）**: DNN + HMM、RNN/LSTM
- **Attention時代（2017〜）**: Encoder-Decoder + Attention
- **Transformer時代（2020〜）**: Whisper、Wav2Vec 2.0等

### 自己教師あり学習の台頭
- **Wav2Vec 2.0**: ラベルなし音声データから学習
- **HuBERT**: マスクされた音声予測
- **効果**: 大量の未ラベルデータを活用、少量のラベルデータで高精度

### End-to-Endモデル
従来は音響モデル+言語モデルの2段階だったが、最近はEnd-to-End（入力音声→出力テキストを直接学習）が主流。
