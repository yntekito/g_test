# 音声処理（Speech Processing）

## 要点
- 音声認識・音声合成・話者認識等を含む時系列データ処理。RNN/LSTM、Transformer、1D-CNNが適切
- 音響特徴量（MFCC、メル周波数等）を抽出し、深層学習で処理するのが現代の主流
- 2D-CNN（画像向け）、探索アルゴリズム（記号的AI）は音声の時系列構造に不適切

## 定義
音声処理は、人間の音声信号をコンピュータで解析・生成・変換する技術分野。主要タスクに音声認識（Speech Recognition）、音声合成（Speech Synthesis / TTS）、話者認識（Speaker Recognition）がある。

---

## 音声データの特性

### アナログ音声からデジタル音声への変換

#### AD変換（アナログ-デジタル変換）★試験頻出
**定義**：音声をコンピュータで扱うために、連続的なアナログ信号を離散的なデジタルデータに変換する手法を**AD変換（A/D変換、アナログ-デジタル変換）**という。

**2つの主要プロセス**：

**1. 標本化（サンプリング: Sampling）**
- **定義**: 連続的な時間信号を一定間隔で標本（サンプル）を取る処理
- **サンプリング周波数**: 1秒間に取得するサンプル数（Hz）
- **標準値**:
  - **CD音質**: 44.1 kHz（44,100回/秒）
  - **電話**: 8 kHz
  - **音声認識**: 16 kHz
- **ナイキスト定理**: 元の信号の最高周波数の**2倍以上**のサンプリング周波数が必要
  - 例：20 kHzまでの音を再現 → 40 kHz以上でサンプリング必要

```
アナログ音声（連続波形）
 ～～～～～～～～
    ↓ 標本化（時間軸を離散化）
 ●  ●  ●  ●  ●  ← サンプル点
```

**2. 量子化（Quantization）**
- **定義**: 振幅（音の大きさ）を離散的な数値に変換する処理
- **量子化ビット数**: 振幅を何段階で表現するか
- **標準値**:
  - **16bit**: 65,536段階（CD音質）
  - **8bit**: 256段階（電話、低品質）
  - **24bit**: 16,777,216段階（プロ用、高品質）
- **量子化誤差**: 連続値を離散値に丸めることで生じる誤差（ノイズ）

```
振幅（連続値）
  ～～～
    ↓ 量子化（振幅を離散化）
  レベル3 ──
  レベル2 ──
  レベル1 ──
```

**AD変換の全体プロセス**：
```
マイク → アナログ音声波形
         ↓
      【標本化】
      時間軸を離散化（例: 44.1kHz）
         ↓
      【量子化】
      振幅を離散化（例: 16bit）
         ↓
      デジタル音声データ（01010110...）
         ↓
      コンピュータで処理可能
```

**試験での問われ方**：
> 「音声をコンピュータで扱うために、離散的なデジタルデータに変換する手法を（　　）という。」

✅ **正解**: 
- **AD変換**（最も一般的）
- **A/D変換**
- **アナログ-デジタル変換**
- **標本化と量子化**（2つのプロセスを含む）

**混同しやすい用語**：
- **標本化のみ**: 時間軸の離散化のみ（振幅はまだアナログ）
- **量子化のみ**: 振幅の離散化のみ（時間はまだ連続）
- **AD変換**: 標本化と量子化の両方を含む全体プロセス

---

### 時系列性
- **連続的な時間変化**: 音声は時間軸に沿って連続的に変化する波形
- **前後の依存関係**: 音素・単語の認識には前後の文脈が重要
- **可変長**: 発話速度・文章長により系列長が変動

### 音響的特性
- **周波数成分**: 音声は複数の周波数の重ね合わせ
- **時間-周波数表現**: スペクトログラム（縦軸:周波数、横軸:時間、色:強度）で可視化
- **音響特徴量**: MFCC（メル周波数ケプストラム係数）、メルスペクトログラム等

---

## 主要タスク

### 1. 音声認識（Speech Recognition / ASR）
**目的**: 音声波形→テキスト変換

**処理フロー**:
```
音声波形 → 音響特徴抽出 → 音響モデル → 言語モデル → テキスト出力
         (MFCC等)      (RNN/Transformer) (確率的補正)
```

**主要技術**:
- **音響モデル**: RNN/LSTM/GRU、Transformer（現在の主流）
- **言語モデル**: N-gram、RNN言語モデル、Transformer
- **CTC（Connectionist Temporal Classification）**: 音声とテキストの時間的対応付け
- **Attention機構**: Decoderが音声の重要部分に注目

**代表的モデル**:
- **DeepSpeech**: RNN + CTC（Baidu、2014）
- **Listen, Attend and Spell**: Encoder-Decoder + Attention（2015）
- **Wav2Vec 2.0**: 自己教師あり学習（Facebook/Meta、2020）
- **Whisper**: Transformer、多言語対応（OpenAI、2022）

### 2. 音声合成（Speech Synthesis / TTS: Text-to-Speech）
**目的**: テキスト→音声波形生成

**処理フロー**:
```
テキスト → 音素列変換 → 韻律予測 → 音響特徴生成 → 音声波形生成
         (前処理)     (アクセント等)  (メルスペクトログラム) (Vocoder)
```

**主要技術**:
- **WaveNet**: 1D-CNN（畳み込み）で波形を直接生成（Google、2016）
- **Tacotron**: Encoder-Decoder + Attention（2017）
- **FastSpeech**: 並列生成で高速化（2019）
- **VITS**: End-to-Endの高品質合成（2021）

### 3. 話者認識（Speaker Recognition）
**目的**: 話者の識別・照合

- **話者識別**: 誰が話しているかを特定
- **話者照合**: 本人かどうかを確認（生体認証）

**技術**: 話者埋め込み（Speaker Embedding）、Deep Speaker等

### 4. 音声強調（Speech Enhancement）
**目的**: ノイズ除去、音質向上

**技術**: U-Net、Denoising Autoencoder等

---

## 音声の時系列データを扱う手法

### 適切な手法

| 手法 | 説明 | 適用例 |
|------|------|--------|
| **RNN/LSTM/GRU** | 時系列データ処理の標準手法 | 音声認識、音声合成 |
| **Transformer** | 並列処理可能、現在の主流 | Whisper、Wav2Vec 2.0 |
| **1D-CNN** | 時間方向の畳み込み | WaveNet、音響特徴抽出 |
| **Encoder-Decoder** | 系列変換 | 音声認識、音声合成 |
| **CTC** | 時間的対応付け | 音声認識（alignment不要） |
| **Attention機構** | 重要部分への注目 | 音声認識、音声合成 |

### 不適切な手法（試験頻出）

| 手法 | 判定 | 理由 |
|------|------|------|
| **探索アルゴリズム（BFS/DFS/A*）** | ❌ **最も不適切** | 記号的AI、離散状態空間向け。連続的な音声データには不適 |
| **通常のCNN（2D畳み込み）** | ❌ 不適切 | 2次元画像向け。時系列の時間構造を直接扱わない |
| **単純な決定木** | ❌ 不適切 | 時系列構造を考慮しない。フレーム単位の分類には使えるが系列全体の処理は困難 |
| **単純な全結合NN** | ❌ 不適切 | 固定長入力のみ、時系列構造を無視 |
| **K-means等のクラスタリング** | ❌ 不適切 | 教師なし学習、時系列構造なし（特徴量抽出後の補助的利用は可） |

**注意**: CNNでも**1D-CNN**（時間方向の畳み込み）は音声処理に有効。通常の2D-CNNが不適切。

---

## 音響特徴量

音声波形を直接扱うのは困難なため、特徴量に変換：

### スペクトル包絡とケプストラム分析（重要）

#### スペクトル包絡
- **定義**: 音声スペクトルの大まかな形状（包絡線）
- **意味**: 音色の特徴を表現（声道の共振特性を反映）
- **重要性**: 個人差、母音の違い、音色の違いを特徴づける

**音色の解釈**：
> 「音色の違いはスペクトル包絡の違いとして解釈される」

#### ケプストラム分析（Cepstrum Analysis）
- **定義**: スペクトル包絡を求める代表的な手法
- **処理**: スペクトルの対数を取り、逆フーリエ変換を適用
- **効果**: 音源情報（ピッチ）と声道情報（音色）を分離

**ケプストラムの計算手順**：
```
音声波形
  ↓ フーリエ変換（FFT）
パワースペクトル
  ↓ 対数変換（log）
対数スペクトル
  ↓ 逆フーリエ変換（IFFT）
ケプストラム ← スペクトル包絡を表現
```

**なぜケプストラムか**：
- 音源（声帯振動、ピッチ）: 高速な周期成分 → ケプストラムの高次成分
- 声道（音色、スペクトル包絡）: 緩やかな変化 → **ケプストラムの低次成分**
- 低次成分のみ取り出すことで**スペクトル包絡を抽出**

#### 線形予測符号化（LPC: Linear Predictive Coding）
- **別のアプローチ**: 声道の共振特性を直接モデル化
- **原理**: 過去のサンプルから未来のサンプルを線形予測
- **用途**: 音声符号化、音声合成

**ケプストラム vs LPC**：
| 項目 | ケプストラム | LPC |
|------|------------|-----|
| **原理** | スペクトルの対数+逆変換 | 線形予測モデル |
| **出力** | ケプストラム係数 | LPC係数 |
| **用途** | 音声認識（MFCC） | 音声符号化、合成 |
| **特徴** | 非線形処理（対数） | 線形モデル |

### MFCC（Mel-Frequency Cepstral Coefficients）
- **最も一般的な音響特徴量**
- **ケプストラム分析の応用**: メル尺度（人間の聴覚特性）を組み込んだケプストラム
- 次元: 通常13次元（+ デルタ・デルタデルタで39次元）
- 音声認識の前処理として標準的

**MFCCの計算手順**：
```
音声波形
  ↓ フーリエ変換（FFT）
パワースペクトル
  ↓ メルフィルタバンク適用
メルスペクトル
  ↓ 対数変換（log）
対数メルスペクトル
  ↓ 離散コサイン変換（DCT）
MFCC ← スペクトル包絡（メル尺度）
```

**MFCCとケプストラムの関係**：
- MFCC = メル尺度のケプストラム
- 人間の聴覚特性（低周波数に敏感、高周波数に鈍感）を反映
- ケプストラム分析の発展形

### メルスペクトログラム
- 周波数軸をメル尺度に変換したスペクトログラム
- 深層学習では直接入力として使用されることが多い

### その他
- **フィルタバンク特徴量**: メルフィルタの出力
- **ピッチ**: 基本周波数（話者特性）
- **音声パワー**: 発話区間検出に利用

---

## 重要キーワード
- **音声認識（ASR: Automatic Speech Recognition）**: 音声→テキスト変換
- **音声合成（TTS: Text-to-Speech）**: テキスト→音声生成
- **話者認識（Speaker Recognition）**: 話者の識別・照合
- **MFCC（Mel-Frequency Cepstral Coefficients）**: 最も一般的な音響特徴量
- **スペクトログラム**: 時間-周波数表現（横軸:時間、縦軸:周波数）
- **CTC（Connectionist Temporal Classification）**: 音声とテキストの時間的対応付け
- **Encoder-Decoder**: 系列変換アーキテクチャ
- **WaveNet**: 1D-CNNによる音声波形生成
- **Transformer**: 現在の音声認識の主流（Whisper等）
- **1D-CNN**: 時間方向の畳み込み、音声処理に有効
- **Attention機構**: 重要部分への注目

---

## 実例

### 音声認識の実用例
- **スマートスピーカー**: Amazon Alexa、Google Assistant、Siri
- **文字起こし**: 会議録、字幕生成
- **音声検索**: YouTubeの音声検索
- **リアルタイム翻訳**: 多言語コミュニケーション

### 音声合成の実用例
- **ナビゲーション**: カーナビ、地図アプリの音声案内
- **スクリーンリーダー**: 視覚障害者支援
- **音声アシスタント**: AIアシスタントの応答
- **コンテンツ制作**: オーディオブック、ニュース読み上げ

### 性能の進化
- **2010年代初頭**: 音声認識の単語誤り率（WER）20-30%
- **2017年（深層学習の普及）**: WER 5-10%
- **2020年代（Transformer）**: WER 3-5%（人間レベルに接近）

---

## 試験での問われ方

### 典型的な出題パターン

#### パターン1：穴埋め問題（スペクトル包絡）
> 「音色の違いはスペクトル包絡の違いとして解釈される。このスペクトル包絡を求める方法を（　　）と呼ぶ。」

✅ **正解**：
- **「ケプストラム分析」** ← **最も一般的な解答**
- 「ケプストラム解析」（同義）
- 「線形予測符号化（LPC）」← 別のアプローチ

**解説**：
- **ケプストラム分析**：スペクトルの対数を取り逆フーリエ変換を適用し、音源（ピッチ）と声道（音色）を分離
- ケプストラムの低次成分がスペクトル包絡を表現
- **MFCC**はケプストラム分析の応用（メル尺度を組み込んだケプストラム）

**別解の説明**：
- **LPC（線形予測符号化）**：声道の共振特性を直接モデル化する別手法
- 過去のサンプルから未来を線形予測
- 音声符号化や合成で使われる

**注意点**：
- ❌ 「フーリエ変換」→ スペクトルを求める手法だが、包絡は求めない
- ❌ 「高速フーリエ変換（FFT）」→ 同上、包絡抽出ではない
- ❌ 「メルフィルタバンク」→ 周波数変換の手法、包絡抽出ではない

#### パターン2：選択肢問題
**問：音声の時系列データを扱う手法として、最も不適切な選択肢を1つ選べ**

✅ **適切な選択肢**：
- **「RNN/LSTM/GRUで時系列パターンを学習する」** → ✅ 適切
- **「Transformerで音声認識を行う」** → ✅ 適切（現在の主流）
- **「1D-CNNで音声波形を処理する」** → ✅ 適切（WaveNet等）
- 「CTCで音声とテキストの対応付けを行う」→ ✅ 適切
- 「Encoder-Decoderで音声認識を行う」→ ✅ 適切
- 「MFCCを抽出して深層学習で処理する」→ ✅ 適切

❌ **不適切な選択肢（試験で狙われる）**：
- **「探索アルゴリズム（BFS/DFS）で音声を処理する」** → ❌ **最も不適切**（記号的AI、連続データに不適）
- **「通常のCNN（2D畳み込み）で音声波形を直接処理する」** → ❌ 不適切（2次元画像向け）
- **「決定木で音声認識を行う」** → ❌ 不適切（時系列構造を考慮しない）
- **「単純な全結合NNで可変長音声を処理する」** → ❌ 不適切（固定長入力のみ）
- **「A*探索で音声認識を行う」** → ❌ 不適切（探索手法は不適）

### 引っ掛けポイント
❌ 「CNNは音声処理に不適」→ 2D-CNNは不適だが、1D-CNNは適切（WaveNet等）  
❌ 「RNNは遅いので音声処理に不向き」→ Transformerが主流だがRNNも有効  
❌ 「探索アルゴリズムで音素系列を探索」→ 記号的な音素は扱えるが、連続的な音響信号には不適  
❌ 「決定木でフレーム分類すれば音声認識可能」→ フレーム単位は可能だが系列全体の時間構造を扱えない  
✅ 「RNN/LSTM/Transformerが適切」→ **正解**  
✅ 「1D-CNNは時間方向の畳み込みで有効」→ **正解**  
✅ 「探索アルゴリズムは音声の時系列処理に不適切」→ **正解**

### 比較されやすい概念
- **RNN vs Transformer**: 逐次処理 vs 並列処理、現在はTransformer主流
- **1D-CNN vs 2D-CNN**: 時間方向の畳み込み vs 画像の2次元畳み込み
- **音声認識 vs 画像認識**: 時系列データ vs 空間データ
- **適切な手法（RNN/Transformer） vs 不適切な手法（探索アルゴリズム）**: 時系列処理 vs 記号的AI
- **連続的データ vs 離散的データ**: 音声波形 vs 記号・状態空間

---

## 補足

### 実務での技術選択
- **リアルタイム性重視**: 軽量なRNN、ストリーミング処理
- **精度重視**: Transformer（Whisper等）
- **オフライン処理**: 大規模モデル可能
- **エッジデバイス**: モデル圧縮、量子化

### 音声処理の課題
- **雑音環境**: ノイズ除去、音声強調が必要
- **話者適応**: 多様な話者への対応
- **方言・訛り**: データ多様性の確保
- **リアルタイム性**: 低遅延処理の実現

### 技術の進化
- **従来（〜2010年）**: HMM（隠れマルコフモデル）+ GMM（混合ガウスモデル）
- **深層学習初期（2012〜）**: DNN + HMM、RNN/LSTM
- **Attention時代（2017〜）**: Encoder-Decoder + Attention
- **Transformer時代（2020〜）**: Whisper、Wav2Vec 2.0等

### 自己教師あり学習の台頭
- **Wav2Vec 2.0**: ラベルなし音声データから学習
- **HuBERT**: マスクされた音声予測
- **効果**: 大量の未ラベルデータを活用、少量のラベルデータで高精度

### End-to-Endモデル
従来は音響モデル+言語モデルの2段階だったが、最近はEnd-to-End（入力音声→出力テキストを直接学習）が主流。
