# 画像認識

## 要点
- 画像から物体・パターンを識別するAI技術。CNNの発展で精度が飛躍的に向上し、ImageNet競技で人間超え達成。
- 主要タスク：画像分類、物体検出（ワンステージ/ツーステージ）、セグメンテーション、顔認識等。
- 深層学習以前は特徴量設計（SIFT、HOG）が主流、現在はEnd-to-Endで特徴抽出から分類まで自動学習。

## 定義
画像認識とは、入力画像からパターン・物体・シーンを識別・分類するコンピュータビジョン技術。深層学習（特にCNN）により、従来の手作業特徴量設計を不要にし、大規模データから自動で特徴を学習する。

## 重要キーワード
- **画像分類**: 画像全体に1つのクラスラベルを割り当てる（例：猫/犬）
- **物体検出**: 画像内の複数物体の位置（バウンディングボックス）とクラスを特定
- **セグメンテーション**: ピクセル単位で領域を分割（セマンティック/インスタンス）
- **CNN（畳み込みニューラルネットワーク）**: 画像認識の基盤技術
- **転移学習**: 事前学習済みモデル（ImageNet）を新タスクにファインチューニング
- **ImageNet**: 大規模画像データセット（1400万枚）、ILSVRCコンペで技術進化を牽引
- **データ拡張**: 回転・反転・切り抜き等で学習データを増やし汎化性能向上

---

## セマンティックセグメンテーション（Semantic Segmentation）

### 要点
セマンティックセグメンテーションは、画像の各ピクセルにクラスラベルを割り当てる手法。物体検出が矩形領域で識別するのに対し、ピクセル単位で領域を正確に分割。自動運転の道路認識、医療画像の臓器抽出等に利用。FCN、U-Net、SegNet、DeepLabが代表的。

### 定義
セマンティックセグメンテーションとは、入力画像の各ピクセルに対してクラスラベル（道路、人、空、建物等）を割り当てるコンピュータビジョン技術。画像を意味のある領域に分割し、各領域が何であるかを識別する。

### 重要キーワード
- **ピクセル単位分類**: 各画素に対してクラスを予測
- **セマンティック vs インスタンス**: セマンティックは同じクラスを区別しない、インスタンスは個別物体を区別
- **エンコーダ-デコーダ**: ダウンサンプリング→アップサンプリングの構造
- **スキップ結合**: エンコーダの特徴をデコーダに直接伝播
- **FCN（Fully Convolutional Network）**: 最初の完全畳み込みベースセグメンテーション
- **U-Net**: 医療画像で広く使用、スキップ結合が特徴
- **SegNet**: プーリングインデックス記憶でメモリ効率化
- **DeepLab**: Atrous Convolution（拡張畳み込み）使用
- **IoU（Intersection over Union）**: セグメンテーションの評価指標

### 詳細

#### 背景
従来の画像分類は画像全体に1つのラベル、物体検出は矩形領域で識別。より詳細な領域理解には、ピクセル単位での分類が必要。深層学習（特にFCN以降）により高精度なセグメンテーションが実現。

#### 主要手法の比較

**FCN（Fully Convolutional Network）- 2015年**:
- **特徴**: 全結合層を畳み込み層に置き換え、任意サイズの画像を処理
- **アップサンプリング**: 転置畳み込み（Deconvolution）
- **スキップ結合**: 浅い層の特徴を深い層に結合し、細部を保持
- **意義**: セマンティックセグメンテーションの基礎を確立

**U-Net - 2015年**:
- **構造**: U字型のエンコーダ-デコーダ
- **特徴**: スキップ結合で対応する層を直接接続
- **利点**: 少ないデータで高精度、医療画像に最適
- **用途**: 細胞・臓器のセグメンテーション

**SegNet - 2015年**:
- **特徴**: プーリングインデックスを記憶・再利用
- **利点**: メモリ効率が高い
- **用途**: 自動運転、リアルタイム処理

**DeepLab - 2017年（V3）**:
- **特徴**: Atrous Convolution（拡張畳み込み）で受容野を拡大
- **ASPP**: Atrous Spatial Pyramid Pooling
- **利点**: 高精度、マルチスケール対応
- **用途**: 一般的な高精度セグメンテーション

#### 図解（タスク比較）
```
画像分類:
入力画像 → "犬"（画像全体に1ラベル）

物体検出:
入力画像 → [矩形1: 犬], [矩形2: 猫]（バウンディングボックス）

セマンティックセグメンテーション:
入力画像 → ピクセルごとに色分け
           [空:青, 道路:灰, 車:赤, 人:緑, 建物:黄]

インスタンスセグメンテーション:
入力画像 → ピクセルごと + 個別識別
           [車1:赤, 車2:橙, 人1:緑, 人2:黄緑]
```

#### 基本プロセス（エンコーダ-デコーダ型）
1. **エンコーダ**: CNNで特徴抽出とダウンサンプリング
2. **ボトルネック**: 最も圧縮された特徴表現
3. **デコーダ**: アップサンプリングで元解像度に復元
4. **分類**: 各ピクセルでSoftmaxによりクラス予測
5. **出力**: 入力と同サイズのクラスマップ

### 試験での問われ方

#### 典型設問
- **「画素単位でクラス識別」→セマンティックセグメンテーション**
- **「ピクセルごとにクラスラベルを割り当てる」→セマンティックセグメンテーション**
- FCN、U-Net、SegNet、DeepLabの特徴と違い
- セマンティック vs インスタンスセグメンテーション
- エンコーダ-デコーダ構造の利点

#### ひっかけポイントと違いの整理

**タスク別比較**:

| タスク | 出力単位 | 個別識別 | 代表手法 | 用途例 |
|--------|----------|----------|----------|--------|
| **画像分類** | 画像全体 | - | ResNet, VGG | 画像カテゴリ判定 |
| **物体検出** | 矩形領域 | 可能 | SSD, YOLO, R-CNN | 物体位置特定 |
| **セマンティック** | ピクセル | 不可 | FCN, U-Net, SegNet | 領域分割 |
| **インスタンス** | ピクセル | 可能 | Mask R-CNN | 個別物体抽出 |

**手法別比較**:

| 手法 | 主な特徴 | メモリ | 精度 | 用途 |
|------|----------|--------|------|------|
| **FCN** | 転置畳み込み | 中 | 中 | 基礎手法 |
| **U-Net** | スキップ結合 | 多 | 高 | 医療画像 |
| **SegNet** | インデックス記憶 | 少 | 中 | リアルタイム |
| **DeepLab** | Atrous Conv | 中 | 非常に高 | 高精度要求 |

**混同注意**:
- **セマンティック vs インスタンス**: セマンティックは「道路」「車」をクラス分類、インスタンスは「車1」「車2」を個別識別
- **物体検出 vs セグメンテーション**: 検出は矩形、セグメンテーションは正確な輪郭
- **画像分類 vs セグメンテーション**: 分類は画像全体、セグメンテーションはピクセル単位

**出題パターン**:
- 「画素単位でクラス識別」→**セマンティックセグメンテーション**
- 「スキップ結合で特徴マップ結合」→**U-Net**
- 「プーリングインデックス記憶」→**SegNet**
- 「全結合層を畳み込み層に置き換え」→**FCN**
- 「個別の物体をピクセル単位で区別」→**インスタンスセグメンテーション（Mask R-CNN）**

#### G検定頻出の穴埋め問題パターン

**問題**: 畳み込みニューラルネットワーク（CNN）をセマンティックセグメンテーションタスクに利用した手法であり、畳み込み層のみで構成されるモデルは（　）である。

**答え**: **FCN（Fully Convolutional Network）**

**理由**:
1. **「畳み込み層のみ」がキーワード**: FCNは全結合層（Fully Connected Layer）を畳み込み層に置き換えた最初のセグメンテーション手法
2. **「セマンティックセグメンテーション」**: ピクセル単位のクラス分類タスク
3. **他手法との違い**:
   - 従来のCNN（AlexNet、VGG）: 畳み込み層 + 全結合層（画像分類用）
   - FCN: すべて畳み込み層（任意サイズの画像を処理可能）

**FCNの特徴（試験頻出）**:
- ✅ **全結合層を持たない**: 1×1畳み込みで置き換え
- ✅ **任意サイズの入力**: 全結合層がないため画像サイズ制約なし
- ✅ **転置畳み込み（Deconvolution）**: ダウンサンプリングした特徴マップを元サイズに復元
- ✅ **スキップ結合**: 浅い層の特徴を深い層に結合（細部保持）
- ✅ **ピクセル単位の予測**: 出力は入力と同じサイズのクラスマップ

**混同しやすい選択肢**:
| 選択肢 | 特徴 | 誤答理由 |
|--------|------|----------|
| **U-Net** | U字型構造、スキップ結合強化 | 畳み込み層のみだが、FCNの発展形。問題文が「最初の手法」を問う場合は誤答 |
| **SegNet** | プーリングインデックス記憶 | 畳み込み層のみだが、FCNより後発 |
| **DeepLab** | Atrous Convolution使用 | 畳み込み層のみだが、拡張畳み込みという特殊構造が特徴 |
| **Mask R-CNN** | R-CNNベース | インスタンスセグメンテーション、セマンティックではない |
| **ResNet** | 残差結合 | 画像分類用、セグメンテーションではない |
| **VGG** | 畳み込み+全結合層 | 全結合層を持つため不適切 |

**判定フロー（穴埋め問題）**:
```
問題文を読む
    ↓
「セマンティックセグメンテーション」とある？
    ↓ YES → セグメンテーション手法に絞る（FCN、U-Net、SegNet、DeepLab）
    ↓
「畳み込み層のみ」とある？
    ↓ YES → 全結合層を持たない手法（FCN系）
    ↓
「最初の」「基礎となった」等の記述がある？
    ↓ YES → FCN（2015年、Long et al.）が正解
    ↓ NO → U-Net、SegNet、DeepLabも候補（文脈で判断）
```

**別の出題パターン**:
- 「全結合層を畳み込み層に置き換えた最初のセグメンテーション手法」→ **FCN**
- 「任意サイズの画像を処理できるセグメンテーション手法」→ **FCN**（全結合層がないため）
- 「転置畳み込みでアップサンプリングを行う」→ **FCN**
- 「スキップ結合により細部を保持する」→ **FCN、U-Net**（両方とも使用）

### 補足

#### 実務観点
**用途**: 
- 自動運転: 道路・歩行者・車両の領域分割
- 医療: 臓器・腫瘍の抽出
- 衛星画像: 土地利用分類
- ARアプリ: 背景除去

**評価指標**: 
- **IoU（Intersection over Union）**: 予測と正解の重なり度 = (交差面積) / (和集合面積)
- **mIoU（mean IoU）**: 全クラスのIoU平均
- **Pixel Accuracy**: ピクセル単位の正解率

**課題**: 
- クラス不均衡（背景が大部分を占める）
- 境界の正確な予測が困難
- 計算コストが高い
- アノテーションコスト（ピクセル単位のラベル付け）

#### 最新動向
- **Transformer系**: SegFormer、Segmenter等
- **リアルタイム化**: BiSeNet、ICNet等
- **弱教師あり**: 粗いアノテーションから学習
- **3Dセグメンテーション**: 点群データへの拡張

#### 関連トピック
- [CNN](../06_deep_learning/cnn.md) - ベースとなる畳み込みネットワーク
- [SegNet](07_ai_applications/image_recognition.md) - 具体的手法の詳細
- [評価指標](../05_machine_learning/evaluation_metrics.md) - IoU、Pixel Accuracy

---

## SSD（Single Shot MultiBox Detector）

### 要点
SSDはYOLOと並ぶワンステージ物体検出器で、領域候補生成と分類を1回の順伝播で同時実行。R-CNNのようなツーステージ検出器と異なり、推論速度が速くリアルタイム処理に適する。複数スケールの特徴マップから異なるサイズの物体を検出する点が特徴。

### 定義
SSD（Single Shot MultiBox Detector）は、畳み込みニューラルネットワーク（CNN）の複数層から得られる特徴マップ上で、デフォルトボックス（アンカー）を基準に物体の位置（バウンディングボックス）とクラスを同時に予測するワンステージ物体検出アルゴリズム。

### 重要キーワード
- **ワンステージ検出器**: 領域候補生成と分類を1つのネットワークで一度に実行
- **ツーステージ検出器**: R-CNN系のように、領域候補→分類の2段階処理
- **デフォルトボックス（アンカー）**: 事前定義された候補矩形
- **マルチスケール特徴マップ**: 異なる解像度の層から検出を行う
- **バウンディングボックス**: 物体を囲む矩形領域
- **YOLO**: 同様のワンステージ検出器
- **Non-Maximum Suppression (NMS)**: 重複検出の除去処理

### 詳細

#### 背景
従来のR-CNN系（Faster R-CNN等）はRegion Proposal Network（RPN）で候補領域を生成し、その後分類する**ツーステージ検出器**で精度は高いが処理が遅い。SSDは2016年に提案され、YOLOと同様に**ワンステージ**で処理することで高速化を実現。

#### 基本プロセス
1. **特徴抽出**: VGG16等のベースネットワークで画像から特徴マップを抽出
2. **マルチスケール検出**: 複数の畳み込み層（異なる解像度）でデフォルトボックスを配置
3. **予測**: 各デフォルトボックスに対し、クラススコアと位置オフセットを出力
4. **Non-Maximum Suppression (NMS)**: 重複検出を除去して最終結果を得る

#### 図解（アーキテクチャ）
```
入力画像 (300×300)
  ↓
VGG-16ベース (特徴抽出)
  ↓
Conv4_3 → 検出ヘッド (大きい物体)
  ↓
Conv7   → 検出ヘッド (中程度の物体)
  ↓
Conv8_2 → 検出ヘッド (小さい物体)
  ↓
... (さらに小さいスケール)
  ↓
各層で: クラススコア + バウンディングボックスオフセット
  ↓
NMS → 最終検出結果
```

#### 主な特徴
- **速度**: 1回の順伝播で完結（59 FPS @SSD300）
- **精度**: マルチスケール特徴により様々なサイズの物体に対応
- **デフォルトボックス**: アスペクト比・スケールを変えた複数の候補を事前定義

### 試験での問われ方

#### 典型設問
- SSDの検出方式（ワンステージ/ツーステージ）
- R-CNN/Faster R-CNN/YOLOとの比較（速度・精度・処理段階）
- マルチスケール特徴マップの役割
- **穴埋め問題**: 「SSDは位置の特定とクラスの識別を同時に行う（ワンステージ検出器）であり、出力層から入力画像の位置における物体らしさと矩形領域を出力する手法であり、（リアルタイム物体検出）を目的としている。」

#### ひっかけポイントと違いの整理

| 項目 | SSD | R-CNN/Faster R-CNN | YOLO |
|------|-----|-------------------|------|
| **検出方式** | ワンステージ | ツーステージ | ワンステージ |
| **処理段階** | 同時予測（位置+クラス） | 候補生成→分類 | 同時予測 |
| **速度** | 高速（リアルタイム可） | 遅い | 非常に高速 |
| **精度** | 中〜高 | 高精度 | 中程度 |
| **特徴** | マルチスケール特徴マップ | RPN使用 | グリッド分割 |
| **小物体検出** | 比較的得意 | 得意 | 苦手 |

**混同注意**:
- **R-CNNとの違い**: SSD��領域候補生成（RPN）を行わず、デフォルトボックスで直接予測
- **YOLOとの違い**: SSDは複数層から検出、YOLOは最終層のみ（初期版）
- **セグメンテーションとの違い**: SSDは矩形検出、Mask R-CNNはピクセル単位のマスク生成

**出題パターン**:
- 「ワンステージ/ツーステージ」の用語選択
- 「リアルタイム処理に適した手法」→SSD/YOLO
- 「高精度が必要な場合」→Faster R-CNN

### 補足

#### 実務観点
- **用途**: 自動運転（歩行者・車両検出）、監視カメラ、ロボットビジョン
- **トレードオフ**: 速度と精度のバランスが重要。用途に応じてSSD/YOLO（速度重視）かFaster R-CNN（精度重視）を選択
- **実装**: TensorFlow、PyTorchで実装済みモデルが利用可能
- **改良版**: SSD512（入力サイズ大）、DSSD（デコンボリューション追加）等

#### 関連トピック
- [CNN](../06_deep_learning/cnn.md) - ベースとなる畳み込みネットワーク
- [評価指標](../05_machine_learning/evaluation_metrics.md) - mAP（mean Average Precision）、IoU

---

## R-CNN（Region-based Convolutional Neural Network）

### 要点
- **R-CNN**: 物体検出の先駆的手法（2014年）、Selective Searchで領域候補を抽出→各候補をCNNで分類する**ツーステージ検出器**。
- **特徴**: 画像分類で学習したCNNを物体検出に応用、精度は高いが処理が遅い（領域ごとにCNN実行）。
- **進化**: Fast R-CNN（2015年、RoI Pooling導入）→ Faster R-CNN（2015年、RPN導入で高速化）→ Mask R-CNN（2017年、セグメンテーション対応）

### 定義
**R-CNN（Region-based Convolutional Neural Network）**は、画像内の物体を検出するために、まず**Selective Search**で約2000個の領域候補（Region Proposals）を生成し、各候補領域をCNNに入力して特徴抽出、SVMで分類するツーステージ物体検出手法。Girshickら（2014）により提案。

### 重要キーワード
- **R-CNN**: 領域候補ベースのCNN物体検出（2014年）
- **Fast R-CNN**: RoI Poolingで高速化（2015年）
- **Faster R-CNN**: RPN導入でEnd-to-End学習（2015年）
- **Mask R-CNN**: インスタンスセグメンテーション対応（2017年）
- **ツーステージ検出器**: 領域候補生成→分類の2段階処理
- **Selective Search**: 領域候補を生成する古典的手法（R-CNNで使用）
- **RPN（Region Proposal Network）**: 学習可能な領域候補生成（Faster R-CNNで導入）
- **RoI Pooling（Region of Interest Pooling）**: 可変サイズの領域を固定サイズに変換

### 詳細

#### 背景：物体検出へのCNNの応用

**2012年以前**:
- HOG（Histogram of Oriented Gradients）+ SVM等の手作業特徴量
- DPM（Deformable Part Model）が主流
- 精度に限界

**2012年：ImageNet革命**:
- AlexNetが画像分類で圧勝
- CNNの特徴抽出能力が実証される

**2014年：R-CNN登場**:
- CNNを物体検出に応用
- 領域候補ベースのアプローチで精度向上

#### R-CNN系の進化

```
R-CNN (2014)
    ↓ RoI Pooling導入、CNN共有化
Fast R-CNN (2015)
    ↓ RPN導入、End-to-End学習
Faster R-CNN (2015)
    ↓ セグメンテーション追加
Mask R-CNN (2017)
```

### R-CNN（オリジナル版、2014年）

#### 処理フロー

```
入力画像
    ↓
1. Selective Search（約2000個の領域候補を抽出）
    ↓
2. 各候補領域をリサイズ（227×227）
    ↓
3. CNN（AlexNet）で特徴抽出【各領域ごと】
    ↓
4. SVM分類器でクラス判定【各領域ごと】
    ↓
5. Bounding Box Regression（位置調整）
    ↓
出力：物体の位置とクラス
```

#### 特徴と課題

**特徴**:
- ✅ CNNの高い特徴抽出能力を物体検出に適用
- ✅ 従来手法より大幅に精度向上（Pascal VOCで約50%mAP）
- ✅ 画像分類の事前学習を活用（転移学習）

**課題（G検定頻出）**:
- ❌ **処理が非常に遅い**: 各領域候補（約2000個）ごとにCNN実行
- ❌ **訓練が複雑**: CNN、SVM、回帰器を個別に訓練
- ❌ **メモリ消費大**: 大量の領域候補を保存
- ❌ **End-to-Endでない**: Selective Searchは学習不可

### Fast R-CNN（2015年）

#### 改善点

**主な変更**:
1. **RoI Pooling導入**: 画像全体に1回だけCNN適用、領域ごとの特徴はPoolingで抽出
2. **統合学習**: 分類と位置調整を同時に学習（Multi-task Loss）
3. **SVMの廃止**: Softmaxで直接分類

**処理フロー**:
```
入力画像
    ↓
1. CNN（VGG16等）で特徴マップ抽出【1回だけ】
    ↓
2. Selective Searchで領域候補（CPUで実行）
    ↓
3. RoI Pooling（特徴マップから各領域の特徴を抽出）
    ↓
4. 全結合層で分類＋位置調整
    ↓
出力
```

**効果**:
- 訓練時間：R-CNNの約9倍高速
- 推論時間：R-CNNの約25倍高速
- 精度：若干向上

**残存課題**:
- Selective SearchがCPUで遅い
- End-to-End学習ではない

### Faster R-CNN（2015年）

#### 革新的改善：RPN導入

**RPN（Region Proposal Network）**:
- 学習可能な領域候補生成ネットワーク
- CNNの特徴マップから直接候補を生成
- Selective Searchを完全に置き換え

**処理フロー**:
```
入力画像
    ↓
1. CNN（VGG16/ResNet等）で特徴マップ抽出
    ↓
2. RPN（特徴マップから領域候補を生成）
    ↓
3. RoI Pooling
    ↓
4. 分類＋位置調整
    ↓
出力
```

**特徴**:
- ✅ **完全なEnd-to-End学習**: 全てのコンポーネントが微分可能
- ✅ **高速化**: Selective Searchより10倍以上高速
- ✅ **精度向上**: 領域候補の質が向上
- ✅ **GPU活用**: 全処理をGPUで実行可能

**性能**:
- Pascal VOCで約70-75%mAP
- 推論速度：約5-10 FPS（GPU使用時）
- 現在でも高精度が必要な場面で使用

### Mask R-CNN（2017年）

#### セグメンテーションへの拡張

**追加機能**:
- Faster R-CNNに**マスク生成ブランチ**を追加
- 各物体のピクセル単位のマスクを予測
- インスタンスセグメンテーションを実現

**構造**:
```
入力画像
    ↓
CNN + RPN（Faster R-CNNと同じ）
    ↓
RoI Align（RoI Poolingの改良版）
    ↓
    ├→ 分類ヘッド
    ├→ Bounding Boxヘッド
    └→ マスクヘッド【新規追加】
    ↓
出力：クラス + 矩形 + マスク
```

**特徴**:
- ✅ **インスタンスセグメンテーション**: 個別物体をピクセル単位で区別
- ✅ **マルチタスク学習**: 分類、検出、セグメンテーションを同時実行
- ✅ **RoI Align**: ピクセル位置を正確に保持（RoI Poolingより精度向上）

### 試験での問われ方

#### 典型的な選択肢問題：R-CNNの説明として最も適切なもの

**✅ 適切な選択肢（正解パターン）**:

1. **「Selective Searchで領域候補を生成し、各候補をCNNで特徴抽出してSVMで分類する」**
   - ○ R-CNNの正確な説明

2. **「ツーステージ検出器であり、領域候補生成と分類を2段階で行う」**
   - ○ R-CNN系の基本的な特徴

3. **「画像分類で学習したCNNを物体検出に応用した先駆的手法」**
   - ○ R-CNNの歴史的意義

4. **「各領域候補ごとにCNNを実行するため処理が遅い」**
   - ○ R-CNNの主な課題

5. **「Faster R-CNNではRPNを導入し、Selective Searchを置き換えた」**
   - ○ 進化の過程

**❌ 不適切な選択肢（誤答パターン）**:

1. **「ワンステージ検出器であり、領域候補生成と分類を同時に行う」**
   - ✗ R-CNNは**ツーステージ検出器**（ワンステージはYOLO、SSD）

2. **「画像をグリッド分割し、各グリッドで物体を検出する」**
   - ✗ これはYOLOの説明

3. **「リアルタイム処理が可能で、自動運転に最適」**
   - ✗ R-CNNは遅い（リアルタイムはYOLO、SSD）

4. **「デフォルトボックス（アンカー）を用いて複数スケールで検出」**
   - ✗ これはSSDの説明

5. **「RPN（Region Proposal Network）を用いて領域候補を生成」**
   - ✗ これは**Faster R-CNN**の特徴（R-CNNはSelective Search）

6. **「ピクセル単位でセグメンテーションマスクを生成する」**
   - ✗ これは**Mask R-CNN**の特徴

7. **「セマンティックセグメンテーション用の手法」**
   - ✗ R-CNNは**物体検出**、Mask R-CNNは**インスタンスセグメンテーション**

8. **「畳み込み層のみで構成され、全結合層を持たない」**
   - ✗ これはFCNの説明、R-CNNは全結合層あり

#### 判定フロー

```
選択肢を読む
    ↓
「Selective Search」「領域候補約2000個」とある？
    ↓ YES → R-CNN（オリジナル）の可能性大
    ↓
「ツーステージ」「2段階」とある？
    ↓ YES → R-CNN系（正解の可能性）
    ↓
「ワンステージ」「同時実行」とある？
    ↓ YES → ❌ R-CNNではない（YOLO/SSD）
    ↓
「RPN」とある？
    ↓ YES → Faster R-CNN（R-CNNではない）
    ↓
「マスク」「インスタンスセグメンテーション」とある？
    ↓ YES → Mask R-CNN（R-CNNではない）
    ↓
「処理が遅い」「各領域ごとにCNN」とある？
    ↓ YES → R-CNN（オリジナル）の課題（正解）
```

#### 手法別比較表（G検定頻出）

| 手法 | 年 | 領域候補生成 | 特徴 | 速度 | 用途 |
|------|---|------------|------|------|------|
| **R-CNN** | 2014 | Selective Search | 各候補でCNN実行 | 非常に遅い | 先駆的研究 |
| **Fast R-CNN** | 2015 | Selective Search | RoI Pooling導入 | やや遅い | - |
| **Faster R-CNN** | 2015 | **RPN** | End-to-End学習 | 中速 | **高精度検出** |
| **Mask R-CNN** | 2017 | RPN | マスク生成追加 | 中速 | **インスタンスセグ** |
| **YOLO** | 2016 | なし | グリッド分割 | 非常に高速 | リアルタイム |
| **SSD** | 2016 | なし | デフォルトボックス | 高速 | リアルタイム |

**見極めポイント**:
- **R-CNN**: Selective Search + 遅い
- **Faster R-CNN**: RPN + 高精度
- **Mask R-CNN**: RPN + マスク
- **YOLO/SSD**: ワンステージ + 高速

#### 混同しやすい概念の整理

| 項目 | R-CNN系 | YOLO/SSD | FCN系 |
|------|---------|----------|-------|
| **タスク** | 物体検出 | 物体検出 | セグメンテーション |
| **出力** | 矩形+クラス | 矩形+クラス | ピクセルラベル |
| **処理段階** | ツーステージ | ワンステージ | - |
| **速度** | 遅〜中 | 高速 | 中 |
| **精度** | 高 | 中 | 高 |
| **代表例** | Faster R-CNN | YOLO v3 | FCN, U-Net |

### 補足：実務での活用

#### 用途別推奨

**高精度が必要**:
- Faster R-CNN、Mask R-CNN
- 例：医療画像診断、品質検査

**リアルタイム処理**:
- YOLO、SSD
- 例：自動運転、監視カメラ

**インスタンス識別**:
- Mask R-CNN
- 例：ロボットピッキング、AR

#### 最新動向（2024-2026年）

**Transformer系検出器**:
- DETR（Detection Transformer）: アンカー不要、End-to-End
- Swin Transformer: 階層的構造でR-CNN系を置き換え

**効率化**:
- YOLOv8、YOLOv9: 精度と速度の両立
- EfficientDet: Compound Scalingで効率化

### 関連トピック
- [CNN](../06_deep_learning/cnn.md) - ベースとなる畳み込みネットワーク
- [SSD](image_recognition.md) - ワンステージ検出器との比較
- [セマンティックセグメンテーション](image_recognition.md) - 本ファイル上部
- [評価指標](../05_machine_learning/evaluation_metrics.md) - mAP、IoU

---

## SegNet（Segmentation Network）

### 要点
SegNetはセマンティックセグメンテーション向けエンコーダ-デコーダ構造のネットワーク。エンコーダのプーリング時に**最大値の位置（インデックス）を記憶**し、デコーダのアンプーリングで再利用する点が特徴。メモリ効率が高く、境界保存に優れる。

### 定義
SegNetは、VGG16ベースのエンコーダで特徴抽出とダウンサンプリングを行い、対称的なデコーダでアップサンプリングしてピクセル単位のクラス予測を行うセマンティックセグメンテーションモデル。最大プーリング時のインデックスを記憶・再利用することで、空間情報を効率的に保持する。

### 重要キーワード
- **セマンティックセグメンテーション**: ピクセル単位で各領域のクラスを予測
- **エンコーダ-デコーダ構造**: 圧縮→復元の対称的なネットワーク
- **最大プーリングインデックス**: プーリングで選ばれた最大値の位置情報
- **アンプーリング（Unpooling）**: プーリングの逆操作、保存したインデックスで復元
- **U-Net**: スキップ結合を使う類似アーキテクチャ
- **FCN（Fully Convolutional Network）**: 全結合層を畳み込み層に置き換えたセグメンテーション手法

### 詳細

#### 背景
セマンティックセグメンテーションでは、空間解像度を保ちながら特徴抽出を行う必要がある。SegNetは2015年に提案され、プーリングインデックスの記憶により、メモリ効率と境界精度を両立。

#### アーキテクチャ

**エンコーダ**:
1. **特徴抽出**: VGG16の畳み込み層を使用
2. **最大プーリング**: ダウンサンプリングと同時に最大値の位置インデックスを記憶
3. **段階的圧縮**: 解像度を1/2ずつ削減

**デコーダ**:
1. **アンプーリング**: 記憶したインデックスを使い、元の位置に値を配置
2. **畳み込み**: アンプーリング後に畳み込みで特徴を復元
3. **段階的復元**: エンコーダと対称的に解像度を2倍ずつ拡大

#### 図解（アーキテクチャ）
```
エンコーダ                          デコーダ
入力画像                            出力（クラスマップ）
  ↓                                    ↑
Conv+ReLU ──────インデックス記憶──→ Upconv+ReLU
  ↓                                    ↑
MaxPool (1/2) ──インデックス保存──→ Unpooling
  ↓                                    ↑
Conv+ReLU ──────インデックス記憶──→ Upconv+ReLU
  ↓                                    ↑
MaxPool (1/4) ──インデックス保存──→ Unpooling
  ↓                                    ↑
   ... (繰り返し)                     ...
  ↓                                    ↑
特徴ベクトル ─────────────────→ 復元開始
```

#### プーリングインデックスの仕組み
```
元の特徴マップ (4×4)     MaxPooling (2×2)
[1  3  2  8]             [3  8]  + インデックス: [(0,1), (0,3)]
[0  2  4  1]             [5  9]                   [(1,0), (1,3)]
[5  1  0  2]
[2  0  9  1]

Unpooling時に元の位置に配置:
[0  3  0  8]
[5  0  0  1]  ← 空白は0埋め or 後続の畳み込みで補完
[0  0  0  0]
[0  0  9  0]
```

#### 主な特徴
- **メモリ効率**: 特徴マップ全体ではなくインデックスのみ保存（U-Netより軽量）
- **境界保存**: プーリング位置を正確に復元し、物体境界を精密に再現
- **対称性**: エンコーダとデコーダが完全対称

### 試験での問われ方

#### 典型設問
- **「最大プーリングの位置を記憶するセグメンテーション手法」→SegNet**
- **「エンコーダとデコーダが対になっている」→SegNet、U-Net**
- **「セマンティックセグメンテーションに利用される」→SegNet、U-Net、FCN、DeepLab**
- エンコーダ-デコーダ構造の利点
- U-Net、FCNとの違い

#### ひっかけポイントと違いの整理

| 項目 | SegNet | U-Net | FCN | DeepLab |
|------|--------|-------|-----|---------|
| **構造** | エンコーダ-デコーダ | エンコーダ-デコーダ | 全畳み込み | 拡張畳み込み |
| **特徴** | プーリングインデックス記憶 | スキップ結合 | 転置畳み込み | Atrous Convolution |
| **メモリ** | 効率的 | やや多い | 中程度 | 中程度 |
| **境界精度** | 高 | 非常に高 | 中 | 高 |
| **用途** | 自動運転、屋外シーン | 医療画像 | 一般的 | 一般的・高精度 |

**混同注意**:
- **SegNet vs U-Net**: SegNetはインデックス記憶、U-Netは特徴マップ全体をスキップ結合
- **SegNet vs FCN**: SegNetはインデックス利用、FCNは転置畳み込みでアップサンプリング
- **セマンティック vs インスタンスセグメンテーション**: セマンティックは同じクラスを区別しない、インスタンスは個別に識別（Mask R-CNN等）

**出題パターン**:
- 「エンコーダで最大プーリングした位置を記憶」→**SegNet**
- 「スキップ結合で特徴マップを結合」→**U-Net**
- 「全結合層を畳み込み層に置き換え」→**FCN**
- 「医療画像セグメンテーションで広く使われる」→**U-Net**

### 補足

#### 実務観点
- **用途**: 自動運転（道路・歩行者領域分割）、衛星画像解析、ロボットビジョン
- **実装**: Keras、PyTorchで実装可能。VGG16の事前学習重みが利用可能
- **後継**: PSPNet、DeepLabV3等でさらに高精度化
- **課題**: 小物体の検出、クラス不均衡への対応

#### U-Netとの使い分け
- **SegNet**: メモリ制約がある場合、リアルタイム処理
- **U-Net**: 高精度が必要な場合（医療画像）、学習データが少ない場合

#### 関連トピック
- [CNN](../06_deep_learning/cnn.md) - ベースとなる畳み込みネットワーク
- [評価指標](../05_machine_learning/evaluation_metrics.md) - IoU（Intersection over Union）

---

## 試験での問われ方（画像認識全般）
- **AlexNet/VGGNet/ResNet**: 代表的CNNアーキテクチャの特徴と年代
- **物体検出手法の分類**: ワンステージ（YOLO、SSD）vsツーステージ（R-CNN系）
- **転移学習**: ImageNet事前学習モデルの利用メリット
- **データ拡張**: 過学習対策としての有効性

### 引っ掛けポイント
- **分類vs検出vsセグメンテーション**: タスクの違いを明確に（分類は画像全体、検出は矩形、セグメンテーションはピクセル単位）
- **精度vs速度**: リアルタイム要求ならYOLO/SSD、高精度ならFaster R-CNN
- **転移学習の前提**: ドメインが近いほど効果大（医療画像→自然画像は難）

## 補足（画像認識全般）
- **実務課題**: 学習データの質・量、クラス不均衡、未知物体への対応、計算コスト
- **最新動向**: Vision Transformer（ViT）、DETR（Transformerベース物体検出）、自己教師あり学習
- **評価指標**: 分類は精度・F1、検出はmAP（mean Average Precision）、セグメンテーションはIoU（Intersection over Union）

---

## 画像オープンデータセット

### 要点
代表的な画像オープンデータセットには、ImageNet、MNIST、CIFAR-10/100、COCO、Pascal VOC、Open Images等がある。試験では、オープンデータの定義（誰でもアクセス可能、再利用・再配布の自由）を理解し、**個人情報を含むデータ、著作権保護された画像、商用利用制限のあるデータ**が不適切な選択肢として出題される。

### 定義
画像オープンデータセットとは、誰でも自由にアクセス・利用・再配布が可能な画像データの集合。深層学習の学習・評価に広く利用され、技術的・法的障壁なく研究・教育・商用目的で活用できる（ライセンス条項による制限あり）。

### 重要キーワード
- **ImageNet**: 1400万枚、2万カテゴリ、ILSVRCで使用、深層学習発展の起点
- **MNIST**: 手書き数字7万枚（訓練6万、テスト1万）、28×28ピクセル、初学者向け定番
- **CIFAR-10/100**: 6万枚の32×32カラー画像、10または100クラス、小規模で学習が早い
- **COCO (Common Objects in Context)**: 33万枚、80カテゴリ、物体検出・セグメンテーション用
- **Pascal VOC**: 2万枚、20クラス、物体検出の標準ベンチマーク
- **Open Images**: 900万枚、6000カテゴリ、Googleが提供、CC-BY
- **オープンデータの定義**: アクセス・再利用・再配布の自由、技術的・法的障壁なし

### 詳細

#### 主要データセットの特徴

**ImageNet**:
- **規模**: 1400万枚以上、2万以上のカテゴリ（WordNetの階層構造に基づく）
- **用途**: 画像分類、転移学習の事前学習
- **歴史**: ILSVRC（ImageNet Large Scale Visual Recognition Challenge, 2010-2017）で使用
- **意義**: AlexNet（2012）がエラー率を大幅低下させ、深層学習ブームの火付け役
- **ライセンス**: 研究・教育用途で広く利用可能、商用利用には一部制限あり
- **構成**: 訓練データ約120万枚、検証データ5万枚、テストデータ10万枚（1000クラス版）

**MNIST**:
- **規模**: 70,000枚（訓練60,000、テスト10,000）
- **内容**: 手書き数字0-9
- **形式**: 28×28ピクセル、グレースケール
- **特徴**: 前処理済みで扱いやすい、機械学習の"Hello World"
- **ライセンス**: パブリックドメイン
- **用途**: アルゴリズムの動作確認、教育

**CIFAR-10/100**:
- **CIFAR-10**: 60,000枚（訓練50,000、テスト10,000）、10クラス（飛行機、車、鳥、猫等）
- **CIFAR-100**: 60,000枚、100クラス（20の大カテゴリに分類）
- **形式**: 32×32ピクセル、カラー（RGB）
- **特徴**: 低解像度で学習が早い、研究用途に適する
- **ライセンス**: MIT License

**COCO (Common Objects in Context)**:
- **規模**: 330,000枚以上（訓練118,000、検証5,000、テスト41,000）
- **カテゴリ**: 80の物体カテゴリ
- **アノテーション**: バウンディングボックス、セグメンテーションマスク、キャプション
- **用途**: 物体検出、インスタンスセグメンテーション、画像キャプション生成
- **特徴**: 複雑なシーン、複数物体、遮蔽を含む
- **ライセンス**: CC-BY 4.0
- **評価指標**: mAP（mean Average Precision）を使用

**Pascal VOC**:
- **規模**: 約20,000枚（VOC2012）
- **カテゴリ**: 20クラス（人、乗り物、動物、家具等）
- **アノテーション**: バウンディングボックス、セグメンテーションマスク
- **用途**: 物体検出、セグメンテーションの標準ベンチマーク
- **歴史**: 2005-2012年のコンペティションで使用
- **特徴**: COCOより小規模だが、精密なアノテーション

**Open Images**:
- **規模**: 900万枚以上
- **カテゴリ**: 6000以上のクラス
- **提供元**: Google
- **アノテーション**: 画像レベルラベル、バウンディングボックス、セグメンテーション、視覚的関係
- **ライセンス**: CC-BY（クリエイティブ・コモンズ 表示）
- **特徴**: 非常に大規模、多様なカテゴリ

#### 不適切な選択肢の例

**❌ 個人情報を含むデータ**:
- **例**: 企業の顧客画像データベース、監視カメラの生映像、患者の医療画像（匿名化前）
- **問題点**: 個人情報保護法・GDPR違反、プライバシー侵害
- **対策**: 匿名化・同意取得・目的外利用の禁止

**❌ 著作権保護された画像**:
- **例**: プロカメラマンの作品集、映画のスチール写真、商用ストックフォト（Getty Images等）
- **問題点**: 著作権者の許諾なしに使用不可、著作権法違反
- **誤解**: インターネット上の画像≠自由に使える画像

**❌ 商用利用制限のあるデータセット**:
- **例**: 学術研究のみ許可、非営利用途限定のデータセット
- **問題点**: 真のオープンデータの定義（再利用・再配布の自由）に反する
- **注意**: ライセンス条項（CC-BY-NC等）を必ず確認

**❌ 技術的障壁のあるデータ**:
- **例**: 有料API経由のみアクセス可能、特定ソフトウェア必須
- **問題点**: オープンデータの「技術的障壁なし」に反する

#### オープンデータの定義

**3つの要件**:
1. **アクセスの自由**: 誰でも取得・ダウンロード可能
2. **再利用の自由**: 加工・分析・派生作品の作成が可能
3. **再配布の自由**: 他者への共有・公開が可能

**オープンライセンスの例**:
- **CC0（パブリックドメイン）**: すべての権利放棄、制約なし
- **CC-BY**: 帰属表示（クレジット記載）が必要
- **CC-BY-SA**: 帰属表示+継承（同じライセンスで再配布）
- **CC-BY-NC**: 帰属表示+非営利のみ（商用利用不可）
- **MIT License**: 著作権表示があれば自由に利用可能

### 試験での問われ方

#### 典型設問
- **「画像のオープンデータセットとして最も不適切なものは？」**
  - 選択肢: ImageNet、MNIST、COCO、**企業の顧客画像データベース**
  - 正解: 企業の顧客画像データベース（個人情報保護・オープン性の欠如）

- **「深層学習の画像分類で広く使われる大規模データセットは？」**
  - 正解: ImageNet

- **「手書き数字認識の定番データセットは？」**
  - 正解: MNIST

- **「物体検出のアノテーションを含むデータセットは？」**
  - 正解: COCO、Pascal VOC

#### ひっかけポイントと違いの整理

**オープンデータセット vs 不適切なデータ**:

| 項目 | オープンデータセット | 不適切なデータ |
|------|---------------------|---------------|
| **アクセス** | 誰でも自由 | 制限あり（有料・許可必要） |
| **個人情報** | 匿名化済み・なし | 個人識別可能 |
| **著作権** | オープンライセンス | 保護されている |
| **商用利用** | 可能（ライセンス次第） | 制限あり・不可 |
| **例** | ImageNet、MNIST、COCO | 企業顧客DB、著作物、監視映像 |

**データセット別比較**:

| データセット | 規模 | 用途 | ライセンス | 商用利用 | 特徴 |
|------------|------|------|----------|---------|------|
| **ImageNet** | 1400万枚 | 画像分類・転移学習 | 研究用途 | 一部制限 | 深層学習の基盤 |
| **MNIST** | 7万枚 | 手書き数字 | パブリックドメイン | 可能 | 初学者向け |
| **CIFAR-10/100** | 6万枚 | 小画像分類 | MIT License | 可能 | 研究・教育 |
| **COCO** | 33万枚 | 物体検出・セグメンテーション | CC-BY | 可能（表示義務） | 複雑なシーン |
| **Pascal VOC** | 2万枚 | 物体検出 | 研究用途 | 一部制限 | 標準ベンチマーク |
| **Open Images** | 900万枚 | 多目的 | CC-BY | 可能（表示義務） | 大規模・多様 |
| **企業顧客データ** | - | - | ❌クローズド | 不可 | 個人情報・非公開 |
| **著作権画像** | - | - | ❌保護 | 不可 | 許諾必要 |

**混同注意**:
- **ImageNetは完全オープンではない**: 商用利用には制限がある場合があるが、研究用途では広く利用可能なため「オープンデータセット」として扱われる
- **医療画像の扱い**: 匿名化・同意取得済みのデータセット（NIH Chest X-rays、MIMIC-CXR等）は公開されているが、**生の患者データは不適切**
- **ライセンスの違い**: CC0（完全パブリックドメイン）、CC-BY（表示義務）、CC-BY-NC（非営利のみ）等の違いを理解

**出題パターン**:
- 「個人情報を含むデータ」→❌不適切（プライバシー侵害）
- 「著作権保護された画像集」→❌不適切（著作権法違反）
- 「学術研究のみ許可」→❌真のオープンではない（ただし、試験では「オープン」として扱われることもあるので文脈を確認）
- 「ImageNet、MNIST、CIFAR」→✅適切（代表的オープンデータセット）

### 補足

#### 実務での注意点
- **ライセンス確認**: 商用利用可否、帰属表示の要否、派生作品の制約を必ず確認
- **個人情報保護**: 顔画像等は匿名化・同意取得が必須、GDPR・個人情報保護法を遵守
- **データセットバイアス**: 人種、性別、年齢等の偏りに注意（公平性の問題）
- **用途適合性**: 規模、品質、ドメイン（医療、自然画像等）が目的に適しているか確認
- **データ拡張**: 既存データセットから回転・反転・ノイズ付加等で学習データを増やす

#### 匿名化された医療画像データセットの例
- **NIH Chest X-rays**: 11万枚の胸部X線画像（匿名化済み）
- **MIMIC-CXR**: 37万枚の胸部X線画像と放射線レポート
- **BraTS**: 脳腫瘍MRI画像（Brain Tumor Segmentation Challenge）
- **これらは適切に匿名化・倫理審査済みのためオープンデータセットとして利用可能**

#### データセット選定のポイント
- **規模**: 小規模（MNIST、CIFAR）→動作確認、大規模（ImageNet、COCO）→高精度モデル
- **ドメイン**: 自然画像（ImageNet）、医療（NIH）、衛星画像（SpaceNet）等
- **タスク**: 分類（ImageNet）、検出（COCO）、セグメンテーション（Pascal VOC）
- **前処理**: MNISTは前処理済み、ImageNetは生画像

#### 関連トピック
- [個人情報保護](../09_law_ethics/personal_data_protection.md) - データ利用の法的側面
- [GDPR](../09_law_ethics/gdpr.md) - 欧州のデータ保護規則
- [バイアスと公平性](../09_law_ethics/bias_and_fairness.md) - データセットのバイアス問題
- [CNN](../06_deep_learning/cnn.md) - 画像認識の基盤技術
- [転移学習](../06_deep_learning/neural_network_basics.md) - 事前学習済みモデルの活用
