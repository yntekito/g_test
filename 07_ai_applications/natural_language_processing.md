# 自然言語処理（Natural Language Processing, NLP）

## 要点（試験用）
- **形態素解析**：文を形態素（意味の最小単位）に分割し品詞を判別。日本語処理の基礎。
- **構文解析**：文の文法構造（主語・述語等）を解析。形態素解析の次段階。
- **意味解析**：文の意味内容を理解。最も高次な処理。Transformer（BERT/GPT）が主流。
- **機械翻訳の歴史**：～1970年代＝ルールベース、1990年代～＝統計的、2015年代～＝ニューラル（Transformer）。

## 定義
**自然言語処理（NLP）**は、人間が日常使う言語（自然言語）をコンピュータで処理・理解・生成する技術。音声認識・機械翻訳・感情分析・質問応答など幅広い応用がある。

基本的な処理階層：
1. **形態素解析**：単語分割＋品詞判別
2. **構文解析**：文法構造の解析
3. **意味解析**：意味内容の理解
4. **文脈理解**：複数文の関係把握

## 重要キーワード
- **形態素（Morpheme）**：意味を持つ最小単位。「走る」「ない」など。
- **形態素解析（Morphological Analysis）**：文を形態素に分割し、各形態素の品詞・活用形等を判別する処理。日本語では必須（単語間に空白なし）。
- **トークン化（Tokenization）**：文章を単語や部分文字列に分割する処理。形態素解析より単純（品詞判別なし）。
- **構文解析（Syntactic Analysis, Parsing）**：文の文法構造を解析し、主語・述語・修飾関係等を特定。構文木で表現。
- **係り受け解析（Dependency Parsing）**：単語間の依存関係（どの単語がどの単語を修飾するか）を解析。
- **意味解析（Semantic Analysis）**：文の意味内容を理解し、概念・関係を抽出。
- **固有表現抽出（Named Entity Recognition, NER）**：人名・地名・組織名等を識別。
- **品詞（Part-of-Speech, POS）**：名詞・動詞・形容詞・助詞など文法的カテゴリ。
- **形態素解析器**：MeCab、Janome、Sudachi等のツール。辞書と統計モデルで分割・品詞判別。
- **分かち書き**：形態素解析の分割結果を空白区切りで表記（日本語の前処理）。
- **Word2Vec**：単語を分散表現（ベクトル）に変換する手法。Skip-gram/CBOW。
- **BERT（Bidirectional Encoder Representations from Transformers）**：双方向Transformerで文脈を考慮した単語埋め込み。
- **GPT（Generative Pre-trained Transformer）**：自己回帰型Transformerで文章生成。
- **Transformer**：Self-Attentionで長距離依存を学習。現代NLPの主流アーキテクチャ。
- **Attention機構**：入力の重要部分に注目する仕組み。機械翻訳で導入。
- **ルールベース機械翻訳**：文法規則と辞書を人手で作成（～1970年代後半）。
- **統計的機械翻訳**：対訳データから統計的に学習（1990年代～2010年代）。
- **ニューラル機械翻訳（NMT）**：深層学習による翻訳、Transformer使用（2015年代～）。
- **Seq2Seq（Sequence-to-Sequence）**：Encoder-Decoderで系列変換、機械翻訳の基礎。

## 詳細

### 形態素解析（Morphological Analysis）
**最も基本的な処理**で、文章を形態素に分割し、各形態素の品詞・活用形を判別する。

#### プロセス例（日本語）
```
入力: "私は東京に行きました"

【形態素解析の出力】
私    ：名詞（代名詞）
は    ：助詞（係助詞）
東京  ：名詞（固有名詞・地名）
に    ：助詞（格助詞）
行き  ：動詞（五段活用・連用形）
まし  ：助動詞（丁寧・過去）
た    ：助動詞（過去）

【分かち書き】
私 は 東京 に 行き まし た
```

#### 形態素解析の技術
1. **辞書ベース**：既知単語を辞書から検索（MeCab等）
2. **統計モデル**：出現確率・品詞遷移確率で最適分割を選択（HMM、CRF）
3. **深層学習**：RNN/Transformerで文脈考慮（BERT等）

#### 日本語での重要性
- 英語等は単語間に空白→トークン化が容易
- 日本語は単語の区切りがない→形態素解析が必須前処理

### トークン化との違い
| 処理 | 分割 | 品詞判別 | 用途 |
|------|------|----------|------|
| **形態素解析** | ✅ | ✅ | 日本語等、品詞情報が必要な場合 |
| **トークン化** | ✅ | ❌ | 英語等、単純な前処理 |

```
例: "I love cats"

【トークン化】
["I", "love", "cats"]

【形態素解析（品詞付き）】
I/代名詞, love/動詞, cats/名詞
```

### 構文解析（Syntactic Analysis）
形態素解析の**次段階**で、文の文法構造を解析する。

#### 構文木の例
```
入力: "私は猫が好きです"

       好き（述語）
      /    \
    私（主語）  猫（目的語）
```

- 主語・述語・目的語の関係を特定
- 修飾関係（形容詞→名詞など）を解析
- 文の階層構造を木構造で表現

### 係り受け解析（Dependency Parsing）
単語間の依存関係（どの単語がどの単語を修飾するか）を解析。

```
入力: "美しい花が咲いた"

美しい → 花（修飾）
花 → 咲いた（主語）
```

### 意味解析（Semantic Analysis）
文の**意味内容**を理解する最も高次な処理。

- **固有表現抽出（NER）**：人名・地名・日付等を識別
- **関係抽出**：エンティティ間の関係を特定（「東京は日本の首都」→ 首都(東京, 日本)）
- **感情分析**：ポジティブ/ネガティブ判定
- **質問応答**：質問文に対する回答生成

### NLPの代表的タスク

| タスク | 説明 | 代表手法 |
|--------|------|----------|
| **機械翻訳** | 言語間の自動翻訳 | Transformer, Seq2Seq |
| **文書分類** | テキストをカテゴリ分類 | CNN, BERT |
| **固有表現抽出** | 人名・地名等の識別 | CRF, BERT |
| **感情分析** | 感情極性の判定 | RNN, BERT |
| **質問応答** | 質問に対する回答生成 | BERT, GPT |
| **要約** | 文書の要約生成 | Seq2Seq, T5 |
| **対話システム** | 会話応答の生成 | GPT, Transformer |

### 機械翻訳の歴史（試験頻出）

機械翻訳は自然言語処理の主要応用の一つで、その発展は**3つの時代**に分けられる。

#### 1. ルールベース機械翻訳（～1970年代後半）
**第1世代：人手で文法規則と辞書を作成**

**特徴**：
- 言語学者が文法規則を記述
- 単語辞書を人手で構築
- 構文解析→変換→生成の3段階

**方式**：
- **直接翻訳**：単語対応のみ
- **トランスファー方式**：原言語→中間表現→目標言語
- **インターリンガ方式**：言語非依存な意味表現を介する

**利点**：
- 文法規則が明確なら高精度
- ドメイン限定なら実用的

**欠点**：
- 規則作成に膨大なコスト
- 例外処理が困難
- スケーラビリティが低い

**代表システム**：
- SYSTRAN（1968年～）
- Eurotra（EU）

#### 2. 統計的機械翻訳（1990年代～2010年代前半）
**第2世代：大量の対訳データから統計的に学習**

**背景**：
- 大規模対訳コーパスの利用可能化
- 計算能力の向上
- IBM Model（1990年代前半）の登場

**特徴**：
- **対訳コーパス**から翻訳確率を学習
- **言語モデル**で自然な文を選択
- 人手で規則を書かない

**基本式**：
$$
\hat{e} = \arg\max_e P(e|f) = \arg\max_e P(f|e) \cdot P(e)
$$
- $e$: 英語文（目標言語）
- $f$: 外国語文（原言語）
- $P(f|e)$: 翻訳モデル（対訳コーパスから学習）
- $P(e)$: 言語モデル（自然さの評価）

**手法**：
- **単語ベース**：単語対応の学習
- **句ベース（Phrase-based）**：フレーズ単位の翻訳（主流）
- **階層的フレーズ**：構文構造を考慮

**利点**：
- 規則作成不要
- 大量データで高精度
- 多言語対応が容易

**欠点**：
- 大量の対訳データが必要
- 長距離依存の処理が苦手
- 文脈理解が不十分

**代表システム**：
- Google翻訳（2006年～2016年）
- Moses（オープンソース）

#### 3. ニューラル機械翻訳（2010年代後半～現在）
**第3世代：深層学習（ニューラルネット）による翻訳**

**背景**：
- 深層学習の発展
- Seq2Seqモデルの登場（2014年）
- Transformerの登場（2017年）

**特徴**：
- **End-to-End学習**：原言語→目標言語を直接学習
- **Encoder-Decoder構造**：文全体を一度エンコード→デコード
- **Attention機構**：翻訳時に原文の重要部分に注目

**発展段階**：
1. **RNN/LSTM Seq2Seq**（2014-2016）：系列変換の基礎
2. **Attention機構**（2015-2016）：入力の重要部分に注目
3. **Transformer**（2017～）：Self-Attentionで並列処理、現在の主流

**Transformerの利点**：
- 長距離依存の学習に優れる
- 並列処理で高速
- 大規模事前学習（BERT/GPT）の活用

**利点**：
- 翻訳精度の大幅向上
- 文脈理解が改善
- 流暢で自然な翻訳

**欠点**：
- 大量の計算リソースが必要
- 学習に時間がかかる
- ハルシネーション（誤訳生成）の可能性

**代表システム**：
- Google翻訳（2016年～NMT化）
- DeepL（Transformer）
- GPT系（多言語対応）

#### 機械翻訳の歴史まとめ表

| 時代 | 方式 | 主要技術 | データ | 精度 |
|------|------|----------|--------|------|
| **～1970年代** | **ルールベース** | 文法規則・辞書 | 人手作成 | 低～中 |
| **1990年代～** | **統計的** | 対訳コーパス・確率モデル | 大量対訳データ | 中～高 |
| **2015年代～** | **ニューラル** | Seq2Seq・Transformer | 大規模データ | 高 |

#### 試験での問われ方（機械翻訳の歴史）

**典型問題**：
**問**: 機械翻訳の歴史として、1970年代後半までは（A）機械翻訳、1990年代以降は（B）機械翻訳が主流であった。

- ✅ **正解**: (A) **ルールベース**、(B) **統計的**
- ❌ 誤答: (A) 統計的、(B) ニューラル → 年代が不一致
- ❌ 誤答: (A) ニューラル、(B) ルールベース → 逆転している

**キーポイント**：
- **～1970年代後半：ルールベース**（人手で文法規則作成）
- **1990年代～2010年代：統計的**（対訳データから学習）
- **2015年代～現在：ニューラル**（深層学習、Transformer）

**比較問題**：
- ルールベース vs 統計的：人手規則 vs データ駆動
- 統計的 vs ニューラル：単語/句ベース vs End-to-End学習
- 各方式の利点・欠点を問う問題

**引っ掛けポイント**：
- 「1990年代にニューラル」→ ❌ ニューラルは2010年代後半～
- 「ルールベースが現在の主流」→ ❌ 現在はニューラル（Transformer）
- 「統計的翻訳は使われていない」→ △ 一部で併用される場合もある

## 実例

### 例1：形態素解析の実行（MeCab）
```python
import MeCab

tagger = MeCab.Tagger()
text = "私は猫が好きです"
result = tagger.parse(text)

# 出力:
# 私     名詞,代名詞,一般,*
# は     助詞,係助詞,*,*
# 猫     名詞,一般,*,*
# が     助詞,格助詞,一般,*
# 好き   名詞,形容動詞語幹,*,*
# です   助動詞,*,*,*
```

### 例2：形態素解析 vs トークン化
```
英語: "I love cats"
→ トークン化で十分: ["I", "love", "cats"]

日本語: "私は猫が好きです"
→ 形態素解析が必要: ["私", "は", "猫", "が", "好き", "です"]
  （品詞も判別）
```

### 例3：処理階層の違い
```
入力: "東京の桜が美しい"

【形態素解析】
東京/名詞, の/助詞, 桜/名詞, が/助詞, 美しい/形容詞

【構文解析】
主語: 桜
述語: 美しい
修飾: 東京の→桜

【意味解析】
場所: 東京
対象: 桜
属性: 美しい（ポジティブ）
```

## 試験での問われ方

### 典型問題
**問**: 「文章を言語上で意味を持つ最小単位に分け、それぞれの品詞などを判別する処理」として最も適切なものを選べ。

- ✅ **正解**: 形態素解析
- ❌ 誤答1: 構文解析（文の構造を解析、形態素分割ではない）
- ❌ 誤答2: 意味解析（意味内容の理解、品詞判別ではない）
- ❌ 誤答3: トークン化（分割のみ、品詞判別を含まない）
- ❌ 誤答4: 係り受け解析（単語間の依存関係、形態素分割ではない）

### 比較問題（違いのポイント）

| 処理 | 入力単位 | 出力内容 | 処理階層 |
|------|----------|----------|----------|
| **形態素解析** | 文 | 形態素＋品詞 | **最も基礎** |
| トークン化 | 文 | 単語列（品詞なし） | 基礎 |
| 構文解析 | 形態素列 | 文法構造（主語・述語等） | 中級 |
| 係り受け解析 | 形態素列 | 単語間依存関係 | 中級 |
| 意味解析 | 文 | 意味内容・概念 | **最も高次** |

### 引っ掛けポイント
1. **「最小単位に分ける」→形態素解析**（構文解析は文の構造分析）
2. **「品詞を判別」→形態素解析**（トークン化は分割のみ）
3. **「単語間の関係」→係り受け解析**（形態素解析は分割＋品詞のみ）
4. **英語と日本語の違い**：英語はトークン化で済むことが多い、日本語は形態素解析が必須
5. **処理順序**：形態素解析→構文解析→意味解析（段階的に高次処理へ）

### 実務観点の設問
- **問**: 日本語文書の前処理として最初に行うべき処理は？
  - → 形態素解析（単語分割と品詞判別）
- **問**: 英語と日本語でNLP前処理の違いは？
  - → 英語はトークン化、日本語は形態素解析が必要（単語区切りなし）

## 補足

### 実務上の注意点
1. **辞書の選択**：IPA辞書、NEologd（新語対応）等、用途に応じて選択。
2. **未知語処理**：辞書にない単語の扱い（固有名詞・新語等）。
3. **分割の曖昧性**：「今日は」→「今日/は」or「今/日は」（文脈依存）。
4. **処理速度**：大量文書ではMeCabが高速、精度重視ならBERT等。
5. **言語依存性**：各言語で異なるツール・手法が必要。

### 現代NLPの主流
- **Transformer系モデル**（BERT/GPT）：形態素解析なしでサブワード（BPE等）で直接処理
- **End-to-End学習**：前処理を最小化し、ニューラルネットで一括学習
- ただしG検定では**古典的なNLP処理階層**（形態素解析→構文解析→意味解析）の理解が重要

### 関連トピック
- [Transformer](../06_deep_learning/transformer.md)：現代NLPの主流アーキテクチャ
- [RNN](../06_deep_learning/rnn.md)：系列データ処理、Seq2Seqで機械翻訳
- [音声処理](speech_processing.md)：音声認識（ASR）と自然言語処理の連携
