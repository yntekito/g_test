# 自然言語処理（Natural Language Processing, NLP）

## 要点
- **形態素解析**：文を形態素（意味の最小単位）に分割し品詞を判別。日本語処理の基礎。
- **構文解析**：文の文法構造（主語・述語等）を解析。形態素解析の次段階。
- **意味解析**：文の意味内容を理解。最も高次な処理。Transformer（BERT/GPT）が主流。
- **機械翻訳の歴史**：～1970年代＝ルールベース、1990年代～＝統計的、2015年代～＝ニューラル（Transformer）。

## 定義
**自然言語処理（NLP）**は、人間が日常使う言語（自然言語）をコンピュータで処理・理解・生成する技術。音声認識・機械翻訳・感情分析・質問応答など幅広い応用がある。

基本的な処理階層：
1. **形態素解析**：単語分割＋品詞判別
2. **構文解析**：文法構造の解析
3. **意味解析**：意味内容の理解
4. **文脈理解**：複数文の関係把握

## 重要キーワード
- **形態素（Morpheme）**：意味を持つ最小単位。「走る」「ない」など。
- **形態素解析（Morphological Analysis）**：文を形態素に分割し、各形態素の品詞・活用形等を判別する処理。日本語では必須（単語間に空白なし）。
- **トークン化（Tokenization）**：文章を単語や部分文字列に分割する処理。形態素解析より単純（品詞判別なし）。
- **構文解析（Syntactic Analysis, Parsing）**：文の文法構造を解析し、主語・述語・修飾関係等を特定。構文木で表現。
- **係り受け解析（Dependency Parsing）**：単語間の依存関係（どの単語がどの単語を修飾するか）を解析。
- **意味解析（Semantic Analysis）**：文の意味内容を理解し、概念・関係を抽出。
- **固有表現抽出（Named Entity Recognition, NER）**：人名・地名・組織名等を識別。
- **品詞（Part-of-Speech, POS）**：名詞・動詞・形容詞・助詞など文法的カテゴリ。
- **形態素解析器**：MeCab、Janome、Sudachi等のツール。辞書と統計モデルで分割・品詞判別。
- **分かち書き**：形態素解析の分割結果を空白区切りで表記（日本語の前処理）。
- **Word2Vec**：単語を分散表現（ベクトル）に変換する手法。Skip-gram/CBOW。
- **BERT（Bidirectional Encoder Representations from Transformers）**：双方向Transformerで文脈を考慮した単語埋め込み。
- **GPT（Generative Pre-trained Transformer）**：自己回帰型Transformerで文章生成。
- **Transformer**：Self-Attentionで長距離依存を学習。現代NLPの主流アーキテクチャ。
- **Attention機構**：入力の重要部分に注目する仕組み。機械翻訳で導入。
- **ルールベース機械翻訳**：文法規則と辞書を人手で作成（～1970年代後半）。
- **統計的機械翻訳**：対訳データから統計的に学習（1990年代～2010年代）。
- **ニューラル機械翻訳（NMT）**：深層学習による翻訳、Transformer使用（2015年代～）。
- **Seq2Seq（Sequence-to-Sequence）**：Encoder-Decoderで系列変換、機械翻訳の基礎。

## 詳細

### 形態素解析（Morphological Analysis）
**最も基本的な処理**で、文章を形態素に分割し、各形態素の品詞・活用形を判別する。

#### プロセス例（日本語）
```
入力: "私は東京に行きました"

【形態素解析の出力】
私    ：名詞（代名詞）
は    ：助詞（係助詞）
東京  ：名詞（固有名詞・地名）
に    ：助詞（格助詞）
行き  ：動詞（五段活用・連用形）
まし  ：助動詞（丁寧・過去）
た    ：助動詞（過去）

【分かち書き】
私 は 東京 に 行き まし た
```

#### 形態素解析の技術
1. **辞書ベース**：既知単語を辞書から検索（MeCab等）
2. **統計モデル**：出現確率・品詞遷移確率で最適分割を選択（HMM、CRF）
3. **深層学習**：RNN/Transformerで文脈考慮（BERT等）

#### 日本語での重要性
- 英語等は単語間に空白→トークン化が容易
- 日本語は単語の区切りがない→形態素解析が必須前処理

### トークン化との違い
| 処理 | 分割 | 品詞判別 | 用途 |
|------|------|----------|------|
| **形態素解析** | ✅ | ✅ | 日本語等、品詞情報が必要な場合 |
| **トークン化** | ✅ | ❌ | 英語等、単純な前処理 |

```
例: "I love cats"

【トークン化】
["I", "love", "cats"]

【形態素解析（品詞付き）】
I/代名詞, love/動詞, cats/名詞
```

### 構文解析（Syntactic Analysis）
形態素解析の**次段階**で、文の文法構造を解析する。

#### 構文木の例
```
入力: "私は猫が好きです"

       好き（述語）
      /    \
    私（主語）  猫（目的語）
```

- 主語・述語・目的語の関係を特定
- 修飾関係（形容詞→名詞など）を解析
- 文の階層構造を木構造で表現

### 係り受け解析（Dependency Parsing）
単語間の依存関係（どの単語がどの単語を修飾するか）を解析。

```
入力: "美しい花が咲いた"

美しい → 花（修飾）
花 → 咲いた（主語）
```

### 意味解析（Semantic Analysis）
文の**意味内容**を理解する最も高次な処理。

- **固有表現抽出（NER）**：人名・地名・日付等を識別
- **関係抽出**：エンティティ間の関係を特定（「東京は日本の首都」→ 首都(東京, 日本)）
- **感情分析**：ポジティブ/ネガティブ判定
- **質問応答**：質問文に対する回答生成

---

### 固定表現（Collocation / 慣用句・イディオム）★試験頻出

#### 定義
**固定表現**は、複数の単語が結びついて特定の意味を持つ表現。構成要素の単語の意味から全体の意味を予測できない場合が多い。

**類似用語**：
- **慣用句**：固定的に使われる言い回し
- **イディオム（Idiom）**：特定の文化的背景を持つ慣用表現
- **連語（Collocation）**：頻繁に共起する単語の組み合わせ

#### 固定表現の例

**日本語**：
- 「腹を割って話す」→ 直訳の意味ではなく「率直に話す」
- 「足を洗う」→ 「悪い習慣をやめる」
- 「猫の手も借りたい」→ 「とても忙しい」
- 「油を売る」→ 「仕事をさぼる」

**英語**：
- "kick the bucket" → 直訳「バケツを蹴る」、実際は「死ぬ」
- "break the ice" → 直訳「氷を割る」、実際は「場を和ませる」
- "piece of cake" → 直訳「ケーキ一切れ」、実際は「簡単なこと」
- "by the way" → 「ところで」（文字通りの意味ではない）

#### 固定表現の特徴

| 特徴 | 説明 | 例 |
|------|------|-----|
| **意味の非構成性** | 各単語の意味の合計≠全体の意味 | 「猫の手」≠猫の物理的な手 |
| **語順の固定性** | 順序を変えられない | 「白黒つける」○、「黒白つける」× |
| **置換不可能性** | 類義語で置き換えられない | 「腹を割る」○、「胃を割る」× |
| **文化依存性** | 言語・文化圏で意味が異なる | 日本語の慣用句は直訳不可 |

#### 自然言語処理での課題

**問題点**：
- **直訳では意味が通じない**：機械翻訳で誤訳の原因
- **辞書に載っていない**：新しい固定表現は認識困難
- **構成要素から意味推測不可**：個別に学習が必要

**対策**：
1. **固定表現辞書**：既知の慣用句を辞書登録
2. **フレーズベース翻訳**：句単位で翻訳（統計的機械翻訳）
3. **文脈学習**：大規模コーパスから自動学習（BERT/GPT等）
4. **共起統計**：頻繁に共起する単語の組み合わせを検出

#### 機械翻訳での固定表現

**統計的機械翻訳（2000年代）**：
- **句ベース（Phrase-based）翻訳**：単語単位ではなく句単位で翻訳
- 対訳コーパスから固定表現の対応を学習
- 例：「kick the bucket」→「死ぬ」を句として学習

**ニューラル機械翻訳（2015年代～）**：
- **End-to-End学習**：固定表現を文脈から自動学習
- **Attention機構**：関連する単語群をまとめて処理
- **Transformer**：長距離依存を学習、固定表現の認識が向上

---

### 試験での問われ方（固定表現）

#### 典型的な出題パターン

**パターン1：固定表現の説明として最も適切な選択肢を選べ**

✅ **適切な選択肢（正しい説明）**：
- **「複数の単語が結びついて特定の意味を持つ表現」** → ✅ **最も適切**
- **「構成要素の単語の意味から全体の意味を予測できない表現」** → ✅ 適切
- **「慣用句やイディオムなど、固定的に使われる言い回し」** → ✅ 適切
- **「語順が固定されており、類義語で置き換えられない表現」** → ✅ 適切
- **「機械翻訳で直訳すると誤訳になりやすい表現」** → ✅ 適切
- **「句ベース翻訳で対応される表現」** → ✅ 適切

❌ **不適切な選択肢（誤った説明、試験で狙われる）**：
- **「単一の単語で意味を持つ表現」** → ❌ **最も不適切**（複数単語の組み合わせ）
- **「形態素解析で分割された各単語の意味の合計で理解できる表現」** → ❌ 不適切（非構成的）
- **「単語を自由に入れ替えられる表現」** → ❌ 不適切（語順固定）
- **「品詞を判別する処理」** → ❌ 不適切（形態素解析の説明）
- **「文法構造を解析する処理」** → ❌ 不適切（構文解析の説明）
- **「常に同じ意味を持つ単語」** → ❌ 不適切（文脈で意味が変わる場合がある）

---

**パターン2：固定表現の例を選ぶ問題**

✅ **固定表現に該当する例**：
- 「腹を割って話す」（率直に話す）
- 「猫の手も借りたい」（とても忙しい）
- 「油を売る」（仕事をさぼる）
- "kick the bucket"（死ぬ）
- "piece of cake"（簡単なこと）

❌ **固定表現ではない例**：
- 「赤いりんご」（各単語の意味の合計）
- 「速く走る」（構成的な意味）
- 「大きな家」（通常の修飾関係）

---

**パターン3：機械翻訳での固定表現の扱い**

> 「機械翻訳で固定表現を正しく翻訳するための手法として、最も適切な選択肢を1つ選べ。」

✅ **適切な手法**：
- **句ベース翻訳**（統計的機械翻訳）
- 固定表現辞書の使用
- 文脈学習（Transformer）
- 対訳コーパスからの自動学習

❌ **不適切な手法**：
- 単語単位の直訳（意味が通じない）
- 形態素解析のみ（意味理解なし）

---

#### 引っ掛けポイント

| ひっかけ | 正しい理解 |
|----------|------------|
| ❌ 単一単語で意味を持つ | ✅ 複数単語の組み合わせ |
| ❌ 各単語の意味の合計 | ✅ 非構成的（全体で新しい意味） |
| ❌ 語順を自由に変えられる | ✅ 語順は固定 |
| ❌ 類義語で置換可能 | ✅ 置換不可能 |
| ❌ 形態素解析で理解可能 | ✅ 意味解析が必要 |

---

### TF-IDF（Term Frequency-Inverse Document Frequency）

**TF-IDF**は、文書中の単語の重要度を表す統計的尺度。「その単語がその文書でどれくらい特徴的か」を数値化する。

#### 定義

TF-IDF = TF（単語頻度） × IDF（逆文書頻度）

**TF（Term Frequency：単語頻度）**：
- ある単語が**特定の文書内**で出現する頻度
- 計算式：$\text{TF}(t, d) = \frac{\text{文書}d\text{内での単語}t\text{の出現回数}}{\text{文書}d\text{の総単語数}}$
- 直感：「その文書でよく使われる単語ほど高い値」

**IDF（Inverse Document Frequency：逆文書頻度）**：
- ある単語が**全文書中でどれくらい希少か**を表す
- 計算式：$\text{IDF}(t) = \log \frac{\text{全文書数}}{\text{単語}t\text{を含む文書数}}$
- 直感：「どの文書にも出る単語（"the", "は"）はIDF低い、特定文書だけに出る単語はIDF高い」

**TF-IDF**：
$$\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)$$

- **高い値**：その文書で頻繁に使われ、かつ他文書では珍しい → **文書の特徴語**
- **低い値**：どの文書にも出る一般的な単語（"です", "ます", "the", "is"）

#### 実例：ニュース文書の重要語抽出

**文書集合**：
- 文書A：「AIは医療診断で活用されている。AIの精度向上が課題。」
- 文書B：「機械学習は画像認識に使われる。精度向上が重要。」
- 文書C：「深層学習の応用が進んでいる。画像認識で実用化。」

**文書Aでの各単語のTF-IDF計算**：

| 単語 | 文書A内のTF | 出現文書数 | IDF | TF-IDF | 解釈 |
|------|-------------|-----------|-----|--------|------|
| **AI** | 2/12 = 0.167 | 1 | log(3/1) = 0.477 | 0.080 | **高い**（文書A特有） |
| **医療診断** | 1/12 = 0.083 | 1 | log(3/1) = 0.477 | 0.040 | **高い**（文書A特有） |
| **精度向上** | 1/12 = 0.083 | 2 | log(3/2) = 0.176 | 0.015 | 中程度（2文書に出現） |
| **は** | 1/12 = 0.083 | 3 | log(3/3) = 0 | 0 | **低い**（全文書に出現） |
| **で** | 1/12 = 0.083 | 3 | log(3/3) = 0 | 0 | **低い**（全文書に出現） |

**結論**：文書Aの特徴語は「AI」「医療診断」→ 文書の内容を的確に表す。

#### 用途

1. **情報検索（Search Engine）**：
   - クエリ単語のTF-IDFが高い文書を上位表示
   - Googleの初期アルゴリズムで使用

2. **文書分類**：
   - TF-IDFベクトルを特徴量として機械学習
   - ナイーブベイズ、SVMの入力に

3. **キーワード抽出**：
   - TF-IDF上位の単語を文書の要約キーワードとして抽出

4. **推薦システム**：
   - ユーザーが閲覧した文書のTF-IDFベクトルから類似文書を推薦

#### TF-IDFの限界と発展

**限界**：
- **単語の順序を無視**（Bag-of-Words前提）→ 「犬が猫を追う」と「猫が犬を追う」が同じ
- **単語の意味を考慮しない**：「大きい」と「巨大」が別単語扱い
- **文脈を理解しない**：「銀行（bank）」の多義性を区別できない

**発展的手法**：
- **Word2Vec（2013）**：単語を意味ベクトルに変換。類似語を考慮。
- **BERT（2018）**：文脈を考慮した単語埋め込み。多義性を解決。
- **BM25**：TF-IDFの改良版。情報検索で広く使用。

#### 試験での問われ方

**典型問題**：

> 「文書中に含まれる単語が、文書内でどれくらい重要かを表す統計的尺度として、最も適切な選択肢を1つ選べ。」

✅ **正解**：**TF-IDF**（Term Frequency-Inverse Document Frequency）

❌ **不正解の選択肢**：
- **Word2Vec**：単語の分散表現（ベクトル化）。重要度の尺度ではない。
- **Attention機構**：系列データの重要部分に注目。TF-IDFとは別の概念。
- **形態素解析**：単語分割と品詞判別。重要度計算ではない。
- **構文解析**：文法構造の解析。重要度とは無関係。

**ひっかけポイント**：
- ❌ 「単語の意味的類似度」→ Word2Vec/BERTの役割
- ❌ 「文法構造の解析」→ 構文解析の役割
- ❌ 「単語の共起関係」→ 共起行列の役割
- ✅ 「文書内での単語の特徴度・重要度」→ **TF-IDF**

**比較問題**：

| 手法 | 目的 | 出力 |
|------|------|------|
| **TF-IDF** | 単語の文書内重要度を数値化 | スカラー値（重要度スコア） |
| **Word2Vec** | 単語を意味ベクトルに変換 | 高次元ベクトル（類似度計算可能） |
| **BERT** | 文脈考慮の単語埋め込み | 文脈依存ベクトル（多義性解決） |
| **形態素解析** | 単語分割と品詞判別 | 形態素リスト＋品詞タグ |

---

### Bag of Words（BoW）と N-Gram

#### Bag of Words（BoW：単語の袋）

**BoW（Bag of Words）**は、文書を**単語の出現頻度**だけで表現する最もシンプルな手法。単語の**順序を完全に無視**する。

**定義**：
- 文書内の各単語の出現回数をカウント
- 順序情報を捨てて「単語の袋」として扱う
- ベクトル表現：各要素が単語の出現回数

**実例**：

```
文書1：「私は猫が好きです」
文書2：「私は犬が好きです」
文書3：「猫と犬が好きです」

【BoW表現（出現回数ベクトル）】
語彙：[私, は, 猫, が, 好き, です, 犬, と]

文書1 = [1, 1, 1, 1, 1, 1, 0, 0]
        私 は 猫 が 好き です 犬 と
文書2 = [1, 1, 0, 1, 1, 1, 1, 0]
文書3 = [0, 0, 1, 1, 1, 1, 1, 1]
```

**特徴**：
- ✅ **簡単**: 実装が容易、計算コスト低
- ✅ **解釈性**: どの単語が重要か一目瞭然
- ❌ **順序無視**: 「猫が犬を追う」と「犬が猫を追う」が同じ
- ❌ **文脈無視**: 単語の関係性を捉えられない
- ❌ **疎なベクトル**: 語彙数が大きいとベクトルがほぼゼロで埋まる

**用途**：
- 文書分類（ナイーブベイズ、SVM）
- スパムフィルタ
- TF-IDFの基礎（BoW + 重み付け）

---

#### N-Gram

**N-Gram**は、**連続するN個の単語（または文字）の組み合わせ**を分析する手法。BoWと異なり**順序を考慮**する。

**定義**：
- **N=1（Unigram）**：単語単位（BoWと同じ）
- **N=2（Bigram）**：連続する2単語の組
- **N=3（Trigram）**：連続する3単語の組
- **N≥4**：4-gram、5-gram等

**実例**：

```
文：「私は猫が好きです」

【Unigram（1-gram）】
["私", "は", "猫", "が", "好き", "です"]

【Bigram（2-gram）】
["私は", "は猫", "猫が", "が好き", "好きです"]

【Trigram（3-gram）】
["私は猫", "は猫が", "猫が好き", "が好きです"]
```

**文字N-Gram（Character N-Gram）**：
```
文：「猫が好き」

【文字Bigram】
["猫が", "が好", "好き"]

【文字Trigram】
["猫が好", "が好き"]
```

**特徴**：
- ✅ **順序考慮**: 単語の並び方を部分的に捉える
- ✅ **文脈情報**: 近接単語の関係を表現
- ✅ **統計的言語モデル**: N-gram確率で文の自然さを評価
- ❌ **語彙爆発**: Nが大きいとパターン数が膨大
- ❌ **データスパース**: 学習データに出現しない組み合わせが多数

**用途**：
- 言語モデル（文の確率計算）
- 文章生成
- スペルチェック
- 音声認識（次単語予測）
- 機械翻訳（統計的手法）

**N-gram言語モデル**：
- 次単語の出現確率を前N-1単語から予測
- 例：「猫が」の後に「好き」が来る確率 $P(\text{好き}|\text{猫が})$

---

#### BoW、N-Gram、TF-IDF、フォルマント分析の比較（重要）

| 項目 | BoW | N-Gram | TF-IDF | フォルマント分析 |
|------|-----|--------|--------|-----------------|
| **分野** | 自然言語処理 | 自然言語処理 | 自然言語処理 | **音声処理** |
| **対象** | テキスト文書 | テキスト文書 | テキスト文書 | **音声信号** |
| **目的** | 文書のベクトル表現 | 単語パターン抽出 | 単語の重要度計算 | **音声の周波数特性分析** |
| **順序** | ❌ 無視 | ✅ 考慮（N個まで） | ❌ 無視 | — （時系列） |
| **出力** | 出現回数ベクトル | N個の組み合わせリスト | 重要度スコア | **共鳴周波数（Hz）** |
| **計算** | 単語カウント | 連続パターン抽出 | TF × IDF | **FFT、スペクトル分析** |
| **利点** | 簡単、高速 | 文脈考慮、次単語予測 | 重要語抽出 | **母音識別、音色分析** |
| **欠点** | 順序無視、疎 | 語彙爆発、データスパース | 順序無視 | 音声専用、テキスト不可 |
| **用途例** | 文書分類 | 言語モデル、統計的機械翻訳 | 情報検索、キーワード抽出 | **音声認識、話者認識** |

**ひっかけポイント**：
- ❌ 「BoWは順序を考慮する」→ 誤り。**順序を完全に無視**
- ✅ 「N-Gramは順序を考慮する」→ 正しい。連続N個の組み合わせ
- ❌ 「TF-IDFは単語の意味的類似度を計算」→ 誤り。**重要度のみ**（意味はWord2Vec）
- ❌ 「フォルマント分析はテキストの重要語を抽出」→ 誤り。**音声の周波数分析**
- ✅ 「フォルマント分析は母音の識別に使われる」→ 正しい

---

#### 試験での問われ方

**典型問題1：BoWの説明**

> 「文書を単語の出現頻度で表現し、単語の順序を無視する手法として、最も適切な選択肢を1つ選べ。」

✅ **正解**：**Bag of Words（BoW）**

❌ **不正解の選択肢**：
- **N-Gram**：順序を考慮する（連続N個の組み合わせ）
- **TF-IDF**：重要度計算（BoWに重み付けを追加したもの）
- **Word2Vec**：単語の意味ベクトル化
- **フォルマント分析**：音声の周波数特性分析（テキスト処理ではない）

---

**典型問題2：N-Gramの説明**

> 「連続するN個の単語の組み合わせを分析し、単語の順序を考慮する手法として、最も適切な選択肢を1つ選べ。」

✅ **正解**：**N-Gram**

❌ **不正解の選択肢**：
- **BoW**：順序を無視
- **TF-IDF**：順序を無視、重要度計算
- **形態素解析**：単語分割と品詞判別（パターン抽出ではない）

---

**典型問題3：フォルマント分析との区別**

> 「音声信号の周波数特性を分析し、母音の識別に使用される手法として、最も適切な選択肢を1つ選べ。」

✅ **正解**：**フォルマント分析**

❌ **不正解の選択肢**：
- **BoW**：テキスト処理（音声ではない）
- **N-Gram**：テキスト処理（音声ではない）
- **TF-IDF**：テキスト処理（音声ではない）
- **MFCC**：音声特徴量だが、フォルマント自体ではない

---

#### 実務での使い分け

| 状況 | 推奨手法 | 理由 |
|------|---------|------|
| **単純な文書分類** | BoW + TF-IDF | 簡単、高速、解釈性高 |
| **統計的言語モデル** | N-Gram | 次単語予測に適切 |
| **意味理解が必要** | Word2Vec、BERT | BoW/N-Gramは意味を捉えられない |
| **音声認識** | **MFCC、フォルマント分析** | 音響特徴量が必要 |
| **現代NLP（深層学習）** | Transformer（BERT/GPT） | 長距離依存、文脈理解に優れる |

---

### NLPの代表的タスク

| タスク | 説明 | 代表手法 |
|--------|------|----------|
| **機械翻訳** | 言語間の自動翻訳 | Transformer, Seq2Seq |
| **文書分類** | テキストをカテゴリ分類 | CNN, BERT |
| **固有表現抽出** | 人名・地名等の識別 | CRF, BERT |
| **感情分析** | 感情極性の判定 | RNN, BERT |
| **質問応答** | 質問に対する回答生成 | BERT, GPT |
| **要約** | 文書の要約生成 | Seq2Seq, T5 |
| **対話システム** | 会話応答の生成 | GPT, Transformer |

### 機械翻訳の歴史（試験頻出）

機械翻訳は自然言語処理の主要応用の一つで、その発展は**3つの時代**に分けられる。

#### 1. ルールベース機械翻訳（～1970年代後半）
**第1世代：人手で文法規則と辞書を作成**

**特徴**：
- 言語学者が文法規則を記述
- 単語辞書を人手で構築
- 構文解析→変換→生成の3段階

**方式**：
- **直接翻訳**：単語対応のみ
- **トランスファー方式**：原言語→中間表現→目標言語
- **インターリンガ方式**：言語非依存な意味表現を介する

**利点**：
- 文法規則が明確なら高精度
- ドメイン限定なら実用的

**欠点**：
- 規則作成に膨大なコスト
- 例外処理が困難
- スケーラビリティが低い

**代表システム**：
- SYSTRAN（1968年～）
- Eurotra（EU）

#### 2. 統計的機械翻訳（1990年代～2010年代前半）
**第2世代：大量の対訳データから統計的に学習**

**背景**：
- 大規模対訳コーパスの利用可能化
- 計算能力の向上
- IBM Model（1990年代前半）の登場

**特徴**：
- **対訳コーパス**から翻訳確率を学習
- **言語モデル**で自然な文を選択
- 人手で規則を書かない

**基本式**：
$$
\hat{e} = \arg\max_e P(e|f) = \arg\max_e P(f|e) \cdot P(e)
$$
- $e$: 英語文（目標言語）
- $f$: 外国語文（原言語）
- $P(f|e)$: 翻訳モデル（対訳コーパスから学習）
- $P(e)$: 言語モデル（自然さの評価）

**手法**：
- **単語ベース**：単語対応の学習
- **句ベース（Phrase-based）**：フレーズ単位の翻訳（主流）
- **階層的フレーズ**：構文構造を考慮

**利点**：
- 規則作成不要
- 大量データで高精度
- 多言語対応が容易

**欠点**：
- 大量の対訳データが必要
- 長距離依存の処理が苦手
- 文脈理解が不十分

**代表システム**：
- Google翻訳（2006年～2016年）
- Moses（オープンソース）

#### 3. ニューラル機械翻訳（2010年代後半～現在）
**第3世代：深層学習（ニューラルネット）による翻訳**

**背景**：
- 深層学習の発展
- Seq2Seqモデルの登場（2014年）
- Transformerの登場（2017年）

**特徴**：
- **End-to-End学習**：原言語→目標言語を直接学習
- **Encoder-Decoder構造**：文全体を一度エンコード→デコード
- **Attention機構**：翻訳時に原文の重要部分に注目

**発展段階**：
1. **RNN/LSTM Seq2Seq**（2014-2016）：系列変換の基礎
2. **Attention機構**（2015-2016）：入力の重要部分に注目
3. **Transformer**（2017～）：Self-Attentionで並列処理、現在の主流

**Transformerの利点**：
- 長距離依存の学習に優れる
- 並列処理で高速
- 大規模事前学習（BERT/GPT）の活用

**利点**：
- 翻訳精度の大幅向上
- 文脈理解が改善
- 流暢で自然な翻訳
- 長距離依存の学習に優れる
- End-to-End学習で複雑な規則不要

**欠点（問題点）**：
- **大量の計算リソースが必要**：学習・推論にGPU必須
- **学習に時間がかかる**：大規模データで数日～数週間
- **ハルシネーション（誤訳生成）**：流暢だが誤った翻訳を生成する可能性
- **大量の対訳データが必要**：低リソース言語では性能低下
- **ブラックボックス化**：なぜその翻訳になったか説明困難
- **ドメイン適応が必要**：専門分野では追加学習が必要
- **流暢すぎて誤訳に気づきにくい**：自然な文だが意味が異なる場合あり

**代表システム**：
- Google翻訳（2016年～NMT化）
- DeepL（Transformer）
- GPT系（多言語対応）

#### ニューラル機械翻訳（NMT）の問題点★試験頻出

##### NMTの実際の問題点（適切な選択肢）

✅ **NMTの問題点として適切**：

1. **「大量の計算リソース（GPU等）が必要である」**
   - 深層学習モデルの学習・推論には高性能GPUが必須
   - 統計的翻訳より計算コストが高い

2. **「学習に長時間を要する」**
   - 大規模データでの学習に数日～数週間かかる
   - 統計的翻訳より学習時間が長い

3. **「ハルシネーション（誤訳）を生成する可能性がある」**
   - 流暢だが事実と異なる翻訳を生成することがある
   - 確率的な生成のため予測不可能な誤訳

4. **「大量の対訳データが必要である」**
   - 数百万～数千万文の対訳コーパスが必要
   - 低リソース言語では性能が低下

5. **「ブラックボックスで翻訳理由を説明しづらい」**
   - なぜその翻訳になったか内部処理が不透明
   - ルールベースのような明示的な規則がない

6. **「ドメイン適応が必要である」**
   - 一般ドメインで学習したモデルは専門分野で精度低下
   - 医療・法律等では追加学習が必要

7. **「流暢すぎて誤訳に気づきにくい」**
   - 文法的に正しく自然な文でも意味が異なる場合がある
   - 統計的翻訳の不自然な訳より誤訳発見が困難

##### NMTの問題点として最も不適切な選択肢（典型的な誤答）

❌ **NMTの問題点として最も不適切**（実際は利点）：

1. **「人手で文法規則を作成するコストが膨大である」**
   - → ❌ **誤り**：これは**ルールベース翻訳**の問題点
   - NMTは規則不要、データから自動学習

2. **「短距離依存しか学習できない」**
   - → ❌ **誤り**：NMT（特にTransformer）は**長距離依存の学習に優れる**
   - 統計的翻訳やRNNよりも長距離依存を扱える

3. **「文脈理解が不十分である」**
   - → ❌ **誤り**：NMTは**文脈理解が改善**されている
   - 統計的翻訳より文脈を考慮した翻訳が可能

4. **「対訳データが全く不要である」**
   - → ❌ **誤り**：NMTは**大量の対訳データが必要**
   - これはNMTの利点ではなく、逆に問題点（データ要件）

5. **「単語単位でしか翻訳できない」**
   - → ❌ **誤り**：NMTは**文全体を考慮したEnd-to-End翻訳**
   - 統計的翻訳の単語/句ベースの制約がない

6. **「翻訳精度が低く実用に耐えない」**
   - → ❌ **誤り**：NMTは**翻訳精度が大幅向上**
   - 統計的翻訳より高精度（これがNMT化の理由）

7. **「並列処理ができず学習が遅い」**
   - → ❌ **誤り**：Transformerは**並列処理で高速**
   - RNNの逐次処理の問題を解決

##### 機械翻訳方式ごとの問題点の整理（対比表）

| 方式 | 主な問題点 | NMTとの関係 |
|------|------------|-------------|
| **ルールベース** | **人手で規則作成**、スケーラビリティ低い | NMTは規則不要 |
| **統計的** | 単語/句単位、長距離依存が苦手 | NMTは文全体をEnd-to-End学習 |
| **ニューラル** | **計算コスト大**、**大量データ必要**、ハルシネーション | 現在の主流だが課題あり |

##### 試験での問われ方

**典型問題**：「ニューラル機械翻訳（NMT）における問題点として、最も不適切な選択肢を1つ選べ。」

**選択肢例**：
- A. 大量の計算リソースが必要である → ✅ 適切（実際の問題点）
- B. 学習に長時間を要する → ✅ 適切（実際の問題点）
- C. **人手で文法規則を作成するコストが膨大である** → ❌ **最も不適切**（ルールベースの問題点）
- D. ハルシネーション（誤訳）を生成する可能性がある → ✅ 適切（実際の問題点）

**正解**: **C（最も不適切）**

**理由**：
- NMTは**データ駆動**で文法規則を自動学習
- 人手で規則を作成するのは**ルールベース翻訳**の特徴と問題点
- NMTの問題点は「計算コスト」「データ要件」「ハルシネーション」等

##### 引っ掛けポイント

| ひっかけ | 正しい理解 | 実際の問題がある方式 |
|----------|------------|----------------------|
| ❌ 規則作成コスト | ✅ NMTは規則不要、自動学習 | **ルールベース** |
| ❌ 短距離依存のみ | ✅ 長距離依存に優れる | 統計的・RNN |
| ❌ 文脈理解不十分 | ✅ 文脈理解が改善 | 統計的翻訳 |
| ❌ 単語単位のみ | ✅ 文全体をEnd-to-End | 統計的翻訳 |
| ✅ 計算コスト大 | ✅ 実際の問題点 | **ニューラル** |
| ✅ 大量データ必要 | ✅ 実際の問題点 | **ニューラル** |
| ✅ ハルシネーション | ✅ 実際の問題点 | **ニューラル** |

**キーポイント**：
- **ルールベースの問題点**：規則作成コスト、スケーラビリティ
- **統計的翻訳の問題点**：長距離依存が苦手、文脈理解不十分
- **ニューラル翻訳の問題点**：計算コスト、データ要件、ハルシネーション

#### 機械翻訳の歴史まとめ表

| 時代 | 方式 | 主要技術 | データ | 精度 |
|------|------|----------|--------|------|
| **～1970年代** | **ルールベース** | 文法規則・辞書 | 人手作成 | 低～中 |
| **1990年代～** | **統計的** | 対訳コーパス・確率モデル | 大量対訳データ | 中～高 |
| **2015年代～** | **ニューラル** | Seq2Seq・Transformer | 大規模データ | 高 |

#### 試験での問われ方（機械翻訳の歴史）

**典型問題**：
**問**: 機械翻訳の歴史として、1970年代後半までは（A）機械翻訳、1990年代以降は（B）機械翻訳が主流であった。

- ✅ **正解**: (A) **ルールベース**、(B) **統計的**
- ❌ 誤答: (A) 統計的、(B) ニューラル → 年代が不一致
- ❌ 誤答: (A) ニューラル、(B) ルールベース → 逆転している

**キーポイント**：
- **～1970年代後半：ルールベース**（人手で文法規則作成）
- **1990年代～2010年代：統計的**（対訳データから学習）
- **2015年代～現在：ニューラル**（深層学習、Transformer）

**比較問題**：
- ルールベース vs 統計的：人手規則 vs データ駆動
- 統計的 vs ニューラル：単語/句ベース vs End-to-End学習
- 各方式の利点・欠点を問う問題

**引っ掛けポイント**：
- 「1990年代にニューラル」→ ❌ ニューラルは2010年代後半～
- 「ルールベースが現在の主流」→ ❌ 現在はニューラル（Transformer）
- 「統計的翻訳は使われていない」→ △ 一部で併用される場合もある

## 実例

### 例1：形態素解析の実行（MeCab）
```python
import MeCab

tagger = MeCab.Tagger()
text = "私は猫が好きです"
result = tagger.parse(text)

# 出力:
# 私     名詞,代名詞,一般,*
# は     助詞,係助詞,*,*
# 猫     名詞,一般,*,*
# が     助詞,格助詞,一般,*
# 好き   名詞,形容動詞語幹,*,*
# です   助動詞,*,*,*
```

### 例2：形態素解析 vs トークン化
```
英語: "I love cats"
→ トークン化で十分: ["I", "love", "cats"]

日本語: "私は猫が好きです"
→ 形態素解析が必要: ["私", "は", "猫", "が", "好き", "です"]
  （品詞も判別）
```

### 例3：処理階層の違い
```
入力: "東京の桜が美しい"

【形態素解析】
東京/名詞, の/助詞, 桜/名詞, が/助詞, 美しい/形容詞

【構文解析】
主語: 桜
述語: 美しい
修飾: 東京の→桜

【意味解析】
場所: 東京
対象: 桜
属性: 美しい（ポジティブ）
```

## 試験での問われ方

### 典型問題
**問**: 「文章を言語上で意味を持つ最小単位に分け、それぞれの品詞などを判別する処理」として最も適切なものを選べ。

- ✅ **正解**: 形態素解析
- ❌ 誤答1: 構文解析（文の構造を解析、形態素分割ではない）
- ❌ 誤答2: 意味解析（意味内容の理解、品詞判別ではない）
- ❌ 誤答3: トークン化（分割のみ、品詞判別を含まない）
- ❌ 誤答4: 係り受け解析（単語間の依存関係、形態素分割ではない）

### 比較問題（違いのポイント）

| 処理 | 入力単位 | 出力内容 | 処理階層 |
|------|----------|----------|----------|
| **形態素解析** | 文 | 形態素＋品詞 | **最も基礎** |
| トークン化 | 文 | 単語列（品詞なし） | 基礎 |
| 構文解析 | 形態素列 | 文法構造（主語・述語等） | 中級 |
| 係り受け解析 | 形態素列 | 単語間依存関係 | 中級 |
| 意味解析 | 文 | 意味内容・概念 | **最も高次** |

### 引っ掛けポイント
1. **「最小単位に分ける」→形態素解析**（構文解析は文の構造分析）
2. **「品詞を判別」→形態素解析**（トークン化は分割のみ）
3. **「単語間の関係」→係り受け解析**（形態素解析は分割＋品詞のみ）
4. **英語と日本語の違い**：英語はトークン化で済むことが多い、日本語は形態素解析が必須
5. **処理順序**：形態素解析→構文解析→意味解析（段階的に高次処理へ）

### 実務観点の設問
- **問**: 日本語文書の前処理として最初に行うべき処理は？
  - → 形態素解析（単語分割と品詞判別）
- **問**: 英語と日本語でNLP前処理の違いは？
  - → 英語はトークン化、日本語は形態素解析が必要（単語区切りなし）

## 補足

### 実務上の注意点
1. **辞書の選択**：IPA辞書、NEologd（新語対応）等、用途に応じて選択。
2. **未知語処理**：辞書にない単語の扱い（固有名詞・新語等）。
3. **分割の曖昧性**：「今日は」→「今日/は」or「今/日は」（文脈依存）。
4. **処理速度**：大量文書ではMeCabが高速、精度重視ならBERT等。
5. **言語依存性**：各言語で異なるツール・手法が必要。

### 現代NLPの主流
- **Transformer系モデル**（BERT/GPT）：形態素解析なしでサブワード（BPE等）で直接処理
- **End-to-End学習**：前処理を最小化し、ニューラルネットで一括学習
- ただしG検定では**古典的なNLP処理階層**（形態素解析→構文解析→意味解析）の理解が重要

---

## Word2Vec（単語の分散表現）★試験頻出

### 定義
**Word2Vec**は、2013年にGoogleのTomas Mikolovらが発表した、単語を低次元の実数値ベクトル（分散表現）に変換する手法。単語の意味的類似性をベクトル空間上の距離で表現できる。

### 要点
- **分散表現（Distributed Representation）**：単語を200〜300次元の連続値ベクトルで表現
- **意味的類似性**：類似した意味の単語は近いベクトル位置に配置
- **ベクトル演算**：king - man + woman ≈ queen のような意味演算が可能
- **2つの学習方式**：**CBOW**（文脈→中心語）と**Skip-gram**（中心語→文脈）

### Word2Vecの2つのアルゴリズム

#### CBOW（Continuous Bag of Words）

**定義**：
周辺単語（文脈）から中心単語を予測する学習方式。複数の文脈単語を入力として、中心の単語を出力する。

**構造**：
```
【入力】周辺単語（文脈）
   私は [  ] に行く
    ↓    ↓    ↓
   「私」「に」「行く」
         ↓
    入力層（3つの単語ベクトル）
         ↓
    隠れ層（平均化）
         ↓
    出力層（ソフトマックス）
         ↓
【出力】中心単語の予測
    「銀行」
```

**学習プロセス**：
```
文：「私は銀行に行く」

【例1】ウィンドウサイズ = 1
文脈: [私は, に] → 中心語: 銀行

【例2】ウィンドウサイズ = 2
文脈: [私, は, に, 行く] → 中心語: 銀行
```

**特徴**：
- ✅ **高速**: 複数の文脈単語を平均化するため計算が速い
- ✅ **頻出語に強い**: 文脈の統計的パターンを効率的に学習
- ❌ **低頻度語に弱い**: データが少ない単語の学習が不十分
- ✅ **文脈の順序を無視**: Bag of Wordsのように扱う

**計算**：
1. 周辺単語のベクトルを取得
2. ベクトルを平均化（または合計）
3. 隠れ層を経由
4. ソフトマックスで中心単語を予測

---

#### Skip-gram

**定義**：
中心単語から周辺単語（文脈）を予測する学習方式。CBOWとは**逆の予測方向**。

**構造**：
```
【入力】中心単語
    「銀行」
      ↓
   入力層
      ↓
   隠れ層
      ↓
   出力層（ソフトマックス）
      ↓
【出力】周辺単語の予測
  「私」「は」「に」「行く」
```

**学習プロセス**：
```
文：「私は銀行に行く」

中心語: 銀行 → 文脈: [私は, に]（複数の予測タスク）
中心語: 銀行 → 「私」を予測
中心語: 銀行 → 「は」を予測
中心語: 銀行 → 「に」を予測
中心語: 銀行 → 「行く」を予測
```

**特徴**：
- ✅ **低頻度語に強い**: 少量のデータでも各単語を中心に学習
- ✅ **精度が高い**: 小規模データセットで高精度
- ❌ **計算コスト高**: 各中心語から複数の文脈語を予測するため遅い
- ✅ **文脈の順序を考慮**: 各周辺単語を個別に予測

---

### CBOW vs Skip-gram の比較（★試験頻出）

| 項目 | CBOW | Skip-gram |
|------|------|-----------|
| **予測方向** | 文脈 → **中心語** | **中心語** → 文脈 |
| **入力** | 周辺単語（複数） | 中心単語（1つ） |
| **出力** | 中心単語（1つ） | 周辺単語（複数） |
| **計算速度** | **速い** | 遅い |
| **低頻度語** | 弱い | **強い** |
| **大規模データ** | **適している** | やや不向き |
| **小規模データ** | やや不向き | **適している** |
| **精度** | やや低い | **高い** |
| **用途** | 大規模コーパス、速度重視 | 小規模データ、精度重視 |

**覚え方**：
- **CBOW** = Context → Bow（文脈から中心へ）= 速いが粗い
- **Skip-gram** = Skip（飛ばして）周辺を予測 = 遅いが精密

---

### CBOWモデルの説明として最も適切な選択肢（★試験頻出）

> 「自然言語処理において、CBOWモデルに関する説明として、最も適切な選択肢を1つ選べ。」

✅ **正解の選択肢**：
- **周辺単語（文脈）から中心単語を予測する**
- **複数の文脈単語を入力として、中心の単語を出力する**
- **Word2Vecの学習アルゴリズムの1つである**
- **Skip-gramより高速だが、低頻度語の学習はやや劣る**
- **文脈単語を平均化して中心語を予測する**
- **大規模データセットでの学習に適している**

❌ **不適切な選択肢（混同注意）**：

**1. 予測方向の誤り**：
- 「中心単語から周辺単語を予測する」→ ❌ 誤り（それは**Skip-gram**）
- 「中心単語を入力として文脈を出力する」→ ❌ 誤り（それは**Skip-gram**）

**2. 計算速度の誤り**：
- 「Skip-gramより計算コストが高い」→ ❌ 誤り（CBOWの方が**速い**）
- 「低頻度語の学習に最も適している」→ ❌ 誤り（**Skip-gram**が適している）

**3. 他手法との混同**：
- 「文脈を双方向から理解する」→ ❌ これは**BERT**の説明
- 「文書中の単語の重要度を計算する」→ ❌ これは**TF-IDF**の説明
- 「単語の出現頻度のみを記録する」→ ❌ これは**BoW**の説明
- 「連続するN個の単語パターンを分析」→ ❌ これは**N-Gram**の説明

**4. 構造の誤り**：
- 「Transformerを使用する」→ ❌ 誤り（浅いニューラルネットワーク）
- 「RNNで時系列を処理」→ ❌ 誤り（フィードフォワードNN）
- 「Self-Attentionを使用」→ ❌ 誤り（それは**Transformer/BERT**）

**5. 目的の誤り**：
- 「次元削減を目的とする」→ ❌ 誤り（単語の意味表現が目的）
- 「文書分類を行う」→ ❌ 誤り（単語ベクトルの学習が目的）

---

### Word2Vecの重要ポイント整理

**最も重要な特徴**：
1. **単語を連続値ベクトルに変換**（分散表現）
2. **意味的類似性をベクトル距離で表現**
3. **2つの学習方式**：CBOW（文脈→中心）、Skip-gram（中心→文脈）

**ベクトル演算の例**：
```
king - man + woman ≈ queen
東京 - 日本 + フランス ≈ パリ
```

**従来手法（One-Hot Encoding）との違い**：
| 項目 | One-Hot | Word2Vec |
|------|---------|----------|
| 次元 | 語彙数（数万〜） | 200〜300次元 |
| 類似性 | 表現不可 | **距離で表現** |
| スパース | 疎（ほぼ0） | **密（連続値）** |
| 意味 | なし | **あり** |

**試験で狙われる混同**：
- CBOW vs Skip-gram：予測方向、速度、低頻度語
- Word2Vec vs BERT：静的 vs 文脈依存、浅いNN vs Transformer
- Word2Vec vs TF-IDF：意味ベクトル vs 重要度スコア

---

### Word2Vecの実例

**実例：類似単語の検索**

```
【学習データ】
大規模コーパス（Wikipedia等）でWord2Vecを学習

【単語ベクトル】
「犬」 = [0.2, -0.5, 0.8, ..., 0.3]（300次元）
「猫」 = [0.1, -0.4, 0.7, ..., 0.2]
「車」 = [-0.8, 0.3, -0.1, ..., 0.9]

【類似度計算（コサイン類似度）】
cos(犬, 猫) = 0.85（高い類似度 → 意味が近い）
cos(犬, 車) = 0.12（低い類似度 → 意味が遠い）

【類似単語検索】
「犬」に最も近い単語:
1. 猫（0.85）
2. ペット（0.78）
3. 動物（0.72）
```

**実例：ベクトル演算**

```
【アナロジー推論】
king - man + woman = ?

ベクトル計算:
[0.5, 0.3, ...] - [0.2, 0.1, ...] + [-0.1, 0.4, ...] 
= [0.2, 0.6, ...]

最も近い単語を検索:
→ queen（0.92）

【他の例】
東京 - 日本 + フランス ≈ パリ
大きい - 小さい + 長い ≈ 短い
```

---

## BERT（Bidirectional Encoder Representations from Transformers）★試験頻出

### 定義
**BERT**は、Googleが2018年に発表した双方向Transformer Encoderを用いた事前学習言語モデル。文脈を考慮した単語埋め込みを生成し、ファインチューニングで様々なNLPタスクに適用できる。

### 要点
- **双方向（Bidirectional）**：文脈を左右両方向から理解（従来の一方向モデルと異なる）
- **Encoder専用**：TransformerのEncoder部分のみを使用（Decoder不使用）
- **事前学習＋ファインチューニング**：大規模コーパスで事前学習→タスク別に微調整
- **文脈依存表現**：同じ単語でも文脈で異なるベクトル表現を獲得

### BERTの主要特徴

#### 1. アーキテクチャ
- **Transformer Encoderのみ**を使用（Decoder不使用）
- **Self-Attention**で文中の全単語間の関係を学習
- **多層構造**：BERT-Base（12層）、BERT-Large（24層）

```
入力: [CLS] 私は銀行に行く [SEP]
       ↓
Transformer Encoder × N層
       ↓
各単語の文脈依存ベクトル
```

#### 2. 双方向性（Bidirectional）の重要性

**従来の一方向モデル（GPT等）**：
```
私 → は → 銀 → 行 → に → 行 → く
   （左から右のみ）
```

**BERTの双方向処理**：
```
     ←←←←←←←←←
私 は 銀 行 に 行 く
     →→→→→→→→→
  （左右両方向から文脈理解）
```

**利点**：
- 「銀行」という単語を理解する際、前後の「私は」「に行く」の両方を参照
- 文脈の深い理解が可能
- 曖昧性解消に優れる（"bank"が金融機関か河岸かを判断）

#### 3. 事前学習タスク

**タスク1: Masked Language Model（MLM）**
- 入力文の一部をマスク（[MASK]）して、元の単語を予測
- 双方向の文脈から予測学習

```
入力: 私は [MASK] に行く
正解: 銀行

前後の文脈「私は」「に行く」から「銀行」を予測
```

**タスク2: Next Sentence Prediction（NSP）**
- 2つの文が連続しているかを判定
- 文間の関係理解を学習

```
文A: 私は東京に住んでいます。
文B: 東京は日本の首都です。
→ ✅ 連続している（関連あり）

文A: 私は東京に住んでいます。
文B: 猫は動物です。
→ ❌ 連続していない（無関係）
```

#### 4. ファインチューニング
事前学習後、タスク別に微調整することで様々なNLPタスクに対応

| タスク | ファインチューニング方法 | 例 |
|--------|--------------------------|-----|
| **文書分類** | [CLS]トークンの出力を分類 | 感情分析・スパム判定 |
| **固有表現抽出** | 各トークンをタグ分類 | 人名・地名抽出 |
| **質問応答** | 回答の開始・終了位置を予測 | SQuAD（質問応答データセット） |
| **文ペア分類** | 2文の関係を判定 | 含意関係・類似度 |

### BERTと他モデルの比較（試験頻出）

| モデル | アーキテクチャ | 方向性 | 主用途 | 事前学習タスク |
|--------|----------------|--------|--------|----------------|
| **BERT** | Transformer **Encoder** | **双方向** | **文章理解** | MLM + NSP |
| **GPT** | Transformer **Decoder** | **一方向**（左→右） | **文章生成** | 次単語予測 |
| **T5** | Encoder-Decoder | 双方向 | Text-to-Text | Span予測 |
| **ELMo** | Bi-LSTM | 双方向 | 埋め込み | 言語モデル |
| **Word2Vec** | 浅いNN | — | 静的埋め込み | Skip-gram/CBOW |

**重要な対比**：
- **BERT vs GPT**：Encoder（理解） vs Decoder（生成）、双方向 vs 一方向
- **BERT vs Word2Vec**：文脈依存 vs 文脈非依存、動的 vs 静的埋め込み
- **BERT vs ELMo**：Transformer vs LSTM、Self-Attention vs 逐次処理

### 試験での問われ方

#### 典型問題：「BERTについて、最も不適切な選択肢を1つ選べ」

✅ **適切な選択肢**（正しい説明）：
1. **双方向Transformerで文脈を両方向から理解する** → ✅ 適切
2. **TransformerのEncoder部分のみを使用している** → ✅ 適切
3. **Masked Language Modelで事前学習を行う** → ✅ 適切
4. **ファインチューニングで多様なNLPタスクに対応できる** → ✅ 適切
5. **文脈依存の単語表現を獲得する** → ✅ 適切
6. **同じ単語でも文脈で異なるベクトルを持つ** → ✅ 適切

❌ **不適切な選択肢**（誤った説明、試験で狙われる）：
1. **「TransformerのDecoder部分を使用している」** → ❌ **最も不適切**（EncoderのみDecoder不使用）
2. **「一方向（左→右）のみで処理する」** → ❌ 不適切（双方向が特徴）
3. **「文章生成タスクに特化したモデル」** → ❌ 不適切（理解タスク、生成はGPT）
4. **「RNN/LSTMベースのアーキテクチャ」** → ❌ 不適切（Transformerベース）
5. **「事前学習を行わず各タスクをゼロから学習」** → ❌ 不適切（事前学習が核心）
6. **「文脈に依存しない静的な単語埋め込み」** → ❌ 不適切（文脈依存、動的）

#### 引っ掛けポイント

| ひっかけ | 正しい理解 | 混同されるモデル |
|----------|------------|------------------|
| ❌ Decoder使用 | ✅ **Encoder専用** | GPT（Decoder使用） |
| ❌ 一方向処理 | ✅ **双方向処理** | GPT（一方向） |
| ❌ 文章生成 | ✅ **文章理解** | GPT（生成特化） |
| ❌ RNN/LSTM | ✅ **Transformer** | ELMo（LSTM使用） |
| ❌ 静的埋め込み | ✅ **文脈依存** | Word2Vec（静的） |

#### 選択肢の判別ポイント

**問題例**：「BERTについて、最も不適切な選択肢を1つ選べ。」

**選択肢A**: 双方向Transformerで文脈を理解する → ✅ 適切
**選択肢B**: Masked Language Modelで事前学習する → ✅ 適切
**選択肢C**: **TransformerのDecoderのみを使用した自己回帰モデルである** → ❌ **最も不適切**
**選択肢D**: ファインチューニングで様々なNLPタスクに対応できる → ✅ 適切

**正解**: **C（最も不適切）**

**理由**：
- BERTは **Encoderのみ** 使用（Decoder不使用）
- Decoderを使用するのは **GPT**
- BERTは双方向、GPTは自己回帰（一方向）

### 実例

#### 例1：文脈依存の単語表現
```
文1: "私は銀行に預金する"
→ BERTの「銀行」ベクトル：金融機関の意味を強く反映

文2: "川の銀行に座る"
→ BERTの「銀行」ベクトル：河岸の意味を強く反映

同じ「銀行」でも文脈で異なるベクトル表現
（Word2Vecでは常に同じベクトル）
```

#### 例2：Masked Language Modelの学習
```
入力文: "私は東京に [MASK] ました"

BERTの予測候補:
1. 行き（最も可能性高）
2. 来
3. 住み
...

前後の文脈「私は」「東京に」「ました」から予測
```

#### 例3：ファインチューニングの応用
```
【感情分析タスク】
事前学習BERT → [感情データで微調整] → 感情分類器

入力: "この映画は素晴らしい"
出力: ポジティブ（0.95）

入力: "つまらない映画だ"
出力: ネガティブ（0.92）
```

### 補足

#### 実務上の利点
1. **転移学習**：少量のタスク固有データで高精度を達成
2. **多言語対応**：多言語BERTで100以上の言語をサポート
3. **ドメイン適応**：特定分野（医療・法律等）のBERTを構築可能
4. **解釈可能性**：Attentionの可視化で判断根拠を確認

#### BERTの派生モデル
- **RoBERTa**：NSPタスク削除、より長い事前学習
- **ALBERT**：パラメータ共有で軽量化
- **DistilBERT**：知識蒸留で高速化（精度維持）
- **日本語BERT**：日本語コーパスで事前学習

#### 実務での注意点
- **計算コスト**：学習・推論に大量のGPUメモリ必要
- **データ要件**：ファインチューニングにもある程度のデータが必要
- **モデル選択**：タスクに応じてBase/Large、多言語/単一言語を選択

---

## GLUE（General Language Understanding Evaluation）★試験頻出

### 定義
**GLUE**は、自然言語理解タスクを評価するための標準ベンチマーク。9つの多様なNLPタスクで構成され、モデルの汎用的な言語理解能力を測定する。2018年に公開され、BERT等の事前学習モデルの評価に広く使用される。

### 要点
- **9つのタスク**で構成される総合ベンチマーク
- **文章理解**（分類・推論・類似度等）に特化
- BERTやGPTなどの事前学習モデルの性能評価に使用
- 人間の性能を超えるモデルも登場（BERT、GPT-3等）

### GLUEに含まれる9つのタスク

| タスク名 | タスク内容 | データセット | 例 |
|----------|------------|--------------|-----|
| **CoLA** | 文法性判定 | 文が文法的に正しいか | "The cat sleep." → 不正 |
| **SST-2** | 感情分析 | 映画レビューの感情（ポジ/ネガ） | "Great movie!" → ポジティブ |
| **MRPC** | パラフレーズ判定 | 2文が意味的に同じか | 文1と文2が同義か |
| **STS-B** | 意味的類似度 | 2文の類似度スコア（0-5） | 類似度3.5 |
| **QQP** | 質問ペアの同値性 | 2つの質問が同じ意図か | "How to cook rice?" vs "Rice cooking method?" → 同値 |
| **MNLI** | 自然言語推論 | 前提文から仮説文を推論（含意/矛盾/中立） | 前提→仮説の関係判定 |
| **QNLI** | 質問応答推論 | 文章が質問の答えを含むか | 質問+文→答えあり/なし |
| **RTE** | 含意関係認識 | 文1が文2を含意するか | 前提→結論の成立判定 |
| **WNLI** | Winograd Schema | 代名詞の指示対象を推論 | "The trophy didn't fit in the suitcase because **it** was too big." → "it"=trophy? |

### GLUEタスクの詳細

#### 1. CoLA（Corpus of Linguistic Acceptability）
**文法性判定**：文が英語として文法的に正しいかを判定

```
✅ 正しい例: "The cat is sleeping."
❌ 誤った例: "The cat sleep."（三人称単数の動詞形が誤り）
```

#### 2. SST-2（Stanford Sentiment Treebank）
**感情分析**：映画レビューの感情極性を判定（ポジティブ/ネガティブの2値分類）

```
入力: "This movie is fantastic!"
出力: ポジティブ

入力: "Boring and disappointing."
出力: ネガティブ
```

#### 3. MRPC（Microsoft Research Paraphrase Corpus）
**パラフレーズ判定**：2つの文が意味的に同じかを判定

```
文1: "The company will release a new product."
文2: "A new product will be launched by the company."
判定: ✅ パラフレーズ（意味が同じ）

文1: "I like apples."
文2: "I hate oranges."
判定: ❌ パラフレーズでない
```

#### 4. STS-B（Semantic Textual Similarity Benchmark）
**意味的類似度**：2文の類似度を0～5のスコアで評価（回帰タスク）

```
文1: "A dog is running in the park."
文2: "A puppy is playing in the garden."
類似度: 3.5（やや類似）

文1: "I love programming."
文2: "The weather is sunny."
類似度: 0.0（無関係）
```

#### 5. QQP（Quora Question Pairs）
**質問ペアの同値性**：2つの質問が同じ意図を持つかを判定

```
質問1: "How can I learn Python?"
質問2: "What's the best way to study Python?"
判定: ✅ 同じ意図

質問1: "How to cook pasta?"
質問2: "What is quantum physics?"
判定: ❌ 異なる意図
```

#### 6. MNLI（Multi-Genre Natural Language Inference）
**自然言語推論**：前提文から仮説文の真偽を推論（含意/矛盾/中立の3値分類）

```
前提: "All cats are animals."
仮説: "Some animals are cats."
判定: 含意（前提が真なら仮説も真）

前提: "John is taller than Mary."
仮説: "Mary is taller than John."
判定: 矛盾

前提: "I went to Tokyo."
仮説: "I like sushi."
判定: 中立（関係不明）
```

#### 7. QNLI（Question Natural Language Inference）
**質問応答推論**：文章が質問の答えを含むかを判定

```
質問: "Who invented the telephone?"
文章: "Alexander Graham Bell invented the telephone in 1876."
判定: ✅ 答えを含む

質問: "What is the capital of France?"
文章: "Paris is a beautiful city with many museums."
判定: ✅ 答えを含む（Paris=首都と推論）

質問: "When was Einstein born?"
文章: "Einstein was a famous physicist."
判定: ❌ 答えを含まない
```

#### 8. RTE（Recognizing Textual Entailment）
**含意関係認識**：文1（前提）が文2（仮説）を含意するかを判定

```
前提: "The company hired 100 new employees."
仮説: "The company expanded its workforce."
判定: ✅ 含意（雇用増→労働力拡大）

前提: "It rained yesterday."
仮説: "The weather was sunny yesterday."
判定: ❌ 含意しない（矛盾）
```

#### 9. WNLI（Winograd Natural Language Inference）
**Winograd Schema Challenge**：代名詞の指示対象を推論

```
文: "The trophy didn't fit in the suitcase because it was too big."
質問: "it"は何を指すか？
→ trophy（トロフィーが大きすぎてスーツケースに入らない）

文: "The trophy didn't fit in the suitcase because it was too small."
質問: "it"は何を指すか？
→ suitcase（スーツケースが小さすぎてトロフィーが入らない）
```

### GLUEに**含まれない**タスク（試験頻出のひっかけ）

以下は**GLUEベンチマークに含まれないタスク**で、試験で「最も不適切な選択肢」として出題されやすい：

❌ **GLUEに含まれないタスク**：

1. **機械翻訳（Machine Translation）**
   - 言語間の翻訳タスク
   - GLUEは**単一言語（英語）**の理解タスクのみ
   - 翻訳は別のベンチマーク（WMT等）で評価

2. **画像認識（Image Classification）**
   - コンピュータビジョンのタスク
   - GLUEは**テキストのみ**扱う
   - 画像はImageNet等で評価

3. **音声認識（Speech Recognition, ASR）**
   - 音声をテキストに変換
   - GLUEは**テキスト入力のみ**
   - 音声はLibriSpeech等で評価

4. **テキスト生成（Text Generation）**
   - 文章の自動生成タスク
   - GLUEは**理解タスク**のみ（生成含まず）
   - 生成はGPT等で別途評価

5. **固有表現抽出（Named Entity Recognition, NER）**
   - 人名・地名等の識別
   - GLUEには含まれない（系列ラベリングタスク）
   - CoNLL等の別ベンチマークで評価

6. **要約（Summarization）**
   - 文書の要約生成
   - 生成タスクなのでGLUEに含まれない

7. **物体検出（Object Detection）**
   - 画像中の物体位置と種類を特定
   - コンピュータビジョンタスク、GLUEは対象外

### GLUEタスクの分類整理

| カテゴリ | GLUEに含まれるタスク | GLUEに含まれないタスク |
|----------|----------------------|------------------------|
| **文章理解** | CoLA、SST-2、MNLI、QNLI、RTE、WNLI | — |
| **文ペア判定** | MRPC、QQP、STS-B | — |
| **生成** | — | 機械翻訳、要約、文章生成 |
| **系列ラベリング** | — | 固有表現抽出（NER）、品詞タグ付け |
| **画像** | — | 画像認識、物体検出 |
| **音声** | — | 音声認識、音声合成 |

### 試験での問われ方

#### 典型問題：「GLUEのタスクとして、最も不適切な選択肢を1つ選べ」

**選択肢例**：
- A. 感情分析（SST-2） → ✅ 適切（GLUEに含まれる）
- B. パラフレーズ判定（MRPC） → ✅ 適切（GLUEに含まれる）
- C. **機械翻訳** → ❌ **最も不適切**（GLUEに含まれない）
- D. 自然言語推論（MNLI） → ✅ 適切（GLUEに含まれる）

**正解**: **C（最も不適切）**

**理由**：
- GLUEは**単一言語（英語）の理解タスク**のみ
- **機械翻訳**は言語間変換で、GLUEベンチマークに含まれない
- 翻訳はWMT（Workshop on Machine Translation）等で評価

#### 引っ掛けポイント

| ひっかけ選択肢 | 正しい理解 | 含まれる理由/含まれない理由 |
|----------------|------------|----------------------------|
| ✅ 感情分析 | GLUEに含まれる | SST-2タスク |
| ✅ 自然言語推論 | GLUEに含まれる | MNLI、RTE、WNLI |
| ✅ 文法性判定 | GLUEに含まれる | CoLA |
| ✅ 質問応答推論 | GLUEに含まれる | QNLI |
| ❌ **機械翻訳** | **GLUEに含まれない** | 言語間変換、理解タスクでない |
| ❌ **画像認識** | **GLUEに含まれない** | テキストでない |
| ❌ **音声認識** | **GLUEに含まれない** | テキストでない |
| ❌ **テキスト生成** | **GLUEに含まれない** | 生成タスク（理解タスクのみ） |
| ❌ **固有表現抽出** | **GLUEに含まれない** | 系列ラベリング、GLUEは分類中心 |

**重要な対比**：
- **理解タスク（GLUEに含まれる）**：分類・推論・類似度判定
- **生成タスク（GLUEに含まれない）**：翻訳・要約・文章生成
- **マルチモーダル（GLUEに含まれない）**：画像・音声を扱うタスク
- **系列ラベリング（GLUEに含まれない）**：NER・品詞タグ付け

### 実例

#### 例1：モデル評価でのGLUE使用
```
【BERTの評価】
GLUE総合スコア: 80.5
- CoLA: 60.5
- SST-2: 93.5
- MRPC: 88.9
- STS-B: 87.6
- QQP: 71.2
- MNLI: 84.6
- QNLI: 90.5
- RTE: 66.4
- WNLI: 65.1

→ 9タスクの平均で総合的な言語理解能力を評価
```

#### 例2：GLUEと他ベンチマークの使い分け
```
【評価対象】
テキスト理解: GLUE（SST-2、MNLI等）
機械翻訳: WMT（Workshop on Machine Translation）
質問応答: SQuAD（Stanford Question Answering Dataset）
画像認識: ImageNet
音声認識: LibriSpeech

→ タスクごとに適切なベンチマークを使用
```

### 補足

#### SuperGLUE（発展版）
- GLUEが人間性能を超えたため、**より難しいタスク**を集めた後継ベンチマーク
- より複雑な推論・常識推論を要求
- BoolQ、MultiRC、ReCoRD等の新タスクを含む

#### 実務での利用
- **事前学習モデルの選定**：GLUE性能で汎用性を比較
- **ファインチューニングの効果測定**：タスク別の精度向上を確認
- **研究論文での標準評価**：論文で必ず報告されるベンチマーク

#### 日本語の類似ベンチマーク
- **JGLUE（Japanese GLUE）**：日本語版のGLUEベンチマーク
- 日本語の文章理解タスクで構成

### 関連トピック
- [Transformer](../06_deep_learning/transformer.md)：BERTの基盤アーキテクチャ
- [RNN](../06_deep_learning/rnn.md)：系列データ処理、Seq2Seqで機械翻訳
- [音声処理](speech_processing.md)：音声認識（ASR）と自然言語処理の連携
