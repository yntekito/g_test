# キーワード索引

このファイルは、G検定学習リポジトリ内の重要キーワードとその記述先ファイルへのリンクを提供します。

## 使い方
- キーワードで検索し、詳細が記述されているファイルを見つける
- 類似概念の比較や混同しやすい用語の確認に活用

---

## AI歴史
- **第二次AIブーム**: 1980年代、エキスパートシステムの実用化で到来 → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)
- **エキスパートシステム（Expert System）**: 専門家の知識を取り込んだAIシステム、第二次ブームの中核技術 → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)
- **知識ベース（Knowledge Base）**: IF-THENルールの集合 → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)
- **推論エンジン（Inference Engine）**: ルールを適用して推論を実行 → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)
- **IF-THENルール（プロダクションルール）**: 「IF 条件 THEN 結論」形式の知識表現 → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)
- **前向き推論（Forward Chaining）**: データから結論へ推論 → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)
- **後ろ向き推論（Backward Chaining）**: 仮説から証拠を探索 → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)
- **MYCIN（マイシン）**: 医療診断システム、最も有名なエキスパートシステム → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)
- **DENDRAL（デンドラル）**: 化学分析システム、世界初の実用的エキスパートシステム → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)
- **XCON（R1）**: コンピュータ設計のエキスパートシステム、商業的成功 → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)
- **知識獲得のボトルネック**: 知識の形式化が困難でコスト大、第二次ブーム衰退の主因 → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)
- **常識の欠如**: エキスパートシステムが限定領域外の推論ができない問題 → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)
- **第五世代コンピュータプロジェクト**: 日本の国家プロジェクト（1982～1992） → [02_ai_history/second_ai_boom.md](../02_ai_history/second_ai_boom.md)

## AI定義・哲学
- **プランニング（STRIPS）**: 前提条件・行動・結果で行動計画を記述する古典的手法 → [03_ai_definition/planning.md](../03_ai_definition/planning.md)
- **幅優先探索（BFS）**: 同階層のノードを全探索後に次階層へ進む探索手法 → [03_ai_definition/search_algorithms.md](../03_ai_definition/search_algorithms.md)
- **深さ優先探索（DFS）**: 深さ方向に優先的に進む探索手法 → [03_ai_definition/search_algorithms.md](../03_ai_definition/search_algorithms.md)
- **A*探索**: ヒューリスティック関数を用いた効率的探索 → [03_ai_definition/search_algorithms.md](../03_ai_definition/search_algorithms.md)
- **状態空間探索**: 問題を状態と遷移で表現し解を探索 → [03_ai_definition/search_algorithms.md](../03_ai_definition/search_algorithms.md)
- **記号的AI**: ルールベースの推論・探索（探索アルゴリズム、プランニング等） → [03_ai_definition/search_algorithms.md](../03_ai_definition/search_algorithms.md)
- **サブシンボリックAI**: パターン認識・統計的学習（深層学習、機械学習） → [03_ai_definition/search_algorithms.md](../03_ai_definition/search_algorithms.md)
- **ミニマックス法（Minimax）**: 自分の最大利益と相手の最小利益を追求するゲーム探索手法 → [03_ai_definition/search_algorithms.md](../03_ai_definition/search_algorithms.md)
- **αβ枝刈り（Alpha-Beta Pruning）**: ミニマックス法の効率化、不要な枝を探索しない → [03_ai_definition/search_algorithms.md](../03_ai_definition/search_algorithms.md)
- **ゲーム木（Game Tree）**: ゲームの可能な手順を木構造で表現 → [03_ai_definition/search_algorithms.md](../03_ai_definition/search_algorithms.md)
- **ゼロサムゲーム（Zero-sum Game）**: 一方の利益が他方の損失となるゲーム → [03_ai_definition/search_algorithms.md](../03_ai_definition/search_algorithms.md)
- **評価関数（Evaluation Function）**: 盤面の有利・不利を数値化する関数 → [03_ai_definition/search_algorithms.md](../03_ai_definition/search_algorithms.md)
- **モンテカルロ木探索（MCTS）**: ランダムシミュレーションで評価するゲーム探索（AlphaGo） → [03_ai_definition/search_algorithms.md](../03_ai_definition/search_algorithms.md)
- **フレーム問題（Frame Problem）**: 行動の前後で何が変化し何が変化しないかを列挙する計算量爆発の問題 → [03_ai_definition/frame_problem.md](../03_ai_definition/frame_problem.md)
- **常識の問題（Common Sense Problem）**: 人間の一般常識をAIに習得させることの困難さ → [03_ai_definition/frame_problem.md](../03_ai_definition/frame_problem.md)
- **シンボルグラウンディング問題（Symbol Grounding Problem）**: 記号と現実世界の意味の対応付けの困難さ → [03_ai_definition/frame_problem.md](../03_ai_definition/frame_problem.md)
- **資格問題（Qualification Problem）**: 行動の前提条件を全て列挙できない問題 → [03_ai_definition/frame_problem.md](../03_ai_definition/frame_problem.md)
- **Cycプロジェクト**: 人間の常識を論理式で記述する試み（1984年～） → [03_ai_definition/frame_problem.md](../03_ai_definition/frame_problem.md)

## 機械学習
- **損失関数（Loss Function）**: モデルの予測値と正解データとの差分を定量化する関数、学習時に最小化 → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **コスト関数（Cost Function）**: 全訓練データの損失の平均（損失関数とほぼ同義） → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **目的関数（Objective Function）**: 最適化の対象となる関数（損失+正則化項） → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **MSE（Mean Squared Error）**: 回帰タスクの標準的損失関数、二乗誤差の平均 → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **MAE（Mean Absolute Error）**: 外れ値に頑健な回帰誤差指標（誤差の絶対値） → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **交差エントロピー（Cross Entropy）**: 分類タスクの標準的損失関数、確率分布間の距離 → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **Huber損失**: MSEとMAEのハイブリッド、外れ値に頑健かつ微分可能 → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **ヒンジ損失（Hinge Loss）**: SVMで使用されるマージン最大化の損失関数 → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **サポートベクターマシン（SVM）**: カーネルトリックで高次元写像し、マージン最大化で線形分離 → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **カーネルトリック**: 明示的な高次元写像なしに内積を計算する技法 → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **カーネル関数**: RBF、多項式、線形カーネル等でデータを高次元空間に写像 → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **サポートベクター**: 分離超平面に最も近いデータ点 → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **マージン最大化**: クラス間の余白を最大化して汎化性能を向上 → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **マルチタスク学習（Multi-Task Learning）**: 複数の関連タスクを同時に学習し、タスク間で知識を共有 → [05_machine_learning/multi_task_learning.md](../05_machine_learning/multi_task_learning.md)
- **共有表現（Shared Representation）**: マルチタスク学習でタスク間で共有される特徴表現 → [05_machine_learning/multi_task_learning.md](../05_machine_learning/multi_task_learning.md)
- **ハードパラメータ共有（Hard Parameter Sharing）**: 層を物理的に共有するマルチタスク学習の基本構造 → [05_machine_learning/multi_task_learning.md](../05_machine_learning/multi_task_learning.md)
- **ソフトパラメータ共有（Soft Parameter Sharing）**: パラメータ間の距離を制約して共有 → [05_machine_learning/multi_task_learning.md](../05_machine_learning/multi_task_learning.md)
- **負の転移（Negative Transfer）**: タスク間の干渉による性能低下 → [05_machine_learning/multi_task_learning.md](../05_machine_learning/multi_task_learning.md)
- **タスクバランス（Task Balancing）**: 各タスクの損失の重み調整 → [05_machine_learning/multi_task_learning.md](../05_machine_learning/multi_task_learning.md)
- **Actor-Critic**: Actorが行動選択、Criticが行動評価する強化学習手法 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **Actor（行動者）**: 方策を学習し行動を選択 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **Critic（評価者）**: 価値関数を学習し行動を評価 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **強化学習**: 試行錯誤を通じて報酬を最大化する方策を学習 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **Q学習**: 価値ベースの強化学習、Q関数を学習 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **DQN（Deep Q-Network）**: Q学習にニューラルネットワークを適用した深層強化学習 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **デュエリングネットワーク（Dueling Network）**: Q値を状態価値V(s)とアドバンテージA(s,a)に分解して学習するDQN改良版 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **状態価値V(s)**: その状態にいることの価値（行動に依存しない） → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **アドバンテージA(s,a)**: 特定の行動の相対的な良さ（平均からの差） → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **Policy Gradient**: 方策ベースの強化学習、方策を直接学習 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **TD誤差（Temporal Difference Error）**: Criticが計算する予測誤差 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **ε-greedy方策**: 確率εでランダム行動、確率1-εで最良行動を選択する探索戦略 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **探索（Exploration）**: 未知の行動を試して新しい情報を得る → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **活用（Exploitation）**: 現在の知識で最良の行動を選ぶ → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **ε-減衰（ε-decay）**: 学習進行に応じてεを減少させる手法 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **探索と活用のトレードオフ**: 探索しすぎると報酬低下、活用しすぎると局所解 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **Sim-to-Real転移**: シミュレータで学習した方策を実環境に転移する技術 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **ドメインランダマイゼーション（Domain Randomization）**: シミュレータのパラメータをランダム化してロバスト方策を学習 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **シミュレーションギャップ（Reality Gap）**: シミュレータと実環境の差異 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **ドメイン適応（Domain Adaptation）**: 実データで方策を微調整する転移手法 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **システム同定（System Identification）**: 実環境の物理パラメータを推定してシミュレータを調整 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **マルチエージェント強化学習（MARL）**: 複数エージェントが協調的・競争的関係を考慮しながら学習する強化学習 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **協調的MARL（Cooperative MARL）**: 全エージェントが共通目標を持ち協力して学習 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **競争的MARL（Competitive MARL）**: エージェント間で利益が対立、ゼロサムゲーム等 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **混合MARL（Mixed MARL）**: 協調と競争が混在（チーム内協調、チーム間競争） → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **自己対戦（Self-Play）**: 自分自身のコピーと対戦して学習する競争的MARLの手法 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **AlphaGo**: GoogleのDeepMindが開発した囲碁AI、マルチエージェント強化学習の代表例 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **AlphaZero**: 囲碁・将棋・チェスでトップレベルに到達した汎用ゲームAI → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **OpenAI Five**: OpenAIのDota 2プロレベルAI（5 vs 5チーム戦、協調的MARL） → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **AlphaStar**: DeepMindのStarCraft IIプロレベルAI、長期戦略学習 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **CTDE（Centralized Training, Decentralized Execution）**: 学習時は全情報利用、実行時は各エージェント独立判断 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **QMIX**: 個別Q関数を単調結合して共同行動価値を表現する協調的MARL手法 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **ナッシュ均衡（Nash Equilibrium）**: どのエージェントも戦略変更の動機がない状態 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **非定常性（Non-Stationarity）**: 他エージェントの学習により環境が動的変化する問題 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **クレジット割当問題（Credit Assignment）**: チーム報酬から個別貢献度を判定困難な問題 → [05_machine_learning/reinforcement_learning.md](../05_machine_learning/reinforcement_learning.md)
- **教師なし学習（Unsupervised Learning）**: 正解ラベルなしでデータの構造・パターンを発見 → [05_machine_learning/unsupervised_learning.md](../05_machine_learning/unsupervised_learning.md)
- **k-means法**: k個のクラスタ中心でデータをグループ化する基本的クラスタリング手法 → [05_machine_learning/unsupervised_learning.md](../05_machine_learning/unsupervised_learning.md)
- **クラスタリング（Clustering）**: データを類似性に基づいてグループ分け → [05_machine_learning/unsupervised_learning.md](../05_machine_learning/unsupervised_learning.md)
- **顧客セグメンテーション**: k-means法の代表的活用例、購買行動でグループ化 → [05_machine_learning/unsupervised_learning.md](../05_machine_learning/unsupervised_learning.md)
- **エルボー法**: クラスタ内分散の変化から最適なk値を決定 → [05_machine_learning/unsupervised_learning.md](../05_machine_learning/unsupervised_learning.md)
- **シルエット係数**: クラスタリングの質を評価する指標 → [05_machine_learning/unsupervised_learning.md](../05_machine_learning/unsupervised_learning.md)
- **階層的クラスタリング**: データを階層的に統合・分割、デンドログラム作成 → [05_machine_learning/unsupervised_learning.md](../05_machine_learning/unsupervised_learning.md)
- **次元削減（Dimensionality Reduction）**: 高次元データを低次元に圧縮 → [05_machine_learning/unsupervised_learning.md](../05_machine_learning/unsupervised_learning.md)
- **PCA（主成分分析）**: 分散を最大化する軸を見つける次元削減手法 → [05_machine_learning/unsupervised_learning.md](../05_machine_learning/unsupervised_learning.md)
- **RMSE（Root Mean Squared Error）**: 外れ値に敏感な回帰誤差指標（誤差を2乗） → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **MAE（Mean Absolute Error）**: 外れ値に頑健な回帰誤差指標（誤差の絶対値） → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **精度（Accuracy）**: 全予測のうち正解した割合。不均衡データでは注意 → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **適合率（Precision）**: Positiveと予測したもののうち実際にPositiveだった割合 → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **再現率（Recall）**: 実際のPositiveのうち正しく検出できた割合 → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **F1スコア**: 適合率と再現率の調和平均、バランス指標 → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **ROC曲線（Receiver Operating Characteristic Curve）**: 横軸FPR、縦軸TPRで分類性能を可視化、閾値非依存 → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **AUC（Area Under the Curve）**: ROC曲線下の面積、0.5～1.0で1.0に近いほど良い → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **TPR（真陽性率）**: 再現率と同じ、実際の陽性のうち正しく検出した割合 → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **FPR（偽陽性率）**: 実際の陰性を誤って陽性と判定する率 → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **PR曲線（Precision-Recall Curve）**: 横軸再現率、縦軸適合率、不均衡データ向き → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **R²（決定係数）**: モデルがデータ分散を説明する割合 → [05_machine_learning/evaluation_metrics.md](../05_machine_learning/evaluation_metrics.md)
- **アンサンブル学習**: 複数モデルを組み合わせて精度向上する手法の総称 → [05_machine_learning/ensemble_learning.md](../05_machine_learning/ensemble_learning.md)
- **バギング（Bagging）**: 並列訓練で分散低減、ブートストラップサンプリング、アンサンブル学習の一種 → [05_machine_learning/ensemble_learning.md](../05_machine_learning/ensemble_learning.md)
- **ブースティング（Boosting）**: 逐次訓練でバイアス低減、弱学習器を改善、アンサンブル学習の一種 → [05_machine_learning/ensemble_learning.md](../05_machine_learning/ensemble_learning.md)
- **ランダムフォレスト**: バギングと決定木を組み合わせた代表的アンサンブル手法 → [05_machine_learning/ensemble_learning.md](../05_machine_learning/ensemble_learning.md)
- **XGBoost**: 勾配ブースティングの高速実装、Kaggleで頻用 → [05_machine_learning/ensemble_learning.md](../05_machine_learning/ensemble_learning.md)
- **AdaBoost**: 最初のブースティング手法、誤分類重視 → [05_machine_learning/ensemble_learning.md](../05_machine_learning/ensemble_learning.md)
- **スタッキング（Stacking）**: 異種モデルをメタ学習器で統合、アンサンブル学習の一種 → [05_machine_learning/ensemble_learning.md](../05_machine_learning/ensemble_learning.md)
- **弱学習器（Weak Learner）**: 単独では精度が低いがランダムより良いモデル → [05_machine_learning/ensemble_learning.md](../05_machine_learning/ensemble_learning.md)
- **k-分割交差検証（k-fold Cross-Validation）**: データをk個に分割し各回1つをテスト・残りを訓練として繰り返し評価 → [05_machine_learning/cross_validation.md](../05_machine_learning/cross_validation.md)
- **ホールドアウト法**: データを1回だけ訓練・テストに分割する評価手法 → [05_machine_learning/cross_validation.md](../05_machine_learning/cross_validation.md)
- **LOOCV（Leave-One-Out Cross-Validation）**: k=データ数の交差検証、小規模データ向き → [05_machine_learning/cross_validation.md](../05_machine_learning/cross_validation.md)
- **層化k-分割（Stratified k-fold）**: 各foldでクラス比率を保持する交差検証 → [05_machine_learning/cross_validation.md](../05_machine_learning/cross_validation.md)
- **転移学習（Transfer Learning）**: 別タスクで学習した知識を新タスクに活用、少量データで高精度実現 → [05_machine_learning/transfer_learning.md](../05_machine_learning/transfer_learning.md)
- **ファインチューニング（Fine-tuning）**: 事前学習済みモデルの重みを新タスクで微調整する転移学習の代表的手法 → [05_machine_learning/transfer_learning.md](../05_machine_learning/transfer_learning.md)
- **事前学習（Pre-training）**: 大規模データで汎用的な特徴を学習、転移学習の基盤 → [05_machine_learning/transfer_learning.md](../05_machine_learning/transfer_learning.md)
- **ドメイン適応（Domain Adaptation）**: ソースとターゲットのデータ分布の違いに対処する手法 → [05_machine_learning/transfer_learning.md](../05_machine_learning/transfer_learning.md)
- **特徴抽出器（Feature Extractor）**: 事前学習モデルを固定して特徴量として利用 → [05_machine_learning/transfer_learning.md](../05_machine_learning/transfer_learning.md)
- **過学習（Overfitting）**: 訓練データに過度に適合し未知データへの性能が低下する現象 → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **未学習（Underfitting）**: モデルが単純すぎて訓練データすら学習できない状態 → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **汎化（Generalization）**: 未知データに対する予測性能 → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **汎化誤差（Generalization Error）**: テストデータでの誤差 → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **汎化ギャップ（Generalization Gap）**: 訓練誤差とテスト誤差の差 → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **バイアス-バリアンストレードオフ**: モデルの単純さと複雑さのバランス → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **ダブルディセント（Double Descent）**: モデルサイズ増加で性能が一度悪化後に再向上する現象 → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **補間閾値（Interpolation Threshold）**: ダブルディセントで性能が最悪になる点、パラメータ数≈データ数 → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **正則化（Regularization）**: 過学習を防ぐための制約、L1/L2正則化等 → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **L1正則化（Lasso）**: 重みの絶対値の和で制約、スパース性を促進 → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **L2正則化（Ridge）**: 重みの2乗和で制約、重みを小さく保つ → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **Dropout**: 訓練時にランダムにニューロンを無効化して過学習を防ぐ → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **Early Stopping**: 検証誤差が悪化し始めたら学習を停止 → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **データ拡張（Data Augmentation）**: 訓練データを人工的に増やして過学習を防ぐ → [05_machine_learning/overfitting_underfitting.md](../05_machine_learning/overfitting_underfitting.md)
- **ノーフリーランチの定理（No Free Lunch Theorem）**: すべての問題に対して万能な最強のアルゴリズムは存在しない、問題に応じた選択が必須 → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **NFL定理（No Free Lunch Theorem）**: すべての最適化問題の平均性能はどのアルゴリズムでも等しい（Wolpert, 1997） → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **帰納的バイアス（Inductive Bias）**: 各アルゴリズムが持つ問題への仮定（線形性、局所性等）、NFL定理の前提 → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **万能近似定理（Universal Approximation Theorem）**: ニューラルネットは任意の関数を近似可能、NFL定理と矛盾しない → [05_machine_learning/supervised_learning.md](../05_machine_learning/supervised_learning.md)
- **次元の呪い（Curse of Dimensionality）**: 次元増加でデータが疎になり必要データ量が指数増加、過学習リスク上昇する現象 → [05_machine_learning/feature_engineering.md](../05_machine_learning/feature_engineering.md)
- **データの疎密性（Sparsity）**: 高次元空間でデータ点が疎に分布する状態、次元の呪いの主要問題 → [05_machine_learning/feature_engineering.md](../05_machine_learning/feature_engineering.md)
- **距離の均一化**: 高次元で全ての点の距離が等しく見える現象、k-NN等の距離ベースアルゴリズムが機能不全 → [05_machine_learning/feature_engineering.md](../05_machine_learning/feature_engineering.md)
- **カバーの定理（Cover's Theorem）**: 高次元空間では線形分離可能性が向上する定理、次元の呪いの逆説的利点 → [05_machine_learning/feature_engineering.md](../05_machine_learning/feature_engineering.md)
- **特徴選択（Feature Selection）**: 既存特徴量から重要なものを選択する手法、次元の呪いへの対策 → [05_machine_learning/feature_engineering.md](../05_machine_learning/feature_engineering.md)
- **特徴量エンジニアリング（Feature Engineering）**: 生データから有用な特徴量を生成・選択・抽出・変換するプロセス → [05_machine_learning/feature_engineering.md](../05_machine_learning/feature_engineering.md)
- **特徴量生成（Feature Generation）**: 既存データから新しい特徴量を作成する処理（組み合わせ、多項式等） → [05_machine_learning/feature_engineering.md](../05_machine_learning/feature_engineering.md)
- **特徴量抽出（Feature Extraction）**: 生データから有用な特徴を計算・変換して抽出する処理、PCA・オートエンコーダ等 → [05_machine_learning/feature_engineering.md](../05_machine_learning/feature_engineering.md)

## 法律・倫理
- **個人情報保護法**: 個人情報の適正な取扱いを定める法律（2003年制定、2020年・2022年改正） → [09_law_ethics/personal_data_protection.md](../09_law_ethics/personal_data_protection.md)
- **仮名加工情報**: 特定個人を識別できないよう加工、内部分析用、第三者提供原則禁止 → [09_law_ethics/personal_data_protection.md](../09_law_ethics/personal_data_protection.md)
- **匿名加工情報**: 復元不可能に加工、第三者提供可能、本人同意不要 → [09_law_ethics/personal_data_protection.md](../09_law_ethics/personal_data_protection.md)
- **個人識別符号**: マイナンバー、運転免許証番号、指紋、顔認証データ等 → [09_law_ethics/personal_data_protection.md](../09_law_ethics/personal_data_protection.md)
- **要配慮個人情報**: 人種、信条、病歴、犯罪歴等、差別・偏見の原因となる情報 → [09_law_ethics/personal_data_protection.md](../09_law_ethics/personal_data_protection.md)
- **第三者提供**: 仮名加工情報は原則禁止、匿名加工情報は可能 → [09_law_ethics/personal_data_protection.md](../09_law_ethics/personal_data_protection.md)
- **アルゴリズムバイアス（Algorithmic Bias）**: AIが特定属性で不公平判断、訓練データの偏りが主原因 → [09_law_ethics/bias_and_fairness.md](../09_law_ethics/bias_and_fairness.md)
- **訓練データのバイアス**: 過去の差別的慣行や偏った収集方法がデータに反映 → [09_law_ethics/bias_and_fairness.md](../09_law_ethics/bias_and_fairness.md)
- **サンプリングバイアス**: データ収集時の偏り（特定集団の過少/過剰代表） → [09_law_ethics/bias_and_fairness.md](../09_law_ethics/bias_and_fairness.md)
- **プロキシ変数（代理変数）**: 保護属性と相関する変数（郵便番号→人種等）で間接差別 → [09_law_ethics/bias_and_fairness.md](../09_law_ethics/bias_and_fairness.md)
- **統計的公平性（Demographic Parity）**: 各グループで陽性予測率が等しい → [09_law_ethics/bias_and_fairness.md](../09_law_ethics/bias_and_fairness.md)
- **機会の平等（Equal Opportunity）**: 真陽性率が各グループで等しい → [09_law_ethics/bias_and_fairness.md](../09_law_ethics/bias_and_fairness.md)
- **個人的公平性（Individual Fairness）**: 類似個人に類似判断 → [09_law_ethics/bias_and_fairness.md](../09_law_ethics/bias_and_fairness.md)
- **COMPAS問題**: 再犯予測システムが黒人に不利（ProPublica調査） → [09_law_ethics/bias_and_fairness.md](../09_law_ethics/bias_and_fairness.md)
- **公平性のトレードオフ**: 異なる公平性基準を同時に満たすのは困難 → [09_law_ethics/bias_and_fairness.md](../09_law_ethics/bias_and_fairness.md)
- **フィードバックループ**: バイアスある予測が偏ったデータ蓄積を生みバイアス増幅 → [09_law_ethics/bias_and_fairness.md](../09_law_ethics/bias_and_fairness.md)

## 深層学習
- **ニューラルネットワーク基礎**: 入力層・中間層・出力層、活性化関数 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **Softmax関数**: 多クラス分類で各クラスの確率を出力（合計1） → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **Sigmoid関数**: 2値分類の出力層で確率を計算（0～1） → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **ReLU（Rectified Linear Unit）**: 勾配消失問題を解決する活性化関数、中間層で最も使われる → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **勾配消失問題**: 深い層で勾配が0に近づき学習が進まない問題、ReLUで解決 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **Leaky ReLU（リーキー ReLU）**: 負の領域にも小さな勾配（通常0.01）を持たせDying ReLU対策 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **Dying ReLU問題**: ReLUで負の入力ニューロンが活性化しなくなる問題、Leaky ReLUで解決 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **PReLU（Parametric ReLU）**: 負の領域の係数を学習可能なパラメータにしたReLU改良版 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **ELU（Exponential Linear Unit）**: 負の領域で指数関数的に滑らか、ノイズに頑健 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **tanh**: -1～1の出力を持つ活性化関数、Sigmoidより0中心 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **プルーニング（Pruning）**: 不要な重み・ニューロンを削除してモデルを軽量化 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **構造化プルーニング**: ニューロン・フィルタ単位で削除、ハードウェア高速化可能 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **非構造化プルーニング**: 個別の重み単位で削除、高圧縮率 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **スパース性（Sparsity）**: ニューラルネットワークで0要素の割合 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **量子化（Quantization）**: パラメータ精度削減（FP32→INT8）でモデル圧縮、メモリ1/4、推論高速化 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **学習後量子化（PTQ）**: 学習済みモデルを直接量子化、再学習不要 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **量子化認識学習（QAT）**: 学習時に量子化を考慮、精度低下最小 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **知識蒸留（Knowledge Distillation）**: 教師モデルから生徒モデルへ知識転移、小モデルで高精度実現 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **教師モデル（Teacher Model）**: 大規模で高精度なモデル、知識蒸留の知識源 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **生徒モデル（Student Model）**: 小規模で軽量、教師から知識を学ぶ → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **ソフトターゲット（Soft Target）**: 教師の出力確率分布、クラス間関係を含む → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **温度パラメータ（Temperature）**: Softmaxの滑らかさ調整、蒸留で重要 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **クレジット割り当て問題**: どのパラメータが誤差に寄与しているかを明らかにする問題 → [06_deep_learning/backpropagation.md](../06_deep_learning/backpropagation.md)
- **誤差逆伝播法（Backpropagation）**: クレジット割り当て問題を解決する学習アルゴリズム → [06_deep_learning/backpropagation.md](../06_deep_learning/backpropagation.md)
- **連鎖律（Chain Rule）**: 合成関数の微分法則、誤差逆伝播で勾配を効率的に計算 → [06_deep_learning/backpropagation.md](../06_deep_learning/backpropagation.md)
- **構造的クレジット割り当て**: どの重みが誤差に寄与しているかを決定 → [06_deep_learning/backpropagation.md](../06_deep_learning/backpropagation.md)
- **時間的クレジット割り当て**: 時系列データでどの時点が影響したかを決定 → [06_deep_learning/backpropagation.md](../06_deep_learning/backpropagation.md)
- **CNN（畳み込みニューラルネットワーク）**: 画像認識に特化した深層学習モデル → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **ストライド（Stride）**: CNNでフィルタを移動させる幅、大きいほど出力サイズ小 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **パディング（Padding）**: CNNで入力画像の周囲に余白を追加する処理、出力サイズ維持 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **CNN出力サイズ計算**: $(入力 + 2 \times パディング - カーネル) / ストライド + 1$ → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **プーリング層**: CNNで特徴マップのダウンサンプリングを行う層 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **Dilated Convolution（拡張畳み込み）**: フィルタ要素間に穴を開けてパラメータ数を増やさず受容野を拡大 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **Dilation Rate（拡張率）**: Dilated Convolutionで穴の間隔を制御するパラメータ → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **受容野（Receptive Field）**: 各ニューロンが参照する入力領域、Dilated Convで効率的に拡大 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **ASPP（Atrous Spatial Pyramid Pooling）**: 異なるDilation Rateで並列畳み込み、DeepLabで使用 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **Atrous Convolution**: Dilated Convolutionの別名 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **Depthwise Separable Convolution**: 畳み込みを2段階（Depthwise + Pointwise）に分離し計算量を1/8～1/9に削減 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **Depthwise Convolution**: 各チャネルに対して独立に空間方向の畳み込みを実行 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **Pointwise Convolution**: 1×1畳み込みでチャネル間の情報を混合 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **MobileNet**: Depthwise Separable Convolutionを用いたモバイル向け軽量CNNモデル（Google, 2017） → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **Group Convolution**: チャネルをグループ分割して畳み込み、計算量削減 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **NAS（Neural Architecture Search）**: ネットワーク構造を自動設計、大量GPU並列探索で最適アーキテクチャを発見 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **AutoML（Automated Machine Learning）**: ハイパーパラメータ、アーキテクチャ、データ拡張を自動最適化、NASを含む広い概念 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **探索空間（Search Space）**: NASで探索する構造の候補範囲（層の種類・数・接続方法等） → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **強化学習ベースNAS**: RNNコントローラが構造を生成、精度を報酬として学習 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **進化的アルゴリズムNAS**: 構造を個体として進化、選択・交叉・突然変異で最適化 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **DARTS（Differentiable NAS）**: 構造探索を微分可能に定式化、探索時間を大幅短縮 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **重み共有（Weight Sharing）**: NASで候補間で重みを共有し計算コスト削減（ENAS） → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **EfficientNet**: Compound Scaling（複合スケーリング）で深さ・幅・解像度を同時最適化（Google, 2019） → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **Compound Scaling（複合スケーリング）**: ネットワークの深さ・幅・解像度を複合係数で統一的にスケーリング → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **Compound Coefficient（複合係数）**: EfficientNetでスケーリング度合いを制御するパラメータφ → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **NAS（Neural Architecture Search）**: ネットワーク構造を自動設計する手法、EfficientNet-B0に使用 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **Global Average Pooling（GAP）**: 各特徴マップを空間方向に平均化、全結合層の代替で解釈性向上 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **CAM（Class Activation Mapping）**: GAPを使い判断に寄与した画像領域を可視化する手法 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **Grad-CAM**: CAMの改良版、勾配情報を利用して汎用的に可視化 → [06_deep_learning/cnn.md](../06_deep_learning/cnn.md)
- **スケーリング法則（Scaling Laws）**: パラメーター数・データ量・計算量の増加に伴いAI性能が向上する法則、べき乗則に従う → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **べき乗則（Power Law）**: スケーリング法則でモデルサイズと性能の関係を表す式（$L \propto N^{-\alpha}$） → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **Chinchillaの法則**: モデルサイズとデータ量の最適配分比率を示す法則（DeepMind, 2022） → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **収穫逓減（Diminishing Returns）**: スケーリング法則で規模拡大の効果が徐々に小さくなる現象 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **創発能力（Emergent Abilities）**: モデル規模が一定を超えると新しい能力が突然出現する現象 → [06_deep_learning/neural_network_basics.md](../06_deep_learning/neural_network_basics.md)
- **RNN（Recurrent Neural Network）**: 系列データを処理する再帰型ニューラルネットワーク → [06_deep_learning/rnn.md](../06_deep_learning/rnn.md)
- **隠れ状態（Hidden State）**: RNNで前の時刻の情報を保持するベクトル → [06_deep_learning/rnn.md](../06_deep_learning/rnn.md)
- **勾配消失問題（RNN）**: RNNで勾配が減衰し長期依存性の学習が困難になる問題 → [06_deep_learning/rnn.md](../06_deep_learning/rnn.md)
- **BPTT（Backpropagation Through Time）**: RNNの学習手法、時間方向に展開して誤差逆伝播 → [06_deep_learning/rnn.md](../06_deep_learning/rnn.md)
- **Truncated BPTT（短縮版BPTT）**: 一定時間ステップのみ遡って勾配計算、計算コスト削減 → [06_deep_learning/rnn.md](../06_deep_learning/rnn.md)
- **勾配クリッピング（Gradient Clipping）**: 勾配爆発対策、勾配のノルムを閾値で切り詰め → [06_deep_learning/rnn.md](../06_deep_learning/rnn.md)
- **Encoder-Decoder**: 系列変換アーキテクチャ、機械翻訳に最適 → [06_deep_learning/rnn.md](../06_deep_learning/rnn.md)
- **Seq2Seq（Sequence-to-Sequence）**: 可変長入力→可変長出力の系列変換タスク → [06_deep_learning/rnn.md](../06_deep_learning/rnn.md)
- **LSTM（Long Short-Term Memory）**: セルステートと3つのゲートで長期依存を学習するRNN → [06_deep_learning/lstm_gru.md](../06_deep_learning/lstm_gru.md)
- **セルステート（Cell State）**: LSTMの記憶素子、長期記憶を保持する「コンベア」 → [06_deep_learning/lstm_gru.md](../06_deep_learning/lstm_gru.md)
- **メモリセル（Memory Cell）**: セルステートの別名、LSTMブロックの核心 → [06_deep_learning/lstm_gru.md](../06_deep_learning/lstm_gru.md)
- **入力ゲート（Input Gate）**: 新情報をセルステートに追加する量を制御 → [06_deep_learning/lstm_gru.md](../06_deep_learning/lstm_gru.md)
- **忘却ゲート（Forget Gate）**: 過去の情報をセルステートから削除する量を制御 → [06_deep_learning/lstm_gru.md](../06_deep_learning/lstm_gru.md)
- **出力ゲート（Output Gate）**: セルステートから隠れ状態への出力を制御 → [06_deep_learning/lstm_gru.md](../06_deep_learning/lstm_gru.md)
- **GRU（Gated Recurrent Unit）**: LSTMの簡略版、リセット・更新ゲートの2つで制御 → [06_deep_learning/lstm_gru.md](../06_deep_learning/lstm_gru.md)
- **リセットゲート（Reset Gate）**: GRUで過去情報のリセット度を制御 → [06_deep_learning/lstm_gru.md](../06_deep_learning/lstm_gru.md)
- **更新ゲート（Update Gate）**: GRUで過去と現在の情報の混合比率を制御 → [06_deep_learning/lstm_gru.md](../06_deep_learning/lstm_gru.md)
- **勾配爆発問題（Exploding Gradient）**: 勾配が指数的に増大する問題、勾配クリッピングで対処 → [06_deep_learning/lstm_gru.md](../06_deep_learning/lstm_gru.md)
- **コンテキストベクトル**: Encoderが生成する固定長の意味表現 → [06_deep_learning/rnn.md](../06_deep_learning/rnn.md)
- **Attention機構**: Decoderが入力の重要部分に注目する仕組み → [06_deep_learning/rnn.md](../06_deep_learning/rnn.md)
- **双方向RNN**: 順方向・逆方向の両方向から系列を処理 → [06_deep_learning/rnn.md](../06_deep_learning/rnn.md)
- **Transformer**: Self-Attentionで並列処理、長距離依存学習に優れる → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **Self-Attention（自己注意機構）**: 系列内全要素間の関連度を並列計算 → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **Multi-Head Attention**: 複数の視点で並列にAttentionを計算 → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **位置エンコーディング（Positional Encoding）**: sin/cos関数で系列の順序情報を埋め込み → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **BERT**: Transformer EncoderベースのNLPモデル、双方向学習 → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **GPT**: Transformer Decoderベースの生成モデル、自己回帰 → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **Vision Transformer（ViT）**: 画像をパッチ分割してTransformer適用 → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **パッチ分割（Patch Splitting）**: ViTで画像を固定サイズ（16×16等）に分割し系列化する処理 → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **平坦化（Flatten）**: ViTでパッチを2次元→1次元ベクトルに変換する処理 → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **[CLS]トークン**: ViTの分類用特殊トークン、系列先頭に追加 → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **Query、Key、Value**: Self-Attentionの3つの基本ベクトル → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **Masked Self-Attention**: Decoderで未来の情報を隠すマスク機構 → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **Feed-Forward Network**: Transformerの各層でAttention後に適用される全結合層 → [06_deep_learning/transformer.md](../06_deep_learning/transformer.md)
- **生成モデル（Generative Model）**: データの確率分布を学習し新しいデータを生成するモデル → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **オートエンコーダ（Autoencoder, AE）**: 入力を潜在表現に圧縮し再構成する教師なし学習モデル → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **変分オートエンコーダ（VAE）**: 潜在変数に確率分布を導入したオートエンコーダ、滑らかな潜在空間 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **エンコーダ（Encoder）**: 入力を低次元の潜在表現に変換する部分 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **デコーダ（Decoder）**: 潜在表現から出力を生成・再構成する部分 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **再構成（Reconstruction）**: デコーダが元の入力を復元すること、AEとVAEの共通目的 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **潜在変数（Latent Variable）**: 低次元に圧縮されたデータ表現 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **再構成誤差（Reconstruction Error）**: 入力と再構成出力の差、AE/VAEの損失関数 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **KLダイバージェンス（Kullback-Leibler Divergence）**: VAEで潜在分布を正則化する項 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **再パラメータ化トリック（Reparameterization Trick）**: VAEでサンプリングを微分可能にする技法 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **GAN（敵対的生成ネットワーク）**: 生成器と識別器の敵対的学習で高品質生成 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **生成器（Generator）**: GANでランダムノイズから偽データを生成 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **識別器（Discriminator）**: GANで本物と偽物を区別 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **モード崩壊（Mode Collapse）**: GANで生成器が多様性を失う問題 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **デノイジングオートエンコーダ（DAE）**: ノイズ入力から元データを復元、ロバスト学習 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **CycleGAN**: ペア画像なしで双方向ドメイン変換を学習するGAN → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **Cycle Consistency Loss（循環一貫性損失）**: CycleGANで変換後に元に戻せることを保証する損失 → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **Pix2Pix**: ペア画像を使った画像変換GAN（スケッチ→写真等） → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)
- **DCGAN（Deep Convolutional GAN）**: 畳み込み層を使った画像生成GAN → [06_deep_learning/generative_models.md](../06_deep_learning/generative_models.md)

## AI応用
- **SSD（Single Shot MultiBox Detector）**: ワンステージ物体検出器、リアルタイム処理に適する → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **YOLO（You Only Look Once）**: グリッド分割によるワンステージ物体検出、高速 → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **R-CNN（Region-based CNN）**: Selective Searchで領域候補抽出→CNN分類のツーステージ物体検出（2014年） → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **Fast R-CNN**: RoI Pooling導入でR-CNNを高速化（2015年） → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **Faster R-CNN**: RPN導入でEnd-to-End学習、高精度物体検出（2015年） → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **Mask R-CNN**: Faster R-CNNにマスク生成追加、インスタンスセグメンテーション（2017年） → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **Selective Search**: R-CNNで使用される領域候補生成アルゴリズム → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **RPN（Region Proposal Network）**: 学習可能な領域候補生成ネットワーク、Faster R-CNNで導入 → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **RoI Pooling**: 可変サイズの領域を固定サイズに変換、Fast R-CNNで導入 → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **ワンステージ検出器**: 位置とクラスを1回の順伝播で同時予測（SSD、YOLO） → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **ツーステージ検出器**: 領域候補生成→分類の2段階処理（R-CNN系） → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **物体検出**: 画像内の物体位置（バウンディングボックス）とクラスを特定 → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **セマンティックセグメンテーション**: 画素単位でクラス識別を行う手法 → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **インスタンスセグメンテーション**: ピクセル単位で個別物体を区別（Mask R-CNN等） → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **FCN（Fully Convolutional Network）**: 全結合層を畳み込み層に置き換えたセグメンテーション → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **U-Net**: スキップ結合を使うセグメンテーション手法、医療画像で広く使用 → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **SegNet**: 最大プーリングインデックス記憶でメモリ効率化 → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **DeepLab**: Atrous Convolution使用の高精度セグメンテーション → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **エンコーダ-デコーダ**: 圧縮→復元の対称的ネットワーク構造（SegNet、U-Net等） → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **IoU（Intersection over Union）**: セグメンテーションの評価指標、予測と正解の重なり度 → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **音声認識（ASR）**: 音声波形→テキスト変換 → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **音声合成（TTS）**: テキスト→音声生成 → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **AD変換（A/D変換、アナログ-デジタル変換）**: 音声を離散的なデジタルデータに変換する手法、標本化と量子化を含む → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **標本化（サンプリング）**: 連続的な時間信号を一定間隔で標本を取る処理、時間軸の離散化 → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **量子化（Quantization）**: 振幅を離散的な数値に変換する処理、振幅の離散化 → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **サンプリング周波数**: 1秒間に取得するサンプル数（Hz）、CD音質は44.1kHz → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **ナイキスト定理**: 元の信号の最高周波数の2倍以上のサンプリング周波数が必要 → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **量子化ビット数**: 振幅を何段階で表現するか、16bitでCD音質（65,536段階） → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **MFCC（メル周波数ケプストラム係数）**: メル尺度のケプストラム、最も一般的な音響特徴量 → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **スペクトル包絡**: 音声スペクトルの大まかな形状、音色の特徴を表現 → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **ケプストラム分析（Cepstrum Analysis）**: スペクトル包絡を求める手法、音源と声道を分離 → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **LPC（線形予測符号化）**: 声道の共振特性をモデル化するスペクトル包絡抽出手法 → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **CTC（Connectionist Temporal Classification）**: 音声とテキストの時間的対応付け → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **WaveNet**: 1D-CNNによる音声波形生成 → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **1D-CNN**: 時間方向の畳み込み、音声処理に有効 → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **Whisper**: Transformer型の音声認識モデル、OpenAI開発 → [07_ai_applications/speech_processing.md](../07_ai_applications/speech_processing.md)
- **固定表現（Collocation）**: 複数の単語が結びついて特定の意味を持つ表現、慣用句・イディオム → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **慣用句**: 固定的に使われる言い回し、「腹を割る」「猫の手も借りたい」等 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **イディオム（Idiom）**: 特定の文化的背景を持つ慣用表現、直訳では意味が通じない → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **句ベース翻訳（Phrase-based Translation）**: 単語単位ではなく句単位で翻訳、固定表現に対応 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **BERT（Bidirectional Encoder Representations from Transformers）**: 双方向Transformer Encoderで文脈依存の単語埋め込みを生成する事前学習モデル、ファインチューニングで多様なNLPタスクに対応 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **Masked Language Model（MLM）**: BERTの事前学習タスク、入力の一部をマスクして予測 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **Next Sentence Prediction（NSP）**: BERTの事前学習タスク、2文が連続しているかを判定 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **文脈依存表現**: 同じ単語でも文脈で異なるベクトル表現を持つ（BERTの特徴） → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **画像オープンデータセット**: 誰でもアクセス・再利用可能な画像データ集合、ImageNet/MNIST/COCO等 → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **ImageNet**: 1400万枚、2万カテゴリ、深層学習発展の起点となった大規模データセット → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **MNIST**: 手書き数字7万枚、機械学習の"Hello World"、初学者向け定番 → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **CIFAR-10/100**: 6万枚の小画像（32×32）、10または100クラス → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **COCO**: 33万枚、物体検出・セグメンテーション用、バウンディングボックス付き → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **Pascal VOC**: 2万枚、物体検出の標準ベンチマーク → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **Open Images**: Google提供の900万枚以上の大規模データセット → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **オープンデータの定義**: アクセス・再利用・再配布の自由、技術的・法的障壁なし → [07_ai_applications/image_recognition.md](../07_ai_applications/image_recognition.md)
- **レコメンデーションシステム（推薦システム）**: ユーザーの嗜好を分析し最適なアイテムを推薦するシステム → [07_ai_applications/recommendation_system.md](../07_ai_applications/recommendation_system.md)
- **協調フィルタリング（Collaborative Filtering）**: ユーザー間/アイテム間の類似性に基づく推薦手法 → [07_ai_applications/recommendation_system.md](../07_ai_applications/recommendation_system.md)
- **ユーザーベース協調フィルタリング（User-based CF）**: 類似ユーザーの評価から推薦 → [07_ai_applications/recommendation_system.md](../07_ai_applications/recommendation_system.md)
- **アイテムベース協調フィルタリング（Item-based CF）**: 類似アイテムの評価から推薦 → [07_ai_applications/recommendation_system.md](../07_ai_applications/recommendation_system.md)
- **コンテンツベースフィルタリング（Content-based Filtering）**: アイテムの特徴（メタデータ）に基づく推薦 → [07_ai_applications/recommendation_system.md](../07_ai_applications/recommendation_system.md)
- **ハイブリッド推薦**: 協調フィルタリングとコンテンツベースを組み合わせ → [07_ai_applications/recommendation_system.md](../07_ai_applications/recommendation_system.md)
- **コールドスタート問題（Cold Start Problem）**: 新規ユーザー/アイテムに対する推薦が困難な問題 → [07_ai_applications/recommendation_system.md](../07_ai_applications/recommendation_system.md)
- **行列因子分解（Matrix Factorization）**: ユーザー-アイテム評価行列を低次元行列に分解 → [07_ai_applications/recommendation_system.md](../07_ai_applications/recommendation_system.md)
- **コサイン類似度（Cosine Similarity）**: ベクトル間の角度で類似性を測る指標 → [07_ai_applications/recommendation_system.md](../07_ai_applications/recommendation_system.md)
- **ピアソン相関係数（Pearson Correlation）**: 2変数間の線形相関の強さを測る指標 → [07_ai_applications/recommendation_system.md](../07_ai_applications/recommendation_system.md)

## 自然言語処理（NLP）
- **形態素解析（Morphological Analysis）**: 文を形態素（意味の最小単位）に分割し品詞を判別 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **形態素（Morpheme）**: 意味を持つ最小単位（「走る」「ない」等） → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **品詞（Part-of-Speech, POS）**: 名詞・動詞・形容詞・助詞等の文法的カテゴリ → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **トークン化（Tokenization）**: 文章を単語や部分文字列に分割（品詞判別なし） → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **構文解析（Syntactic Analysis）**: 文の文法構造（主語・述語等）を解析 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **係り受け解析（Dependency Parsing）**: 単語間の依存関係（修飾関係等）を解析 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **意味解析（Semantic Analysis）**: 文の意味内容を理解し概念・関係を抽出 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **固有表現抽出（Named Entity Recognition, NER）**: 人名・地名・組織名等を識別 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **形態素解析器**: MeCab、Janome、Sudachi等のツール → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **分かち書き**: 形態素解析の分割結果を空白区切りで表記 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **Word2Vec**: 単語を分散表現（ベクトル）に変換、Skip-gram/CBOW → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **ルールベース機械翻訳**: 文法規則と辞書を人手で作成（～1970年代後半） → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **統計的機械翻訳**: 対訳データから統計的に学習（1990年代～2010年代） → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **ニューラル機械翻訳（NMT）**: 深層学習による翻訳、Transformer使用、計算コスト大・ハルシネーション等の課題あり（2015年代～） → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **ハルシネーション（機械翻訳）**: NMTで流暢だが誤った翻訳を生成する問題 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **Seq2Seq（Sequence-to-Sequence）**: Encoder-Decoderで系列変換、機械翻訳の基礎 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **GLUE（General Language Understanding Evaluation）**: 自然言語理解を評価する標準ベンチマーク、9つの文章理解タスクで構成、機械翻訳等は含まない → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **CoLA（Corpus of Linguistic Acceptability）**: GLUEタスク、文法性判定 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **SST-2（Stanford Sentiment Treebank）**: GLUEタスク、感情分析（ポジ/ネガ） → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **MRPC（Microsoft Research Paraphrase Corpus）**: GLUEタスク、パラフレーズ判定（2文の意味が同じか） → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **STS-B（Semantic Textual Similarity Benchmark）**: GLUEタスク、意味的類似度スコア（0-5） → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **QQP（Quora Question Pairs）**: GLUEタスク、質問ペアの同値性判定 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **MNLI（Multi-Genre Natural Language Inference）**: GLUEタスク、自然言語推論（含意/矛盾/中立） → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **QNLI（Question Natural Language Inference）**: GLUEタスク、質問応答推論（文章が質問の答えを含むか） → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **RTE（Recognizing Textual Entailment）**: GLUEタスク、含意関係認識 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **WNLI（Winograd Natural Language Inference）**: GLUEタスク、代名詞の指示対象推論 → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **SuperGLUE**: GLUEの後継、より難しいタスクを集めた発展版ベンチマーク → [07_ai_applications/natural_language_processing.md](../07_ai_applications/natural_language_processing.md)
- **プロンプトエンジニアリング（Prompt Engineering）**: 生成AIに望ましい出力を得るため指示文を設計・最適化 → [07_ai_applications/prompt_engineering.md](../07_ai_applications/prompt_engineering.md)
- **プロンプト（Prompt）**: AIへの入力指示文 → [07_ai_applications/prompt_engineering.md](../07_ai_applications/prompt_engineering.md)
- **Zero-shot学習**: 例示なしで指示のみで実行 → [07_ai_applications/prompt_engineering.md](../07_ai_applications/prompt_engineering.md)
- **Few-shot学習（In-context Learning）**: 少数の例示でパターン学習 → [07_ai_applications/prompt_engineering.md](../07_ai_applications/prompt_engineering.md)
- **Chain-of-Thought（CoT）プロンプティング**: 思考過程を促す手法、推論タスクで有効 → [07_ai_applications/prompt_engineering.md](../07_ai_applications/prompt_engineering.md)
- **システムプロンプト**: AIの役割・振る舞いを定義する初期設定 → [07_ai_applications/prompt_engineering.md](../07_ai_applications/prompt_engineering.md)
- **ハルシネーション（Hallucination）**: AIが事実でない情報を生成する現象 → [07_ai_applications/prompt_engineering.md](../07_ai_applications/prompt_engineering.md)
- **役割指定（Role Prompting）**: AIに専門家等の役割を与える手法 → [07_ai_applications/prompt_engineering.md](../07_ai_applications/prompt_engineering.md)
- **温度（Temperature）**: 生成のランダム性を制御するパラメータ → [07_ai_applications/prompt_engineering.md](../07_ai_applications/prompt_engineering.md)

## AI社会・ビジネス
- **ディープフェイク（Deepfake）**: 深層学習で生成される高精度な偽画像・偽動画、Deep Learning + Fake → [08_ai_society/deepfake.md](../08_ai_society/deepfake.md)
- **Face Swap（顔入れ替え）**: 動画内の人物の顔を別人の顔に置換するディープフェイク技術 → [08_ai_society/deepfake.md](../08_ai_society/deepfake.md)
- **合成メディア（Synthetic Media）**: AI生成の画像・動画・音声の総称 → [08_ai_society/deepfake.md](../08_ai_society/deepfake.md)
- **偽情報（Misinformation/Disinformation）**: 誤った情報の拡散、ディープフェイクの主な悪用 → [08_ai_society/deepfake.md](../08_ai_society/deepfake.md)
- **ディープフェイク検出（Deepfake Detection）**: 偽動画を識別する技術、生成技術とのいたちごっこ → [08_ai_society/deepfake.md](../08_ai_society/deepfake.md)
- **CAI（Content Authenticity Initiative）**: コンテンツ真正性の業界標準化団体、Adobe等が推進 → [08_ai_society/deepfake.md](../08_ai_society/deepfake.md)
- **C2PA**: コンテンツの出所・編集履歴を記録する標準規格 → [08_ai_society/deepfake.md](../08_ai_society/deepfake.md)
- **デジタルウォーターマーク（Digital Watermark）**: 真正性を証明する電子透かし → [08_ai_society/deepfake.md](../08_ai_society/deepfake.md)
- **CEO詐欺**: 経営者の声を模倣し従業員に送金指示する詐欺手法 → [08_ai_society/deepfake.md](../08_ai_society/deepfake.md)
- **Liar's Dividend**: 本物の証拠も「ディープフェイクだ」と否定できる状況 → [08_ai_society/deepfake.md](../08_ai_society/deepfake.md)
- **シンギュラリティ（Singularity）**: 人工知能が人類の知能を超える転換点、技術的特異点 → [08_ai_society/singularity.md](../08_ai_society/singularity.md)
- **技術的特異点（Technological Singularity）**: シンギュラリティの正式名称 → [08_ai_society/singularity.md](../08_ai_society/singularity.md)
- **レイ・カーツワイル（Ray Kurzweil）**: シンギュラリティを2045年に到来と予測、Google技術責任者 → [08_ai_society/singularity.md](../08_ai_society/singularity.md)
- **収穫加速の法則（Law of Accelerating Returns）**: 技術進歩が指数関数的に加速するというカーツワイルの理論 → [08_ai_society/singularity.md](../08_ai_society/singularity.md)
- **知能爆発（Intelligence Explosion）**: AIの知能が指数関数的に増大する現象 → [08_ai_society/singularity.md](../08_ai_society/singularity.md)
- **再帰的自己改良（Recursive Self-Improvement）**: AIが自分自身を改良し続けるプロセス → [08_ai_society/singularity.md](../08_ai_society/singularity.md)
- **超知能（Superintelligence）**: 人間の知能を大幅に超えるAI → [08_ai_society/singularity.md](../08_ai_society/singularity.md)
- **ヴァーナー・ヴィンジ（Vernor Vinge）**: 1993年にシンギュラリティ概念を初めて本格提唱 → [08_ai_society/singularity.md](../08_ai_society/singularity.md)
- **ニック・ボストロム（Nick Bostrom）**: 超知能のリスク研究、実存的脅威を警告 → [08_ai_society/singularity.md](../08_ai_society/singularity.md)
- **ハードテイクオフ vs ソフトテイクオフ**: 超知能への到達速度の違い（数日 vs 数年） → [08_ai_society/singularity.md](../08_ai_society/singularity.md)
- **価値アライメント問題（AI Alignment Problem）**: AIの目標を人類の価値観に整合させる困難さ → [08_ai_society/singularity.md](../08_ai_society/singularity.md)
- **CRISP-DM**: データマイニングの標準プロセスモデル、6段階で構成 → [08_ai_society/data_utilization.md](../08_ai_society/data_utilization.md)
- **ビジネス理解（Business Understanding）**: CRISP-DMの第1段階、プロジェクト目標を明確化 → [08_ai_society/data_utilization.md](../08_ai_society/data_utilization.md)
- **データ理解（Data Understanding）**: CRISP-DMの第2段階、データの収集と特性把握 → [08_ai_society/data_utilization.md](../08_ai_society/data_utilization.md)
- **データ準備（Data Preparation）**: CRISP-DMの第3段階、モデリング用データセットの構築 → [08_ai_society/data_utilization.md](../08_ai_society/data_utilization.md)
- **モデリング（Modeling）**: CRISP-DMの第4段階、分析手法選択とモデル構築 → [08_ai_society/data_utilization.md](../08_ai_society/data_utilization.md)
- **評価（Evaluation）**: CRISP-DMの第5段階、ビジネス観点でモデルを評価 → [08_ai_society/data_utilization.md](../08_ai_society/data_utilization.md)
- **デプロイ（Deployment）**: CRISP-DMの第6段階、モデルを実運用環境に展開 → [08_ai_society/data_utilization.md](../08_ai_society/data_utilization.md)
- **反復的プロセス**: CRISP-DMの特徴、各段階を行き来しながら改善 → [08_ai_society/data_utilization.md](../08_ai_society/data_utilization.md)
- **業界横断標準**: CRISP-DMの特徴、あらゆる業界で適用可能 → [08_ai_society/data_utilization.md](../08_ai_society/data_utilization.md)

## 法律・倫理
- **営業秘密**: 秘密管理性・有用性・非公知性の3要件で保護される情報 → [09_law_ethics/trade_secret.md](../09_law_ethics/trade_secret.md)
- **不正競争防止法**: 営業秘密の保護を規定する法律 → [09_law_ethics/trade_secret.md](../09_law_ethics/trade_secret.md)
- **ハードロー（Hard Law）**: 法的拘束力のある規制（法律・条例）、違反に罰則あり、強制力高いが柔軟性低い → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **ソフトロー（Soft Law）**: 法的拘束力のないガイドライン・指針、罰則なし、柔軟性高く迅速対応可能だが強制力なし → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **リスクベースアプローチ（AI規制）**: 高リスク領域は早期にハードロー化、低リスクはソフトローで柔軟対応 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **段階的規制**: ソフトローで開始→知見蓄積後にハードロー化する規制手法 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **EU AI Act**: 世界初の包括的AI規制法、高リスクAIに適合性評価義務、違反に巨額罰金（ハードロー） → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **中国生成AI管理弁法**: 生成AIサービスの登録・審査義務を規定する法律（2023施行、ハードロー） → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **AI倫理原則**: 人間中心・透明性・公平性・プライバシー・安全性・アカウンタビリティ・人間の監督 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **AIガバナンス**: AI開発・利用におけるリスク管理と倫理原則の実装体系 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **OECD AI原則**: 世界初の政府間AI合意、42カ国採択（2019、ソフトロー） → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **EU倫理ガイドライン**: 信頼できるAIの7要件、リスクベース分類（ソフトロー） → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **人間中心のAI社会原則**: 日本の内閣府が策定した7つの原則（2019、ソフトロー） → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **米国AI権利章典ブループリント**: 5つの原則を提示（2022、ソフトロー） → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **Ethics by Design**: 設計段階から倫理原則を組み込む開発手法 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **リスクベースアプローチ**: 高リスク領域では厳格、低リスクは柔軟対応 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **透明性（Transparency）**: AIの動作原理・データ利用・判断根拠の説明可能性 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **説明可能性（Explainability）**: 判断根拠を説明できる技術的能力（XAI） → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **分析対象者**: データが収集・分析される本人。直接ユーザーでなくても説明を受ける権利あり → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **アカウンタビリティ（Accountability）**: AI判断の結果に対する責任の所在明確化 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **公平性（Fairness）**: 人種・性別等による不当な差別の排除 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **XAI（Explainable AI）**: 説明可能なAI技術（LIME、SHAP等） → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **LIME**: 局所的な挙動を線形モデルで近似する説明手法 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **SHAP**: シャープレイ値を用いた特徴量の貢献度計算 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **説明可能性と性能のトレードオフ**: 複雑なモデルほど高性能だが説明困難 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **リスクベースアプローチ**: 高リスク領域では説明可能性優先、低リスクは柔軟対応 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **Ethics by Design**: 設計段階から倫理原則を組み込む開発手法 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **Privacy by Design**: 設計段階からプライバシー保護を組み込む手法 → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **法的・倫理的検討のタイミング**: 企画・設計の初期段階から継続的に実施すべき → [09_law_ethics/ai_ethics_principles.md](../09_law_ethics/ai_ethics_principles.md)
- **カメラ画像利活用ガイドブック**: 経済産業省が策定したカメラ画像利活用の指針 → [09_law_ethics/camera_image_guidelines.md](../09_law_ethics/camera_image_guidelines.md)
- **事前告知**: カメラ撮影・利活用開始前の通知（原則） → [09_law_ethics/camera_image_guidelines.md](../09_law_ethics/camera_image_guidelines.md)
- **個人識別符号**: 顔認識データ等、個人情報保護法で定義される → [09_law_ethics/camera_image_guidelines.md](../09_law_ethics/camera_image_guidelines.md)
- **AI開発契約**: AI開発の委託契約、知的財産権・精度保証・データ権利処理を規定 → [09_law_ethics/ai_development_contract.md](../09_law_ethics/ai_development_contract.md)
- **学習済みモデルの著作権**: 契約で帰属を明示（開発者保持 or 委託者譲渡） → [09_law_ethics/ai_development_contract.md](../09_law_ethics/ai_development_contract.md)
- **ベストエフォート条項**: AI精度は確率的で完全保証困難、最善努力義務を規定 → [09_law_ethics/ai_development_contract.md](../09_law_ethics/ai_development_contract.md)
- **成果物の定義**: モデル・重み・ソースコード・ドキュメントの範囲を明確化 → [09_law_ethics/ai_development_contract.md](../09_law_ethics/ai_development_contract.md)
- **瑕疵担保責任**: プログラムのバグは瑕疵、精度未達は瑕疵でない → [09_law_ethics/ai_development_contract.md](../09_law_ethics/ai_development_contract.md)
- **職務著作**: 契約なき場合、著作権は開発者に帰属するデフォルトルール → [09_law_ethics/ai_development_contract.md](../09_law_ethics/ai_development_contract.md)

## AI限界・セキュリティ
- **敵対的攻撃（Adversarial Attack）**: ニューラルネットワークを意図的に誤作動させる攻撃手法 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **敵対的サンプル（Adversarial Examples）**: 誤分類を誘発するために作成された入力データ → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **摂動（Perturbation）**: 入力に加える微小なノイズ、人間には知覚不可能 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **FGSM（Fast Gradient Sign Method）**: 勾配の符号を利用した基本的な敵対的攻撃手法 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **PGD（Projected Gradient Descent）**: FGSMの反復版、より強力な攻撃 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **C&W攻撃（Carlini & Wagner）**: 最も強力な敵対的攻撃の1つ → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **転移可能性（Transferability）**: ある模델で生成した敵対的サンプルが他モデルでも有効な性質 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **ホワイトボックス攻撃**: モデルの内部構造・パラメータを知っている場合の攻撃 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **ブラックボックス攻撃**: モデルの出力のみにアクセスできる場合の攻撃 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **物理的攻撃（Physical Attack）**: 実世界の物体（道路標識等）を改変する攻撃 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **敵対的訓練（Adversarial Training）**: 敵対的サンプルで学習して頑健性を向上させる防御手法 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **ターゲット攻撃（Targeted Attack）**: 特定のクラスに誤分類させる攻撃 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **非ターゲット攻撃（Untargeted Attack）**: 正しいクラス以外に誤分類させる攻撃 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **Universal Adversarial Perturbations**: 1つの摂動で多数の画像を誤分類させる攻撃 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **データ依存性**: AIは訓練データの品質に完全依存する限界 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **説明困難性（ブラックボックス問題）**: 深層学習の判断根拠が不透明な問題 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **ドメイン外データ**: 訓練時に見なかったデータへの対応困難 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)
- **常識の欠如**: 人間の常識や因果推論の理解不足 → [08_ai_society/ai_limitations.md](../08_ai_society/ai_limitations.md)

## 数学・統計
- **情報理論（Information Theory）**: 情報の量・伝達・圧縮を数学的に扱う理論 → [10_math_statistics/information_theory.md](../10_math_statistics/information_theory.md)
- **エントロピー（Entropy）**: 情報の不確実性、$H(X) = -\sum p(x) \log p(x)$ → [10_math_statistics/information_theory.md](../10_math_statistics/information_theory.md)
- **相互情報量（Mutual Information）**: 2変数の依存性、**I(X;Y)=0で独立** → [10_math_statistics/information_theory.md](../10_math_statistics/information_theory.md)
- **KLダイバージェンス（Kullback-Leibler Divergence）**: 分布間の乖離度、非対称 → [10_math_statistics/information_theory.md](../10_math_statistics/information_theory.md)
- **クロスエントロピー（Cross Entropy）**: 分類問題の損失関数 → [10_math_statistics/information_theory.md](../10_math_statistics/information_theory.md)
- **条件付きエントロピー（Conditional Entropy）**: Xを知った後のYの不確実性 → [10_math_statistics/information_theory.md](../10_math_statistics/information_theory.md)
- **情報利得（Information Gain）**: 決定木で分岐時のエントロピー減少量 → [10_math_statistics/information_theory.md](../10_math_statistics/information_theory.md)
- **最適化（Optimization）**: 損失関数を最小化するパラメータを見つける手法 → [10_math_statistics/optimization.md](../10_math_statistics/optimization.md)
- **勾配降下法（Gradient Descent）**: 勾配の逆方向にパラメータを更新する最適化手法 → [10_math_statistics/optimization.md](../10_math_statistics/optimization.md)
- **局所最適解（Local Optimum）**: 探索範囲内で最適だが全体では最適でない可能性がある解 → [10_math_statistics/optimization.md](../10_math_statistics/optimization.md)
- **大域最適解（Global Optimum）**: 全体で最も良い解、全パラメータ空間で最小 → [10_math_statistics/optimization.md](../10_math_statistics/optimization.md)
- **学習率（Learning Rate）**: パラメータ更新の幅を制御するハイパーパラメータ → [10_math_statistics/optimization.md](../10_math_statistics/optimization.md)
- **学習率減衰（Learning Rate Decay）**: 学習の進行に応じて学習率を小さくする手法 → [10_math_statistics/optimization.md](../10_math_statistics/optimization.md)
- **モメンタム（Momentum）**: 1990年代提唱、過去の勾配を慣性として利用し鞍点・局所解を脱出、Adamに採用 → [10_math_statistics/optimization.md](../10_math_statistics/optimization.md)
- **Adam（Adaptive Moment Estimation）**: モメンタムと適応的学習率を組み合わせた最適化手法、現在最も広く使われる → [10_math_statistics/optimization.md](../10_math_statistics/optimization.md)
- **SGD（確率的勾配降下法）**: 1サンプルごとに勾配を計算して更新する手法 → [10_math_statistics/optimization.md](../10_math_statistics/optimization.md)
- **ミニバッチ勾配降下法**: 小さなバッチで勾配を計算、現在の標準手法 → [10_math_statistics/optimization.md](../10_math_statistics/optimization.md)
- **鞍点（Saddle Point）**: 一部方向で極小・別方向で極大、勾配0で停滞、モメンタムで突破 → [10_math_statistics/optimization.md](../10_math_statistics/optimization.md)

---

## 更新履歴
- 2026/01/07: マルチエージェント強化学習（MARL、AlphaGo、AlphaZero、OpenAI Five、AlphaStar、自己対戦、協調・競争、CTDE、QMIX、ナッシュ均衡等）追加
- 2026/01/07: 米国とEUのAI規制アプローチ比較（EU AI Act、米国AI権利章典、包括的規制 vs セクター別規制、リスクベース、罰則、域外適用等）追加
- 2026/01/07: 敵対的攻撃（Adversarial Attack、敵対的サンプル、FGSM、PGD、転移可能性、物理的攻撃、敵対的訓練、ホワイトボックス・ブラックボックス攻撃等）追加
- 2026/01/06: GLUE（9タスク、CoLA、SST-2、MRPC、STS-B、QQP、MNLI、QNLI、RTE、WNLI、含まれないタスク）追加
- 2026/01/06: NAS（Neural Architecture Search、探索空間、探索戦略、強化学習ベース、進化的アルゴリズム、勾配ベース、並列化、DARTS、AutoML等）追加
- 2026/01/06: ハードロー・ソフトロー（法的拘束力、罰則、柔軟性、GDPR、OECD原則、EU AI Act、リスクベースアプローチ、段階的規制等）追加
- 2026/01/03: CNN出力サイズ計算問題、ランダムフォレスト構成要素、バギングとアンサンブル学習の関係、スペクトル包絡とケプストラム分析を索引に追加
- 2026/01/02: 正則化選択肢問題対策（不適切パターン、データ拡張との違い、L1/L2比較、判定フロー等）追加
- 2026/01/02: 量子化・知識蒸留詳細（PTQ/QAT、FP32→INT8、教師・生徒モデル、ソフトターゲット、温度パラメータ、実例等）追加
- 2026/01/02: 転移学習（Transfer Learning、ファインチューニング、事前学習、ドメイン適応、データ効率、穴埋め問題対策等）追加
- 2026/01/02: 個人情報保護法・仮名加工情報（内部分析のみ、第三者提供原則禁止、匿名加工情報との違い、選択肢問題対策等）追加
- 2026/01/02: モメンタム・鞍点問題詳細（1990年代提唱、Adamへの採用、学習停滞防止等）追加
- 2026/01/02: 最適化関連のキーワードを追加（勾配降下法、局所最適解、大域最適解、学習率、モメンタム、Adam、SGD等）
- 2026/01/02: EfficientNet関連のキーワードを追加（Compound Scaling、Compound Coefficient、NAS、深さ・幅・解像度の複合スケーリング等）
- 2026/01/02: 画像オープンデータセット関連のキーワードを追加（ImageNet、MNIST、CIFAR、COCO、Pascal VOC、Open Images、オープンデータの定義等）
- 2026/01/02: レコメンデーションシステム関連のキーワードを追加（協調フィルタリング、コンテンツベースフィルタリング、ハイブリッド推薦、コールドスタート問題、行列因子分解、コサイン類似度、ピアソン相関係数等）
- 2026/01/02: 最適化手法と局所・大域最適解の関連キーワードを追加（局所最適解、大域最適解、勾配降下法、学習率減衰、モーメンタム、Adam、鞍点、多峰性関数等）
- 2026/01/01: EfficientNet関連のキーワードを追加（Compound Scaling、Compound Coefficient、深さスケーリング、幅スケーリング、解像度スケーリング、NAS、MBConv、B0〜B7シリーズ等）
- 2026/01/01: Depthwise Separable Convolution/MobileNet関連のキーワードを追加（軽量化CNN、Depthwise Convolution、Pointwise Convolution、計算量削減、モバイルAI等）
- 2026/01/01: ディープフェイク関連のキーワードを追加（Face Swap、合成メディア、偽情報、検出技術、CAI、C2PA、デジタルウォーターマーク、CEO詐欺、Liar's Dividend等）
- 2026/01/01: シンギュラリティ関連のキーワードを追加（技術的特異点、レイ・カーツワイル、2045年、知能爆発、再帰的自己改良、超知能、価値アライメント問題等）
- 2026/01/01: 連鎖律（Chain Rule）の活用場面を詳細化、試験頻出パターンを追加
- 2026/01/01: CycleGAN（ペア画像不要、Cycle Consistency Loss、Pix2Pix比較等）追加
- 2026/01/01: 過学習・汎化（Overfitting、Underfitting、ダブルディセント、正則化、Dropout、Early Stopping等）追加
- 2026/01/01: 生成モデル（オートエンコーダ、VAE、GAN、再構成、潜在変数、KLダイバージェンス等）追加
- 2026/01/02: アルゴリズムバイアス・公平性（訓練データの偏り、プロキシ変数、統計的公平性、COMPAS、フィードバックループ、選択肢問題対策等）追加
- 2026/01/02: 活性化関数G検定選択肢問題対策（正解パターン、誤答識別、判定フロー等）追加
- 2026/01/01: Global Average Pooling（GAP、CAM、解釈性向上、全結合層代替等）追加
- 2026/01/01: Dilated Convolution（拡張畳み込み、Dilation Rate、受容野拡大、ASPP等）追加
- 2026/01/01: CRISP-DM（ビジネス理解、データ理解、データ準備、モデリング、評価、デプロイ等）追加
- 2026/01/01: 第二次AIブーム・エキスパートシステム（MYCIN、DENDRAL、知識獲得のボトルネック等）追加
- 2026/01/01: マルチタスク学習（共有表現、ハード/ソフトパラメータ共有、負の転移等）追加
- 2026/01/01: BPTT（Truncated BPTT、勾配クリッピング等）追加
- 2026/01/01: Leaky ReLU詳細（Dying ReLU問題、PReLU、ELU等）追加
- 2025/12/31: AIガバナンス（OECD原則、EU倫理GL、人間中心のAI社会原則、Ethics by Design等）追加
- 2025/12/31: プルーニング（構造化/非構造化、スパース性、量子化、知識蒸留等）追加
- 2025/12/31: 活性化関数（ReLU、勾配消失問題、Leaky ReLU、Dying ReLU、tanh等）詳細追加
- 2025/12/31: 教師なし学習（k-means法、クラスタリング、次元削減、PCA、顧客セグメンテーション等）追加
- 2025/12/31: Sim-to-Real転移（ドメインランダマイゼーション、ドメイン適応、システム同定等）追加
- 2025/12/31: ε-greedy方策（探索と活用のトレードオフ、ε-減衰等）追加
- 2025/12/31: 情報理論（エントロピー、相互情報量、KLダイバージェンス、クロスエントロピー等）追加
- 2025/12/31: フレーム問題・常識の問題・シンボルグラウンディング問題（資格問題、Cycプロジェクト等）追加
- 2025/12/31: 機械翻訳の歴史（ルールベース、統計的、ニューラル機械翻訳、Seq2Seq等）追加
- 2025/12/31: ミニマックス法・αβ枝刈り（ゲーム木探索、評価関数、MCTS等）追加
- 2025/12/31: プロンプトエンジニアリング（Zero-shot、Few-shot、CoT、ハルシネーション等）追加
- 2025/12/31: 交差検証（k-分割、ホールドアウト法、LOOCV、層化k-分割等）追加
- 2025/12/31: 自然言語処理（形態素解析、構文解析、トークン化、係り受け解析、固有表現抽出等）追加
- 2025/12/30: AI開発契約（知的財産権、ベストエフォート条項、成果物定義、瑕疵担保責任等）追加
- 2025/12/30: Transformer（Self-Attention、Multi-Head Attention、位置エンコーディング、BERT、GPT等）追加
- 2025/12/30: 音声処理（音声認識、音声合成、MFCC、CTC、WaveNet、Whisper等）追加
- 2025/12/30: RNN（Encoder-Decoder、Seq2Seq、機械翻訳、勾配消失問題等）追加
- 2025/12/30: アンサンブル学習（バギング、ブースティング、ランダムフォレスト、XGBoost等）追加
- 2025/12/30: 法的・倫理的検討のタイミング、Ethics by Design、開発段階別チェックリスト追加
- 2025/12/30: 説明可能性と性能のトレードオフ、XAI技術（LIME、SHAP）追加
- 2025/12/30: 探索手法の適用領域（記号的AI vs サブシンボリックAI）追加
- 2025/12/30: AI倫理原則（透明性、説明可能性、分析対象者の権利等）追加
- 2025/12/30: 評価指標（RMSE、MAE、精度、適合率、再現率、F1、AUC-ROC、R²）追加
- 2025/12/28: 初期版作成（プランニング、探索アルゴリズム、CNN、Softmax、営業秘密）
