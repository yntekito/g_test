# ニューラルネットワーク基礎

## 要点
- 入力層・中間層・出力層で構成され、重み・バイアス・活性化関数で非線形変換を実現。
- 中間層ではReLU等、出力層は2値分類でSigmoid、多クラス分類で**Softmax**を使用。
- 誤差逆伝播法で重み更新し、損失関数（クロスエントロピー等）を最小化。

## 定義
ニューラルネットワークは、生物の神経細胞を模した計算モデルで、複数の層（レイヤー）を持ち、各層のユニット（ニューロン）が重み付き結合で接続される。各ユニットは入力の線形結合に活性化関数を適用し、非線形な表現能力を獲得する。

## 重要キーワード
- **ニューロン（ユニット）**: 入力を受け取り、重み付き和+バイアスに活性化関数を適用して出力。
- **層（Layer）**: 入力層、中間層（隠れ層）、出力層で構成。
- **重み（Weight）**: 入力信号の重要度を調整するパラメータ。
- **バイアス（Bias）**: 閾値調整のための定数項。
- **活性化関数（Activation Function）**: 非線形変換を行う関数（ReLU, Sigmoid, tanh, Softmax等）。
- **Softmax関数**: 多クラス分類の出力層で各クラスの確率を計算（合計1）。
- **Sigmoid関数**: 2値分類の出力層で確率を計算（0～1）。
- **ReLU（Rectified Linear Unit）**: 中間層で広く使われる活性化関数（$\max(0, x)$）。
- **損失関数（Loss Function）**: 予測と正解のズレを定量化（クロスエントロピー、平均二乗誤差等）。
- **誤差逆伝播法（Backpropagation）**: 損失関数の勾配を逆向きに伝播し重みを更新。
- **全結合層（Fully Connected Layer）**: 前層の全ユニットと結合。

## 詳細

### 背景と直観
ニューラルネットワークは1943年のMcCulloch-Pittsモデルに起源を持ち、1980年代の誤差逆伝播法の発見により実用化が進みました。深層学習の基礎となる構造です。

**基本的な考え方**：
- 単純なユニットを多層に組み合わせることで複雑な関数を近似
- 学習データから重みを自動調整し、入力→出力の写像を獲得

### ニューラルネットワークの構造

```
入力層      中間層1     中間層2     出力層
  x1 ──┐
       ├→ h1 ──┐
  x2 ──┤       ├→ h3 ──┐
       ├→ h2 ──┘       ├→ y1
  x3 ──┘               ├→ y2
                       └→ y3
```

**各層の役割**
- **入力層**: 特徴量を受け取る
- **中間層**: 特徴の階層的抽出・変換
- **出力層**: タスクに応じた予測値を出力

### ユニットの計算

1. **線形結合**：$z = \sum_{i=1}^{n} w_i x_i + b$
2. **活性化**：$a = f(z)$

ここで $w_i$ は重み、$b$ はバイアス、$f$ は活性化関数。

### 主要な活性化関数

#### 勾配消失問題と解決策

**勾配消失問題**:
ニューラルネットワークを多層化（深層化）すると、誤差逆伝播時に勾配が層を遡るごとに小さくなり、初期層の重みがほとんど更新されなくなる現象。特にSigmoidやtanhなどの**飽和する活性化関数**で顕著。

**原因**:
- **Sigmoid/tanhの勾配**: 入力が大きい（または小さい）とき、勾配が0に近づく
  - Sigmoid: 勾配の最大値は0.25、入力が±5を超えると勾配≈0
  - tanh: 勾配の最大値は1、入力が±3を超えると勾配≈0
- **連鎖律**: 各層の勾配を掛け合わせるため、0.25 × 0.25 × ... → 指数的に減少
- **結果**: 深い層では学習が進まず、性能が頭打ち

**解決策: ReLU系活性化関数の登場**

---

#### 1. ReLU（Rectified Linear Unit）★最重要
$$\text{ReLU}(x) = \max(0, x) = \begin{cases} x & (x > 0) \\ 0 & (x \leq 0) \end{cases}$$

**勾配**:
$$\frac{d}{dx}\text{ReLU}(x) = \begin{cases} 1 & (x > 0) \\ 0 & (x \leq 0) \end{cases}$$

**勾配消失問題を解決する理由**:
- **正の領域で勾配が常に1**: 飽和しない、勾配が減衰しない
- **線形性**: 正の領域では $f(x) = x$ と同じ、計算が高速
- **スパース性**: 負の入力は0になり、一部のニューロンのみ活性化（効率的）

**利点**:
- 勾配消失を大幅に緩和（深い層でも学習可能）
- 計算が単純で高速（max演算のみ）
- 生物学的妥当性（ニューロンの発火パターンに近い）
- 2012年AlexNet以降、深層学習の標準に

**欠点**:
- **Dying ReLU問題**: 負の入力で勾配が0になり、一度死んだニューロンは復活しない
  - 学習率が大きすぎると多数のニューロンが死ぬ
  - 対策: Leaky ReLU、PReLU、ELU等

**実例（勾配の伝播）**:
```
10層ニューラルネットワークの勾配伝播:

【Sigmoid使用】
各層の勾配: 0.25 × 0.25 × ... × 0.25 (10回)
= 0.25^10 ≈ 0.0000001
→ 初期層の勾配がほぼ0、学習不可

【ReLU使用】
各層の勾配: 1 × 1 × ... × 1 (正の領域)
= 1
→ 勾配が減衰しない、全層で学習可能
```

---

#### 1-2. ReLU系の改良版

---

**Leaky ReLU（リーキー ReLU）**★試験頻出

**定義**:
$$\text{Leaky ReLU}(x) = \begin{cases} x & (x > 0) \\ \alpha x & (x \leq 0) \end{cases}$$

ここで $\alpha$ は小さな定数（通常**0.01**）

**勾配**:
$$\frac{d}{dx}\text{Leaky ReLU}(x) = \begin{cases} 1 & (x > 0) \\ \alpha & (x \leq 0) \end{cases}$$

**特徴と目的**:
- **Dying ReLU問題の解決**: ReLUの最大の欠点である「負の領域で勾配が完全に0になる問題」を解決
- **負の領域にも小さな勾配**: $\alpha$ により負の入力でも勾配が伝播（通常 $\alpha = 0.01$）
- **計算が高速**: ReLUと同様に簡単な演算のみ
- **スパース性は低下**: ReLUと異なり、負の値も0にならない

**Dying ReLU問題とは**:
ReLUでは $x \leq 0$ のとき出力も勾配も0になるため、一度負の領域に入ったニューロンは以降の学習で重みが更新されず「死んだ」状態になる。特に：
- 学習率が大きすぎる場合
- 初期化が不適切な場合
- バッチ正規化がない場合

に多数のニューロンが死ぬことがある。

**Leaky ReLUの解決策**:
負の領域でも $\alpha x$（例: $0.01x$）の小さな出力と勾配 $\alpha$ を保持するため、完全に死ぬことを防ぐ。

**数値例**:
```
入力:  [-2.0, -1.0,  0,  1.0,  2.0]

【ReLU】
出力:  [ 0,    0,    0,  1.0,  2.0]
勾配:  [ 0,    0,  0/1,  1,    1  ]
→ 負の入力では勾配0、学習不可

【Leaky ReLU（α=0.01）】
出力:  [-0.02, -0.01, 0,  1.0,  2.0]
勾配:  [ 0.01,  0.01, 1,  1,    1  ]
→ 負の入力でも小さな勾配、学習可能
```

**グラフ比較**:
```
      ReLU              Leaky ReLU
      |                    |
    2 |     ／            2|     ／
    1 |    ／             1|    ／
y   0 |___               0|__／  (傾き0.01)
   -1 |                 -1|／
      +------- x          +------- x
     -2  0  2            -2  0  2
```

**利点**:
- Dying ReLU問題を緩和
- 勾配消失問題も解決（ReLU同様）
- 計算コスト低い（ReLUとほぼ同じ）
- 実装が簡単

**欠点**:
- スパース性の低下（全ニューロンが活性化）
- 効果が限定的な場合もある（PReLUやELUの方が良いケースも）
- $\alpha$ のハイパーパラメータ調整が必要

**実務での使用**:
- ReLUでDying ReLU問題が発生した場合の代替
- 画像認識、物体検出等で広く使用（ResNet、YOLO等）
- $\alpha = 0.01$ が標準的な設定

---

**PReLU（Parametric ReLU / パラメトリック ReLU）**

**定義**:
$$\text{PReLU}(x) = \begin{cases} x & (x > 0) \\ \alpha x & (x \leq 0) \end{cases}$$

**Leaky ReLUとの違い**:
- Leaky ReLU: $\alpha$ は固定（通常0.01）
- PReLU: $\alpha$ は**学習可能なパラメータ**（勾配降下法で更新）

**特徴**:
- 各ニューロン（またはチャネル）ごとに異なる $\alpha$ を持つことが可能
- データに最適な $\alpha$ を自動学習
- パラメータ数が若干増加（各ニューロンに1つの $\alpha$）

**利点**: Leaky ReLUより柔軟、精度向上の可能性
**欠点**: 過学習リスク増加、実装がやや複雑

**実例**: 
ImageNetコンペティションで使用され、ReLUより高精度を達成（PReLU論文, 2015）

---

**ELU（Exponential Linear Unit）**

**定義**:
$$\text{ELU}(x) = \begin{cases} x & (x > 0) \\ \alpha(e^x - 1) & (x \leq 0) \end{cases}$$

通常 $\alpha = 1.0$

**特徴**:
- 負の領域で**指数関数的に滑らか**
- 出力の平均が0に近づく（収束が速い）
- ノイズに頑健

**利点**: 学習の収束が速い、精度向上
**欠点**: 指数関数の計算コスト（ReLU/Leaky ReLUより遅い）

---

**ReLU系活性化関数の比較**

| 活性化関数 | 正の領域 | 負の領域 | 勾配（負） | 計算速度 | Dying ReLU対策 | 用途 |
|-----------|---------|---------|-----------|---------|---------------|------|
| **ReLU** | $x$ | $0$ | $0$ | ◎ 最速 | ✗ なし | デフォルト |
| **Leaky ReLU** | $x$ | $0.01x$ | $0.01$ | ◎ 高速 | ○ あり | Dying ReLU発生時 |
| **PReLU** | $x$ | $\alpha x$ | $\alpha$ (学習) | ○ 高速 | ○ あり | 高精度重視 |
| **ELU** | $x$ | $\alpha(e^x-1)$ | $\alpha e^x$ | △ 中速 | ○ あり | 収束速度重視 |

**推奨順**:
1. **ReLU**: まず試す（最も標準的）
2. **Leaky ReLU**: Dying ReLU問題が発生したら
3. **PReLU/ELU**: さらなる精度向上が必要な場合

---

**試験での問われ方（Leaky ReLU）**

**典型的な出題パターン**:
1. **定義の確認**: 「Leaky ReLUの式は？」→ $\max(0.01x, x)$ または条件分岐の形
2. **目的**: 「Leaky ReLUは何の問題を解決するか」→ **Dying ReLU問題**
3. **負の領域の処理**: 「負の入力に対する出力は？」→ **αx（通常0.01x）**
4. **勾配の特徴**: 「負の領域での勾配は？」→ **α（通常0.01）**、0ではない
5. **ReLUとの違い**: 「ReLUと比較した利点は？」→ **負の領域でも勾配が伝播**

**ひっかけポイント（混同しやすい記述）**:

| 記述 | 正誤 | 補足 |
|------|------|------|
| 「負の入力で出力が完全に0」 | ✗ | ReLUの特徴、Leaky ReLUは $\alpha x$ |
| 「負の入力で小さな正の値を出力」 | ✗ | 負の入力 → 負の出力（$\alpha x < 0$） |
| 「負の入力で勾配が0」 | ✗ | ReLUの問題点、Leaky ReLUは勾配 $\alpha$ |
| 「負の入力でも勾配が伝播」 | ◎ 正解 | Leaky ReLUの最大の特徴 |
| 「Dying ReLU問題を解決」 | ◎ 正解 | 開発の主目的 |
| 「αは通常0.01」 | ◎ 正解 | 標準的な設定値 |
| 「正の領域では恒等関数」 | ◎ 正解 | $f(x) = x$、ReLUと同じ |

**選択肢で出やすい正解パターン**:
- ✅ 「負の入力に対して、小さな負の勾配を持つ」
- ✅ 「負の領域で $\alpha x$（$\alpha$ は小さな定数）を出力」
- ✅ 「Dying ReLU問題を緩和する」
- ✅ 「負の入力でも学習が進む」

**選択肢で出やすい誤答パターン**:
- ❌ 「負の入力で出力が0になる」→ ReLUの特徴
- ❌ 「負の入力で勾配が0になる」→ ReLUの問題点
- ❌ 「勾配消失問題を引き起こす」→ 逆、解決する側
- ❌ 「出力が0～1の範囲に制限される」→ Sigmoidの特徴

**比較問題でのポイント**:
- **ReLU vs Leaky ReLU**: 「負の領域での挙動」が決定的な違い
  - ReLU: 出力0、勾配0 → Dying ReLU
  - Leaky ReLU: 出力 $\alpha x$、勾配 $\alpha$ → 学習継続
- **Leaky ReLU vs PReLU**: $\alpha$ が固定 vs 学習可能
- **Leaky ReLU vs ELU**: 線形 vs 指数関数（滑らか）

---

#### 2. Sigmoid（2値分類の出力層専用）
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

**勾配**:
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

**特徴**:
- **出力**: 0～1（確率として解釈）
- **勾配**: 最大0.25（x=0のとき）、±5で≈0

**用途**: 2値分類の出力層のみ

**中間層では非推奨の理由**:
- 勾配消失問題が深刻
- 出力が0中心でない（学習の収束が遅い）
- 指数関数の計算コスト

---

#### 3. Softmax（多クラス分類の出力層）★重要
$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

**特徴**
- $K$ クラスの各スコア $z_1, ..., z_K$ を確率に変換
- 全クラスの確率の合計が1：$\sum_{i=1}^{K} p_i = 1$
- 指数関数により最大値を強調（温度パラメータで調整可能）

**具体例**
入力スコア: $[2.0, 1.0, 0.1]$
```
exp(2.0) ≈ 7.39
exp(1.0) ≈ 2.72
exp(0.1) ≈ 1.11
合計 = 11.22

確率:
クラス0: 7.39/11.22 ≈ 0.659
クラス1: 2.72/11.22 ≈ 0.242
クラス2: 1.11/11.22 ≈ 0.099
```

**使い方**: 最終層で適用し、クロスエントロピー損失と組み合わせる

---

#### 4. tanh（中間層、現在はReLUが主流）
$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

**勾配**:
$$\tanh'(x) = 1 - \tanh^2(x)$$

**特徴**:
- **出力**: -1～1（0中心、Sigmoidより良い）
- **勾配**: 最大1（x=0のとき）、±3で≈0

**Sigmoidより優れる点**:
- 0中心の出力（学習の収束が速い）
- 勾配がやや大きい

**ReLUに劣る点**:
- 勾配消失問題は残る
- 計算コストが高い

---

### 活性化関数の比較と使い分け

#### 中間層での選択

| 活性化関数 | 勾配消失 | 計算速度 | 推奨度 | 備考 |
|-----------|---------|---------|--------|------|
| **ReLU** | ◎ なし | ◎ 高速 | ★★★ | 第一選択 |
| **Leaky ReLU** | ◎ なし | ◎ 高速 | ★★☆ | Dying ReLU対策 |
| **ELU** | ◎ なし | △ 中速 | ★☆☆ | 滑らかさ重視 |
| **tanh** | ✗ あり | △ 中速 | ☆☆☆ | RNN等で限定的 |
| **Sigmoid** | ✗ 深刻 | △ 中速 | ✗ 非推奨 | 出力層のみ |

**推奨**:
1. **デフォルト**: ReLU
2. **Dying ReLU問題が発生**: Leaky ReLU、PReLU
3. **RNNの内部ゲート**: tanh、Sigmoid（特殊用途）

#### 出力層での選択

| タスク | 活性化関数 | 損失関数 | 出力範囲 |
|--------|-----------|----------|---------|
| 2値分類 | **Sigmoid** | 2値クロスエントロピー | 0～1 |
| 多クラス分類 | **Softmax** | カテゴリカルクロスエントロピー | 0～1（合計1） |
| 回帰 | 線形（恒等） | 平均二乗誤差（MSE） | -∞～+∞ |
| 正の値のみ回帰 | ReLU/Softplus | MSE | 0～+∞ |

### 損失関数と学習

**クロスエントロピー損失（多クラス分類）**
$$L = -\sum_{i=1}^{K} y_i \log(\hat{y}_i)$$
- $y_i$: 正解ラベル（one-hot）
- $\hat{y}_i$: Softmaxの出力確率

**最適化**
- 誤差逆伝播法で各重みの勾配を計算
- SGD, Adam等のオプティマイザで更新
- $w := w - \eta \frac{\partial L}{\partial w}$

### 図解：順伝播と逆伝播

```
[順伝播]
入力 → 線形変換 → 活性化 → ... → 出力 → 損失計算

[逆伝播]
損失 → ∂L/∂output → ∂L/∂weights ← 連鎖律 ← 各層
```

## 試験での問われ方

### 典型設問
- **「勾配消失問題を解決する活性化関数は？」→ ReLU**
- **「中間層で最も使われる活性化関数は？」→ ReLU**
- **「多クラス分類で確率を出力する関数は？」→ Softmax**
- **「2値分類の出力層は？」→ Sigmoid**
- **「誤差逆伝播で勾配消失を防ぐ役割を持つ活性化関数」→ ReLU**

### 比較されやすい概念

**勾配消失問題関連**:
- **Sigmoid/tanh vs ReLU**: 飽和する（勾配消失） vs 飽和しない（勾配1）
- **深い層での学習**: Sigmoid/tanhは困難、ReLUは可能
- **歴史**: 2012年AlexNetでReLUが標準化、深層学習ブームの一因

**活性化関数の役割別**:
- **中間層**: ReLU系（勾配消失対策）
- **出力層（分類）**: Sigmoid（2値）、Softmax（多クラス）
- **出力層（回帰）**: 線形

**Softmax vs Sigmoid**:
- **Softmax**: 多クラス、合計1、相互依存
- **Sigmoid**: 2値、独立確率、各出力が独立

### 引っ掛けポイント

**勾配消失問題**:
- 「勾配消失を解決」→ **ReLU**（Sigmoid/tanhは×）
- 「多層化で学習が進まない問題」→ 勾配消失問題、解決策はReLU
- 「Dying ReLU」→ ReLUの欠点だが、勾配消失よりマシ

**活性化関数の選択**:
- 「中間層でSigmoidを使う」→ ✗ 勾配消失で非推奨
- 「出力層でReLUを使う」→ △ タスク次第（分類には不適切）
- 「各クラスの確率（合計1）」→ 必ず**Softmax**（Sigmoidは独立確率）

**数値的特徴**:
- **Sigmoid**: 出力0～1、勾配最大0.25
- **tanh**: 出力-1～1、勾配最大1
- **ReLU**: 出力0～∞、勾配0 or 1（飽和しない）
- **Softmax**: 出力0～1（合計1）

### 頻出パターン

**問題文のキーワード**:
- 「勾配消失」「勾配が消失」「深い層で学習が進まない」→ ReLU
- 「多層化する上での問題」「誤差逆伝播における問題」→ 勾配消失問題
- 「各クラスに予測される確率」「確率の合計が1」→ Softmax
- 「2値分類」「0～1の確率」→ Sigmoid

**選択肢の判別**:
| 選択肢 | 中間層 | 出力層（分類） | 勾配消失対策 |
|--------|--------|---------------|-------------|
| **ReLU** | ◎ 推奨 | △ 場合による | ◎ Yes |
| **Sigmoid** | ✗ 非推奨 | ○ 2値分類 | ✗ No |
| **tanh** | △ 古い | ✗ 不適切 | ✗ No |
| **Softmax** | ✗ 不適切 | ◎ 多クラス | - |
| **線形** | ✗ 不適切 | ○ 回帰 | - |

## 補足
- **実務的観点**：
  - 画像分類（ImageNet等）では最終層にSoftmax + クロスエントロピーが標準
  - 中間層はReLUファミリー（ReLU, Leaky ReLU, PReLU, ELU）が主流
  - 深い層では勾配消失対策（Batch Normalization, Residual接続）が重要
  - Softmaxの温度パラメータで確率分布の尖り度を調整可能（知識蒸留等で利用）
- **関連トピック**：
  - [誤差逆伝播法](backpropagation.md) - 重み更新の詳細
  - [CNN](cnn.md) - 画像向けの特殊構造
  - [RNN](rnn.md) - 系列データ向けの再帰構造
  - [過学習対策](../05_machine_learning/overfitting_underfitting.md) - Dropout, 正則化
- **発展**：
  - Attention機構（Softmaxベース）
  - Swish, GELU等の新しい活性化関数
  - Label Smoothingによる過信頼の緩和

---

## プルーニング（Pruning）

### 要点
プルーニングは、ニューラルネットワークの不要な重み（パラメータ）やニューロンを削除してモデルを軽量化する技術。精度を維持しつつ、モデルサイズ削減・推論高速化・メモリ使用量削減を実現。エッジデバイスへのデプロイやリアルタイム処理に不可欠。

### 定義
プルーニング（枝刈り）とは、学習済みニューラルネットワークから重要度の低いパラメータ（重み、ニューロン、フィルタ等）を削除し、モデルを圧縮する手法。削除後は精度回復のためファインチューニングを行うのが一般的。生物の神経系における不要なシナプスの除去（シナプス刈り込み）に着想を得ている。

### 重要キーワード
- **プルーニング（Pruning）**: 不要なパラメータを削除してモデル圧縮
- **重みプルーニング（Weight Pruning）**: 個々の重みを0にする（構造化プルーニング/非構造化プルーニング）
- **ニューロンプルーニング**: ニューロン単位で削除
- **フィルタプルーニング**: CNNのフィルタ（チャネル）単位で削除
- **スパース性（Sparsity）**: 0要素の割合（例: 90%スパース = 10%のパラメータのみ残す）
- **マグニチュード（Magnitude）**: 重みの大きさ、削除基準に使用
- **ファインチューニング**: プルーニング後の精度回復訓練
- **構造化プルーニング**: ニューロン・フィルタ単位で削除、ハードウェア高速化しやすい
- **非構造化プルーニング**: 個別の重み単位で削除、高圧縮率だがハードウェア最適化困難

### 詳細

#### 背景と目的

**深層学習の課題**:
- **モデルが巨大**: ResNet50（25M パラメータ）、GPT-3（175B パラメータ）
- **計算コスト大**: 推論時間・メモリ使用量が膨大
- **デプロイ困難**: スマホ・IoTデバイス等のエッジ環境で動作困難

**プルーニングの目的**:
- **モデルサイズ削減**: ストレージ・メモリ使用量を削減
- **推論高速化**: 計算量削減により高速化
- **エネルギー効率**: 消費電力を削減
- **精度維持**: 重要なパラメータは残し、性能低下を最小化

**生物学的背景**:
人間の脳も発達過程でシナプス刈り込み（Synaptic Pruning）を行い、不要な結合を削除して効率化している。

#### プルーニングの種類

**1. 非構造化プルーニング（Unstructured Pruning）**

**手法**: 個別の重み単位で削除判定

**アルゴリズム**:
```
1. 学習済みモデルを用意
2. 各重み w の重要度を評価（通常は |w| の大きさ）
3. 重要度が低い重み（閾値以下）を0に設定
4. 残った重みでファインチューニング
5. 必要に応じて2～4を反復（反復的プルーニング）
```

**削除基準（マグニチュードベース）**:
- 重みの絶対値 $|w|$ が小さいものを削除
- 閾値設定: $|w| < \theta$ なら $w = 0$

**利点**:
- 高い圧縮率（90%以上のスパース性も可能）
- 実装が簡単
- 精度低下が少ない

**欠点**:
- 不規則なスパースパターン
- 通常のハードウェアで高速化困難（専用ライブラリ必要）
- メモリアクセスが非効率

**例**:
```
元の重み行列:
[0.8  0.1  -0.3]
[-0.05 0.9  0.2]
[0.4  -0.02 0.7]

閾値 0.1 でプルーニング後:
[0.8  0    -0.3]
[0    0.9  0.2]
[0.4  0    0.7]

スパース性: 4/9 ≈ 44%
```

---

**2. 構造化プルーニング（Structured Pruning）**

**手法**: ニューロン・フィルタ・チャネル単位で削除

**種類**:
- **ニューロンプルーニング**: 全結合層のニューロン削除
- **フィルタプルーニング**: CNNの畳み込みフィルタ削除
- **チャネルプルーニング**: 特徴マップのチャネル削除
- **層プルーニング**: 層全体を削除

**アルゴリズム（フィルタプルーニング）**:
```
1. 各フィルタの重要度を評価
   - L1ノルム: Σ|w| の合計
   - L2ノルム: √(Σw²)
   - 出力の活性化値の統計
2. 重要度の低いフィルタを削除
3. ネットワーク構造を調整（削除後のサイズに合わせる）
4. ファインチューニング
```

**利点**:
- **ハードウェア高速化**: 通常のライブラリでそのまま高速化
- **メモリ効率**: 実際にメモリ使用量が減る
- **実装が容易**: 既存フレームワークで扱いやすい

**欠点**:
- 圧縮率が低い（典型的には50～70%削減）
- 精度低下がやや大きい

**例（CNNのフィルタプルーニング）**:
```
元: 64フィルタ（3×3×3）の畳み込み層
↓ 重要度の低い32フィルタを削除
後: 32フィルタの畳み込み層

計算量: 約50%削減
パラメータ数: 約50%削減
```

---

#### プルーニングのタイミング

**1. 学習後プルーニング（Post-training Pruning）**
- 最も一般的
- 学習完了後にプルーニング → ファインチューニング

**2. 学習中プルーニング（During-training Pruning）**
- 学習中に段階的にプルーニング
- Dynamic Network Surgery等

**3. 学習前プルーニング**
- Lottery Ticket Hypothesis（宝くじ仮説）
- 初期化時点で既に「勝ちサブネットワーク」が存在

---

#### 実例と効果

**LeNet（手書き数字認識）**:
- 元: 431KB
- プルーニング後: 36KB（92%削減）
- 精度: ほぼ維持

**AlexNet（画像分類）**:
- 元: 240MB
- プルーニング後: 27MB（89%削減）
- Top-1精度: 57.2% → 57.0%（0.2%低下のみ）

**VGGNet**:
- 構造化プルーニングで計算量を最大5倍削減
- 精度低下: 1%未満

**MobileNet（エッジ向け）**:
- 元々軽量だが、さらにプルーニングで30～50%削減可能

---

#### プルーニングの実践的手順

**典型的なワークフロー**:
```
1. 【学習】
   通常通りモデルを訓練
   
2. 【プルーニング】
   - 重要度評価（マグニチュード等）
   - 低重要度パラメータを削除
   - スパース性: 50% → 70% → 90% と段階的に
   
3. 【ファインチューニング】
   - 残ったパラメータで再訓練
   - エポック数: 元の10～30%程度
   
4. 【評価】
   - 精度・速度・サイズを測定
   - 目標未達なら2に戻る
   
5. 【デプロイ】
   - 軽量化されたモデルを実環境へ
```

**反復的プルーニング**:
```
初期: 100%パラメータ、精度90%
↓ プルーニング（20%削除）+ ファインチューニング
第1回: 80%パラメータ、精度89.5%
↓ プルーニング（20%削除）+ ファインチューニング
第2回: 64%パラメータ、精度89%
↓ プルーニング（20%削除）+ ファインチューニング
第3回: 51%パラメータ、精度88%
```

---

#### 他のモデル圧縮技術との比較

| 手法 | 圧縮率 | 精度維持 | 推論高速化 | 実装難易度 |
|------|-------|---------|-----------|-----------|
| **プルーニング** | 高（50～90%） | 良 | 構造化で高速 | 中 |
| **量子化** | 中（50～75%） | 非常に良 | 高速 | 低 |
| **知識蒸留** | 高（任意） | 良 | 高速 | 高 |
| **低ランク分解** | 中（30～50%） | 良 | 中速 | 高 |

**併用が効果的**:
- プルーニング + 量子化: 最大95%以上の圧縮
- プルーニング + 知識蒸留: 精度とサイズの両立

---

### 試験での問われ方

#### 典型設問
- **「プルーニングの説明として最も適切」→ 不要な重み・ニューロンを削除してモデルを軽量化**
- **「プルーニングの目的」→ モデルサイズ削減、推論高速化**
- **「構造化プルーニング vs 非構造化プルーニング」→ ハードウェア高速化 vs 高圧縮率**
- プルーニングとドロップアウトの違い
- プルーニングの効果（精度・サイズ・速度）

#### ひっかけポイントと違いの整理

**混同注意**:
- **プルーニング vs ドロップアウト**:
  - プルーニング: 学習後に永久削除、モデル圧縮目的
  - ドロップアウト: 学習中に一時的に無効化、過学習対策
  
- **プルーニング vs 量子化**:
  - プルーニング: パラメータ削除（0にする）
  - 量子化: パラメータの精度削減（32bit → 8bit等）

- **構造化 vs 非構造化**:
  - 構造化: ニューロン・フィルタ単位、ハードウェア高速化可
  - 非構造化: 個別の重み単位、高圧縮率

**出題パターン**:
- 「不要なパラメータを削除」→ **プルーニング**
- 「モデルを軽量化してエッジデバイスにデプロイ」→ **プルーニング、量子化**
- 「学習中にニューロンを一時的に無効化」→ **ドロップアウト**（プルーニングではない）
- 「重みの大きさ（マグニチュード）で削除判定」→ **プルーニングの典型的手法**

**選択肢で出やすい記述**:
| 記述 | プルーニング | その他の技術 |
|------|------------|------------|
| 不要な重み・ニューロンを削除 | ◎ 正解 | - |
| モデルサイズを削減 | ◎ 正解 | 量子化、知識蒸留も |
| 学習中にランダムに無効化 | ✗ 不正解 | ドロップアウト |
| 重みの精度を下げる | ✗ 不正解 | 量子化 |
| 小さいモデルに知識を転移 | ✗ 不正解 | 知識蒸留 |
| 生物の枝刈りに着想 | ○ 補足説明 | - |

---

### 補足

#### 実務観点

**適用場面**:
- **エッジデバイス**: スマホ、IoT、組み込みシステム
- **リアルタイム処理**: 自動運転、ロボット制御
- **コスト削減**: クラウド推論コストの削減
- **プライバシー**: オンデバイス処理（データを外部送信しない）

**ツール・ライブラリ**:
- **TensorFlow Model Optimization**: TensorFlowの公式ツール
- **PyTorch Pruning**: torch.nn.utils.prune
- **NVIDIA TensorRT**: 推論最適化（プルーニング+量子化）
- **OpenVINO**: Intelの推論エンジン

**実装例（PyTorch）**:
```python
import torch.nn.utils.prune as prune

# 非構造化プルーニング（L1ノルムベース）
prune.l1_unstructured(module, name='weight', amount=0.5)

# 構造化プルーニング（フィルタ単位）
prune.ln_structured(module, name='weight', amount=0.3, n=2, dim=0)

# グローバルプルーニング（全層まとめて）
parameters_to_prune = [
    (model.conv1, 'weight'),
    (model.conv2, 'weight'),
]
prune.global_unstructured(
    parameters_to_prune,
    pruning_method=prune.L1Unstructured,
    amount=0.5,
)
```

**課題と対策**:
- **精度低下**: 反復的プルーニング、ファインチューニング期間延長
- **層ごとの最適化**: 各層で異なるプルーニング率を設定
- **自動化**: AutoML、Neural Architecture Search（NAS）との組み合わせ

#### 最新動向

**Lottery Ticket Hypothesis（2019）**:
- ランダム初期化された大規模ネットワーク内に、単独で高精度を達成できる小さなサブネットワーク（宝くじ）が存在
- 適切な初期化で小さいモデルも大きいモデル並みの性能

**動的プルーニング**:
- 推論時に入力に応じて動的にパラメータを選択
- 計算量を適応的に調整

**プルーニング + 量子化**:
- 両技術の併用で95%以上の圧縮
- TinyML（極小デバイスでのML）の実現

#### 関連トピック
- [ドロップアウト](../05_machine_learning/overfitting_underfitting.md) - 過学習対策との違い
- [CNN](cnn.md) - フィルタプルーニングの対象
- [モデル最適化](neural_network_basics.md) - 量子化、知識蒸留等