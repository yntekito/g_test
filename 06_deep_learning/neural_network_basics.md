# ニューラルネットワーク基礎

## 要点
- 入力層・中間層・出力層で構成され、重み・バイアス・活性化関数で非線形変換を実現。
- 中間層ではReLU等、出力層は2値分類でSigmoid、多クラス分類で**Softmax**を使用。
- 誤差逆伝播法で重み更新し、損失関数（クロスエントロピー等）を最小化。

## 定義
ニューラルネットワークは、生物の神経細胞を模した計算モデルで、複数の層（レイヤー）を持ち、各層のユニット（ニューロン）が重み付き結合で接続される。各ユニットは入力の線形結合に活性化関数を適用し、非線形な表現能力を獲得する。

## 重要キーワード
- **ニューロン（ユニット）**: 入力を受け取り、重み付き和+バイアスに活性化関数を適用して出力。
- **層（Layer）**: 入力層、中間層（隠れ層）、出力層で構成。
- **重み（Weight）**: 入力信号の重要度を調整するパラメータ。
- **バイアス（Bias）**: 閾値調整のための定数項。
- **活性化関数（Activation Function）**: 非線形変換を行う関数（ReLU, Sigmoid, tanh, Softmax等）。
- **Softmax関数**: 多クラス分類の出力層で各クラスの確率を計算（合計1）。
- **Sigmoid関数**: 2値分類の出力層で確率を計算（0～1）。
- **ReLU（Rectified Linear Unit）**: 中間層で広く使われる活性化関数（$\max(0, x)$）。
- **損失関数（Loss Function）**: 予測と正解のズレを定量化（クロスエントロピー、平均二乗誤差等）。
- **誤差逆伝播法（Backpropagation）**: 損失関数の勾配を逆向きに伝播し重みを更新。
- **全結合層（Fully Connected Layer）**: 前層の全ユニットと結合。

## 詳細

### 背景と直観
ニューラルネットワークは1943年のMcCulloch-Pittsモデルに起源を持ち、1980年代の誤差逆伝播法の発見により実用化が進みました。深層学習の基礎となる構造です。

**基本的な考え方**：
- 単純なユニットを多層に組み合わせることで複雑な関数を近似
- 学習データから重みを自動調整し、入力→出力の写像を獲得

### ニューラルネットワークの構造

```
入力層      中間層1     中間層2     出力層
  x1 ──┐
       ├→ h1 ──┐
  x2 ──┤       ├→ h3 ──┐
       ├→ h2 ──┘       ├→ y1
  x3 ──┘               ├→ y2
                       └→ y3
```

**各層の役割**
- **入力層**: 特徴量を受け取る
- **中間層**: 特徴の階層的抽出・変換
- **出力層**: タスクに応じた予測値を出力

### ユニットの計算

1. **線形結合**：$z = \sum_{i=1}^{n} w_i x_i + b$
2. **活性化**：$a = f(z)$

ここで $w_i$ は重み、$b$ はバイアス、$f$ は活性化関数。

### 主要な活性化関数

#### 1. ReLU（中間層で主流）
$$\text{ReLU}(x) = \max(0, x)$$
- **利点**: 勾配消失問題の緩和、計算高速
- **欠点**: Dying ReLU（負の入力で勾配0）

#### 2. Sigmoid（2値分類の出力層）
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$
- **出力**: 0～1（確率として解釈）
- **欠点**: 勾配消失（中間層では非推奨）

#### 3. Softmax（多クラス分類の出力層）★重要
$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

**特徴**
- $K$ クラスの各スコア $z_1, ..., z_K$ を確率に変換
- 全クラスの確率の合計が1：$\sum_{i=1}^{K} p_i = 1$
- 指数関数により最大値を強調（温度パラメータで調整可能）

**具体例**
入力スコア: $[2.0, 1.0, 0.1]$
```
exp(2.0) ≈ 7.39
exp(1.0) ≈ 2.72
exp(0.1) ≈ 1.11
合計 = 11.22

確率:
クラス0: 7.39/11.22 ≈ 0.659
クラス1: 2.72/11.22 ≈ 0.242
クラス2: 1.11/11.22 ≈ 0.099
```

#### 4. tanh（中間層で使用）
$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
- **出力**: -1～1（Sigmoidより勾配が大きい）

### 活性化関数の使い分け

| タスク | 出力層の活性化 | 損失関数 |
|--------|---------------|----------|
| 2値分類 | **Sigmoid** | 2値クロスエントロピー |
| 多クラス分類 | **Softmax** | カテゴリカルクロスエントロピー |
| 回帰 | 線形（恒等関数） | 平均二乗誤差 |
| 中間層 | **ReLU** / Leaky ReLU | - |

### 損失関数と学習

**クロスエントロピー損失（多クラス分類）**
$$L = -\sum_{i=1}^{K} y_i \log(\hat{y}_i)$$
- $y_i$: 正解ラベル（one-hot）
- $\hat{y}_i$: Softmaxの出力確率

**最適化**
- 誤差逆伝播法で各重みの勾配を計算
- SGD, Adam等のオプティマイザで更新
- $w := w - \eta \frac{\partial L}{\partial w}$

### 図解：順伝播と逆伝播

```
[順伝播]
入力 → 線形変換 → 活性化 → ... → 出力 → 損失計算

[逆伝播]
損失 → ∂L/∂output → ∂L/∂weights ← 連鎖律 ← 各層
```

## 試験での問われ方
- **典型設問**：
  - 「多クラス分類で確率を出力する関数は？」→ **Softmax**
  - 「中間層で最も使われる活性化関数は？」→ **ReLU**
  - 「2値分類の出力層は？」→ **Sigmoid**
- **比較されやすい概念**：
  - **Softmax** vs **Sigmoid**: 多クラス vs 2値、合計1 vs 独立確率
  - **ReLU** vs **Sigmoid**: 中間層（勾配消失なし）vs 出力層（確率）
  - **Softmax** vs **線形出力**: 分類 vs 回帰
- **引っ掛けポイント**：
  - 「各クラスに予測される確率」→ 必ず **Softmax**（Sigmoidは2値のみ）
  - Softmaxの合計が1であることを確認
  - ReLUを出力層で使うと負値が出ない（回帰で不適切な場合あり）
  - 勾配消失問題（Sigmoid/tanh）と対策（ReLU系）
- **頻出パターン**：
  - 出力層の活性化関数と損失関数の組み合わせ
  - 各活性化関数の出力範囲（0～1、-1～1、0～∞）
  - Softmaxの数式と正規化（合計1）の理解

## 補足
- **実務的観点**：
  - 画像分類（ImageNet等）では最終層にSoftmax + クロスエントロピーが標準
  - 中間層はReLUファミリー（ReLU, Leaky ReLU, PReLU, ELU）が主流
  - 深い層では勾配消失対策（Batch Normalization, Residual接続）が重要
  - Softmaxの温度パラメータで確率分布の尖り度を調整可能（知識蒸留等で利用）
- **関連トピック**：
  - [誤差逆伝播法](backpropagation.md) - 重み更新の詳細
  - [CNN](cnn.md) - 画像向けの特殊構造
  - [RNN](rnn.md) - 系列データ向けの再帰構造
  - [過学習対策](../05_machine_learning/overfitting_underfitting.md) - Dropout, 正則化
- **発展**：
  - Attention機構（Softmaxベース）
  - Swish, GELU等の新しい活性化関数
  - Label Smoothingによる過信頼の緩和