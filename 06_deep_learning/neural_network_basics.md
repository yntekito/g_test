# ニューラルネットワーク基礎

## 要点
- 入力層・中間層・出力層で構成され、重み・バイアス・活性化関数で非線形変換を実現。
- 中間層ではReLU等、出力層は2値分類でSigmoid、多クラス分類で**Softmax**を使用。
- 誤差逆伝播法で重み更新し、損失関数（クロスエントロピー等）を最小化。

## 定義
ニューラルネットワークは、生物の神経細胞を模した計算モデルで、複数の層（レイヤー）を持ち、各層のユニット（ニューロン）が重み付き結合で接続される。各ユニットは入力の線形結合に活性化関数を適用し、非線形な表現能力を獲得する。

## 重要キーワード
- **ニューロン（ユニット）**: 入力を受け取り、重み付き和+バイアスに活性化関数を適用して出力。
- **層（Layer）**: 入力層、中間層（隠れ層）、出力層で構成。
- **重み（Weight）**: 入力信号の重要度を調整するパラメータ。
- **バイアス（Bias）**: 閾値調整のための定数項。
- **活性化関数（Activation Function）**: 非線形変換を行う関数（ReLU, Sigmoid, tanh, Softmax等）。
- **Softmax関数**: 多クラス分類の出力層で各クラスの確率を計算（合計1）。
- **Sigmoid関数**: 2値分類の出力層で確率を計算（0～1）。
- **ReLU（Rectified Linear Unit）**: 中間層で広く使われる活性化関数（$\max(0, x)$）。
- **損失関数（Loss Function）**: 予測と正解のズレを定量化（クロスエントロピー、平均二乗誤差等）。
- **誤差逆伝播法（Backpropagation）**: 損失関数の勾配を逆向きに伝播し重みを更新。
- **全結合層（Fully Connected Layer）**: 前層の全ユニットと結合。

## 詳細

### 背景と直観
ニューラルネットワークは1943年のMcCulloch-Pittsモデルに起源を持ち、1980年代の誤差逆伝播法の発見により実用化が進みました。深層学習の基礎となる構造です。

**基本的な考え方**：
- 単純なユニットを多層に組み合わせることで複雑な関数を近似
- 学習データから重みを自動調整し、入力→出力の写像を獲得

### ニューラルネットワークの構造

```
入力層      中間層1     中間層2     出力層
  x1 ──┐
       ├→ h1 ──┐
  x2 ──┤       ├→ h3 ──┐
       ├→ h2 ──┘       ├→ y1
  x3 ──┘               ├→ y2
                       └→ y3
```

**各層の役割**
- **入力層**: 特徴量を受け取る
- **中間層**: 特徴の階層的抽出・変換
- **出力層**: タスクに応じた予測値を出力

### ユニットの計算

1. **線形結合**：$z = \sum_{i=1}^{n} w_i x_i + b$
2. **活性化**：$a = f(z)$

ここで $w_i$ は重み、$b$ はバイアス、$f$ は活性化関数。

### 主要な活性化関数

#### 勾配消失問題と解決策

**勾配消失問題**:
ニューラルネットワークを多層化（深層化）すると、誤差逆伝播時に勾配が層を遡るごとに小さくなり、初期層の重みがほとんど更新されなくなる現象。特にSigmoidやtanhなどの**飽和する活性化関数**で顕著。

**原因**:
- **Sigmoid/tanhの勾配**: 入力が大きい（または小さい）とき、勾配が0に近づく
  - Sigmoid: 勾配の最大値は0.25、入力が±5を超えると勾配≈0
  - tanh: 勾配の最大値は1、入力が±3を超えると勾配≈0
- **連鎖律**: 各層の勾配を掛け合わせるため、0.25 × 0.25 × ... → 指数的に減少
- **結果**: 深い層では学習が進まず、性能が頭打ち

**解決策: ReLU系活性化関数の登場**

---

#### 1. ReLU（Rectified Linear Unit）★最重要
$$\text{ReLU}(x) = \max(0, x) = \begin{cases} x & (x > 0) \\ 0 & (x \leq 0) \end{cases}$$

**勾配**:
$$\frac{d}{dx}\text{ReLU}(x) = \begin{cases} 1 & (x > 0) \\ 0 & (x \leq 0) \end{cases}$$

**勾配消失問題を解決する理由**:
- **正の領域で勾配が常に1**: 飽和しない、勾配が減衰しない
- **線形性**: 正の領域では $f(x) = x$ と同じ、計算が高速
- **スパース性**: 負の入力は0になり、一部のニューロンのみ活性化（効率的）

**利点**:
- 勾配消失を大幅に緩和（深い層でも学習可能）
- 計算が単純で高速（max演算のみ）
- 生物学的妥当性（ニューロンの発火パターンに近い）
- 2012年AlexNet以降、深層学習の標準に

**欠点**:
- **Dying ReLU問題**: 負の入力で勾配が0になり、一度死んだニューロンは復活しない
  - 学習率が大きすぎると多数のニューロンが死ぬ
  - 対策: Leaky ReLU、PReLU、ELU等

**実例（勾配の伝播）**:
```
10層ニューラルネットワークの勾配伝播:

【Sigmoid使用】
各層の勾配: 0.25 × 0.25 × ... × 0.25 (10回)
= 0.25^10 ≈ 0.0000001
→ 初期層の勾配がほぼ0、学習不可

【ReLU使用】
各層の勾配: 1 × 1 × ... × 1 (正の領域)
= 1
→ 勾配が減衰しない、全層で学習可能
```

---

#### 1-2. ReLU系の改良版

---

**Leaky ReLU（リーキー ReLU）**★試験頻出

**定義**:
$$\text{Leaky ReLU}(x) = \begin{cases} x & (x > 0) \\ \alpha x & (x \leq 0) \end{cases}$$

ここで $\alpha$ は小さな定数（通常**0.01**）

**勾配**:
$$\frac{d}{dx}\text{Leaky ReLU}(x) = \begin{cases} 1 & (x > 0) \\ \alpha & (x \leq 0) \end{cases}$$

**特徴と目的**:
- **Dying ReLU問題の解決**: ReLUの最大の欠点である「負の領域で勾配が完全に0になる問題」を解決
- **負の領域にも小さな勾配**: $\alpha$ により負の入力でも勾配が伝播（通常 $\alpha = 0.01$）
- **計算が高速**: ReLUと同様に簡単な演算のみ
- **スパース性は低下**: ReLUと異なり、負の値も0にならない

**Dying ReLU問題とは**:
ReLUでは $x \leq 0$ のとき出力も勾配も0になるため、一度負の領域に入ったニューロンは以降の学習で重みが更新されず「死んだ」状態になる。特に：
- 学習率が大きすぎる場合
- 初期化が不適切な場合
- バッチ正規化がない場合

に多数のニューロンが死ぬことがある。

**Leaky ReLUの解決策**:
負の領域でも $\alpha x$（例: $0.01x$）の小さな出力と勾配 $\alpha$ を保持するため、完全に死ぬことを防ぐ。

**数値例**:
```
入力:  [-2.0, -1.0,  0,  1.0,  2.0]

【ReLU】
出力:  [ 0,    0,    0,  1.0,  2.0]
勾配:  [ 0,    0,  0/1,  1,    1  ]
→ 負の入力では勾配0、学習不可

【Leaky ReLU（α=0.01）】
出力:  [-0.02, -0.01, 0,  1.0,  2.0]
勾配:  [ 0.01,  0.01, 1,  1,    1  ]
→ 負の入力でも小さな勾配、学習可能
```

**グラフ比較**:
```
      ReLU              Leaky ReLU
      |                    |
    2 |     ／            2|     ／
    1 |    ／             1|    ／
y   0 |___               0|__／  (傾き0.01)
   -1 |                 -1|／
      +------- x          +------- x
     -2  0  2            -2  0  2
```

**利点**:
- Dying ReLU問題を緩和
- 勾配消失問題も解決（ReLU同様）
- 計算コスト低い（ReLUとほぼ同じ）
- 実装が簡単

**欠点**:
- スパース性の低下（全ニューロンが活性化）
- 効果が限定的な場合もある（PReLUやELUの方が良いケースも）
- $\alpha$ のハイパーパラメータ調整が必要

**実務での使用**:
- ReLUでDying ReLU問題が発生した場合の代替
- 画像認識、物体検出等で広く使用（ResNet、YOLO等）
- $\alpha = 0.01$ が標準的な設定

---

**PReLU（Parametric ReLU / パラメトリック ReLU）**

**定義**:
$$\text{PReLU}(x) = \begin{cases} x & (x > 0) \\ \alpha x & (x \leq 0) \end{cases}$$

**Leaky ReLUとの違い**:
- Leaky ReLU: $\alpha$ は固定（通常0.01）
- PReLU: $\alpha$ は**学習可能なパラメータ**（勾配降下法で更新）

**特徴**:
- 各ニューロン（またはチャネル）ごとに異なる $\alpha$ を持つことが可能
- データに最適な $\alpha$ を自動学習
- パラメータ数が若干増加（各ニューロンに1つの $\alpha$）

**利点**: Leaky ReLUより柔軟、精度向上の可能性
**欠点**: 過学習リスク増加、実装がやや複雑

**実例**: 
ImageNetコンペティションで使用され、ReLUより高精度を達成（PReLU論文, 2015）

---

**ELU（Exponential Linear Unit）**

**定義**:
$$\text{ELU}(x) = \begin{cases} x & (x > 0) \\ \alpha(e^x - 1) & (x \leq 0) \end{cases}$$

通常 $\alpha = 1.0$

**特徴**:
- 負の領域で**指数関数的に滑らか**
- 出力の平均が0に近づく（収束が速い）
- ノイズに頑健

**利点**: 学習の収束が速い、精度向上
**欠点**: 指数関数の計算コスト（ReLU/Leaky ReLUより遅い）

---

**ReLU系活性化関数の比較**

| 活性化関数 | 正の領域 | 負の領域 | 勾配（負） | 計算速度 | Dying ReLU対策 | 用途 |
|-----------|---------|---------|-----------|---------|---------------|------|
| **ReLU** | $x$ | $0$ | $0$ | ◎ 最速 | ✗ なし | デフォルト |
| **Leaky ReLU** | $x$ | $0.01x$ | $0.01$ | ◎ 高速 | ○ あり | Dying ReLU発生時 |
| **PReLU** | $x$ | $\alpha x$ | $\alpha$ (学習) | ○ 高速 | ○ あり | 高精度重視 |
| **ELU** | $x$ | $\alpha(e^x-1)$ | $\alpha e^x$ | △ 中速 | ○ あり | 収束速度重視 |

**推奨順**:
1. **ReLU**: まず試す（最も標準的）
2. **Leaky ReLU**: Dying ReLU問題が発生したら
3. **PReLU/ELU**: さらなる精度向上が必要な場合

---

**試験での問われ方（Leaky ReLU）**

**典型的な出題パターン**:
1. **定義の確認**: 「Leaky ReLUの式は？」→ $\max(0.01x, x)$ または条件分岐の形
2. **目的**: 「Leaky ReLUは何の問題を解決するか」→ **Dying ReLU問題**
3. **負の領域の処理**: 「負の入力に対する出力は？」→ **αx（通常0.01x）**
4. **勾配の特徴**: 「負の領域での勾配は？」→ **α（通常0.01）**、0ではない
5. **ReLUとの違い**: 「ReLUと比較した利点は？」→ **負の領域でも勾配が伝播**

**ひっかけポイント（混同しやすい記述）**:

| 記述 | 正誤 | 補足 |
|------|------|------|
| 「負の入力で出力が完全に0」 | ✗ | ReLUの特徴、Leaky ReLUは $\alpha x$ |
| 「負の入力で小さな正の値を出力」 | ✗ | 負の入力 → 負の出力（$\alpha x < 0$） |
| 「負の入力で勾配が0」 | ✗ | ReLUの問題点、Leaky ReLUは勾配 $\alpha$ |
| 「負の入力でも勾配が伝播」 | ◎ 正解 | Leaky ReLUの最大の特徴 |
| 「Dying ReLU問題を解決」 | ◎ 正解 | 開発の主目的 |
| 「αは通常0.01」 | ◎ 正解 | 標準的な設定値 |
| 「正の領域では恒等関数」 | ◎ 正解 | $f(x) = x$、ReLUと同じ |

**選択肢で出やすい正解パターン**:
- ✅ 「負の入力に対して、小さな負の勾配を持つ」
- ✅ 「負の領域で $\alpha x$（$\alpha$ は小さな定数）を出力」
- ✅ 「Dying ReLU問題を緩和する」
- ✅ 「負の入力でも学習が進む」

**選択肢で出やすい誤答パターン**:
- ❌ 「負の入力で出力が0になる」→ ReLUの特徴
- ❌ 「負の入力で勾配が0になる」→ ReLUの問題点
- ❌ 「勾配消失問題を引き起こす」→ 逆、解決する側
- ❌ 「出力が0～1の範囲に制限される」→ Sigmoidの特徴

**比較問題でのポイント**:
- **ReLU vs Leaky ReLU**: 「負の領域での挙動」が決定的な違い
  - ReLU: 出力0、勾配0 → Dying ReLU
  - Leaky ReLU: 出力 $\alpha x$、勾配 $\alpha$ → 学習継続
- **Leaky ReLU vs PReLU**: $\alpha$ が固定 vs 学習可能
- **Leaky ReLU vs ELU**: 線形 vs 指数関数（滑らか）

---

#### 2. Sigmoid（2値分類の出力層専用）
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

**勾配**:
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

**特徴**:
- **出力**: 0～1（確率として解釈）
- **勾配**: 最大0.25（x=0のとき）、±5で≈0

**用途**: 2値分類の出力層のみ

**中間層では非推奨の理由**:
- 勾配消失問題が深刻
- 出力が0中心でない（学習の収束が遅い）
- 指数関数の計算コスト

---

#### 3. Softmax（多クラス分類の出力層）★重要
$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

**特徴**
- $K$ クラスの各スコア $z_1, ..., z_K$ を確率に変換
- 全クラスの確率の合計が1：$\sum_{i=1}^{K} p_i = 1$
- 指数関数により最大値を強調（温度パラメータで調整可能）

**具体例**
入力スコア: $[2.0, 1.0, 0.1]$
```
exp(2.0) ≈ 7.39
exp(1.0) ≈ 2.72
exp(0.1) ≈ 1.11
合計 = 11.22

確率:
クラス0: 7.39/11.22 ≈ 0.659
クラス1: 2.72/11.22 ≈ 0.242
クラス2: 1.11/11.22 ≈ 0.099
```

**使い方**: 最終層で適用し、クロスエントロピー損失と組み合わせる

---

#### 4. tanh（中間層、現在はReLUが主流）
$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

**勾配**:
$$\tanh'(x) = 1 - \tanh^2(x)$$

**特徴**:
- **出力**: -1～1（0中心、Sigmoidより良い）
- **勾配**: 最大1（x=0のとき）、±3で≈0

**Sigmoidより優れる点**:
- 0中心の出力（学習の収束が速い）
- 勾配がやや大きい

**ReLUに劣る点**:
- 勾配消失問題は残る
- 計算コストが高い

---

### 活性化関数の比較と使い分け

#### 中間層での選択

| 活性化関数 | 勾配消失 | 計算速度 | 推奨度 | 備考 |
|-----------|---------|---------|--------|------|
| **ReLU** | ◎ なし | ◎ 高速 | ★★★ | 第一選択 |
| **Leaky ReLU** | ◎ なし | ◎ 高速 | ★★☆ | Dying ReLU対策 |
| **ELU** | ◎ なし | △ 中速 | ★☆☆ | 滑らかさ重視 |
| **tanh** | ✗ あり | △ 中速 | ☆☆☆ | RNN等で限定的 |
| **Sigmoid** | ✗ 深刻 | △ 中速 | ✗ 非推奨 | 出力層のみ |

**推奨**:
1. **デフォルト**: ReLU
2. **Dying ReLU問題が発生**: Leaky ReLU、PReLU
3. **RNNの内部ゲート**: tanh、Sigmoid（特殊用途）

#### 出力層での選択

| タスク | 活性化関数 | 損失関数 | 出力範囲 |
|--------|-----------|----------|---------|
| 2値分類 | **Sigmoid** | 2値クロスエントロピー | 0～1 |
| 多クラス分類 | **Softmax** | カテゴリカルクロスエントロピー | 0～1（合計1） |
| 回帰 | 線形（恒等） | 平均二乗誤差（MSE） | -∞～+∞ |
| 正の値のみ回帰 | ReLU/Softplus | MSE | 0～+∞ |

### 損失関数と学習

**クロスエントロピー損失（多クラス分類）**
$$L = -\sum_{i=1}^{K} y_i \log(\hat{y}_i)$$
- $y_i$: 正解ラベル（one-hot）
- $\hat{y}_i$: Softmaxの出力確率

**最適化**
- 誤差逆伝播法で各重みの勾配を計算
- SGD, Adam等のオプティマイザで更新
- $w := w - \eta \frac{\partial L}{\partial w}$

### 図解：順伝播と逆伝播

```
[順伝播]
入力 → 線形変換 → 活性化 → ... → 出力 → 損失計算

[逆伝播]
損失 → ∂L/∂output → ∂L/∂weights ← 連鎖律 ← 各層
```

## 試験での問われ方

### 典型設問
- **「勾配消失問題を解決する活性化関数は？」→ ReLU**
- **「中間層で最も使われる活性化関数は？」→ ReLU**
- **「多クラス分類で確率を出力する関数は？」→ Softmax**
- **「2値分類の出力層は？」→ Sigmoid**
- **「誤差逆伝播で勾配消失を防ぐ役割を持つ活性化関数」→ ReLU**

### 比較されやすい概念

**勾配消失問題関連**:
- **Sigmoid/tanh vs ReLU**: 飽和する（勾配消失） vs 飽和しない（勾配1）
- **深い層での学習**: Sigmoid/tanhは困難、ReLUは可能
- **歴史**: 2012年AlexNetでReLUが標準化、深層学習ブームの一因

**活性化関数の役割別**:
- **中間層**: ReLU系（勾配消失対策）
- **出力層（分類）**: Sigmoid（2値）、Softmax（多クラス）
- **出力層（回帰）**: 線形

**Softmax vs Sigmoid**:
- **Softmax**: 多クラス、合計1、相互依存
- **Sigmoid**: 2値、独立確率、各出力が独立

### 引っ掛けポイント

**勾配消失問題**:
- 「勾配消失を解決」→ **ReLU**（Sigmoid/tanhは×）
- 「多層化で学習が進まない問題」→ 勾配消失問題、解決策はReLU
- 「Dying ReLU」→ ReLUの欠点だが、勾配消失よりマシ

**活性化関数の選択**:
- 「中間層でSigmoidを使う」→ ✗ 勾配消失で非推奨
- 「出力層でReLUを使う」→ △ タスク次第（分類には不適切）
- 「各クラスの確率（合計1）」→ 必ず**Softmax**（Sigmoidは独立確率）

**数値的特徴**:
- **Sigmoid**: 出力0～1、勾配最大0.25
- **tanh**: 出力-1～1、勾配最大1
- **ReLU**: 出力0～∞、勾配0 or 1（飽和しない）
- **Softmax**: 出力0～1（合計1）

### 頻出パターン

**問題文のキーワード**:
- 「勾配消失」「勾配が消失」「深い層で学習が進まない」→ ReLU
- 「多層化する上での問題」「誤差逆伝播における問題」→ 勾配消失問題
- 「各クラスに予測される確率」「確率の合計が1」→ Softmax
- 「2値分類」「0～1の確率」→ Sigmoid

---

### G検定選択肢問題の見極めポイント★試験直前必読

#### Q. 活性化関数の説明として、最も適切なものを1つ選べ

**正解となる選択肢の典型例**:
- ✅ 「ニューロンの出力に**非線形性**を与える関数」
- ✅ 「線形結合の結果を**非線形変換**して次の層へ伝える」
- ✅ 「ネットワークに**複雑なパターン**を学習させるための関数」
- ✅ 「各ユニットの**重み付き和**に適用し、出力を決定する関数」

**誤答となる選択肢とその正体**:
| 誤選択肢の記述 | 正体 | 見極めキーワード |
|---------------|------|----------------|
| 「**入力データを正規化**する関数」 | **Batch Normalization** | 正規化、平均0、分散1 |
| 「**重みパラメータを更新**する関数」 | **最適化アルゴリズム**（SGD、Adam） | 更新、最適化、勾配降下 |
| 「**誤差を計算**する関数」 | **損失関数**（Loss Function） | 誤差、損失、正解との差 |
| 「**過学習を防ぐ**関数」 | **正則化**（Dropout、L2） | 過学習、汎化、正則化 |
| 「**学習率を調整**する関数」 | **学習率スケジューラ** | 学習率、スケジュール |
| 「**勾配を計算**する関数」 | **誤差逆伝播法** | 勾配計算、微分 |

#### 混同しやすい概念の完全対比表

| 概念 | 役割 | 適用タイミング | 具体例 | キーワード |
|------|------|--------------|--------|-----------|
| **活性化関数** | 非線形変換 | 各層の出力計算時 | ReLU, Sigmoid, Softmax | 非線形、出力変換 |
| **損失関数** | 誤差評価 | 学習時（目的関数） | クロスエントロピー、MSE | 誤差、損失、評価 |
| **最適化手法** | 重み更新 | 学習時（勾配計算後） | SGD, Adam, RMSprop | 更新、最適化、勾配降下 |
| **正規化層** | データ分布調整 | 各層の入力正規化 | Batch Norm, Layer Norm | 正規化、平均、分散 |
| **正則化** | 過学習抑制 | 学習全体 | Dropout, L2正則化 | 過学習、汎化、ペナルティ |
| **誤差逆伝播** | 勾配計算 | 学習時（損失から逆向き） | Backpropagation | 勾配、連鎖律、逆伝播 |

#### 選択肢のパターン別攻略法

**パターン1: 活性化関数の「目的」を問う**
```
Q. 活性化関数の主な役割は何か？

✅ 正解: 「非線形変換により複雑な関数を近似可能にする」
❌ 誤答: 「学習を高速化する」→ 最適化手法の役割
❌ 誤答: 「過学習を防ぐ」→ 正則化の役割
❌ 誤答: 「勾配を計算する」→ 誤差逆伝播の役割
```

**パターン2: 活性化関数が「ない場合」の問題**
```
Q. 活性化関数を使わない場合の問題は？

✅ 正解: 「多層でも線形変換の合成にしかならない」
✅ 正解: 「1層のネットワークと等価になる」
❌ 誤答: 「学習が全く進まない」→ 極端すぎる
❌ 誤答: 「過学習が起きる」→ 逆、表現力が低下
```

**パターン3: 具体的な活性化関数の特徴**
```
Q. ReLUの説明として正しいものは？

✅ 正解: 「正の領域で勾配が1、勾配消失を緩和」
✅ 正解: 「max(0, x)で計算が高速」
❌ 誤答: 「出力が0～1に制限される」→ Sigmoidの特徴
❌ 誤答: 「確率を出力する」→ Softmaxの特徴
```

**パターン4: 用途別の使い分け**
```
Q. 多クラス分類の出力層で使う活性化関数は？

✅ 正解: 「Softmax」
❌ 誤答: 「ReLU」→ 中間層向き
❌ 誤答: 「Sigmoid」→ 2値分類向き
❌ 誤答: 「tanh」→ 出力層には不適切
```

#### 試験本番での判定フローチャート

```
活性化関数の問題を見たら：

1. 「非線形」「非線形変換」のキーワードがあるか？
   Yes → 正解候補
   No  → 次へ

2. 以下のキーワードがあるか？
   - 「正規化」「平均0」 → Batch Norm（誤答）
   - 「更新」「最適化」 → 最適化手法（誤答）
   - 「誤差」「損失」 → 損失関数（誤答）
   - 「過学習」「汎化」 → 正則化（誤答）
   
3. 「中間層」「出力層」の指定はあるか？
   - 中間層 → ReLU系が正解
   - 出力層（分類） → Softmax/Sigmoidが正解
   - 出力層（回帰） → 線形が正解
```

**選択肢の判別**:
| 選択肢 | 中間層 | 出力層（分類） | 勾配消失対策 |
|--------|--------|---------------|-------------|
| **ReLU** | ◎ 推奨 | △ 場合による | ◎ Yes |
| **Sigmoid** | ✗ 非推奨 | ○ 2値分類 | ✗ No |
| **tanh** | △ 古い | ✗ 不適切 | ✗ No |
| **Softmax** | ✗ 不適切 | ◎ 多クラス | - |
| **線形** | ✗ 不適切 | ○ 回帰 | - |

## 補足
- **実務的観点**：
  - 画像分類（ImageNet等）では最終層にSoftmax + クロスエントロピーが標準
  - 中間層はReLUファミリー（ReLU, Leaky ReLU, PReLU, ELU）が主流
  - 深い層では勾配消失対策（Batch Normalization, Residual接続）が重要
  - Softmaxの温度パラメータで確率分布の尖り度を調整可能（知識蒸留等で利用）
- **関連トピック**：
  - [誤差逆伝播法](backpropagation.md) - 重み更新の詳細
  - [CNN](cnn.md) - 画像向けの特殊構造
  - [RNN](rnn.md) - 系列データ向けの再帰構造
  - [過学習対策](../05_machine_learning/overfitting_underfitting.md) - Dropout, 正則化
- **発展**：
  - Attention機構（Softmaxベース）
  - Swish, GELU等の新しい活性化関数
  - Label Smoothingによる過信頼の緩和

---

## プルーニング（Pruning）

### 要点
プルーニングは、ニューラルネットワークの不要な重み（パラメータ）やニューロンを削除してモデルを軽量化する技術。精度を維持しつつ、モデルサイズ削減・推論高速化・メモリ使用量削減を実現。エッジデバイスへのデプロイやリアルタイム処理に不可欠。

### 定義
プルーニング（枝刈り）とは、学習済みニューラルネットワークから重要度の低いパラメータ（重み、ニューロン、フィルタ等）を削除し、モデルを圧縮する手法。削除後は精度回復のためファインチューニングを行うのが一般的。生物の神経系における不要なシナプスの除去（シナプス刈り込み）に着想を得ている。

### 重要キーワード
- **プルーニング（Pruning）**: 不要なパラメータを削除してモデル圧縮
- **重みプルーニング（Weight Pruning）**: 個々の重みを0にする（構造化プルーニング/非構造化プルーニング）
- **ニューロンプルーニング**: ニューロン単位で削除
- **フィルタプルーニング**: CNNのフィルタ（チャネル）単位で削除
- **スパース性（Sparsity）**: 0要素の割合（例: 90%スパース = 10%のパラメータのみ残す）
- **マグニチュード（Magnitude）**: 重みの大きさ、削除基準に使用
- **ファインチューニング**: プルーニング後の精度回復訓練
- **構造化プルーニング**: ニューロン・フィルタ単位で削除、ハードウェア高速化しやすい
- **非構造化プルーニング**: 個別の重み単位で削除、高圧縮率だがハードウェア最適化困難

### 詳細

#### 背景と目的

**深層学習の課題**:
- **モデルが巨大**: ResNet50（25M パラメータ）、GPT-3（175B パラメータ）
- **計算コスト大**: 推論時間・メモリ使用量が膨大
- **デプロイ困難**: スマホ・IoTデバイス等のエッジ環境で動作困難

**プルーニングの目的**:
- **モデルサイズ削減**: ストレージ・メモリ使用量を削減
- **推論高速化**: 計算量削減により高速化
- **エネルギー効率**: 消費電力を削減
- **精度維持**: 重要なパラメータは残し、性能低下を最小化

**生物学的背景**:
人間の脳も発達過程でシナプス刈り込み（Synaptic Pruning）を行い、不要な結合を削除して効率化している。

#### プルーニングの種類

**1. 非構造化プルーニング（Unstructured Pruning）**

**手法**: 個別の重み単位で削除判定

**アルゴリズム**:
```
1. 学習済みモデルを用意
2. 各重み w の重要度を評価（通常は |w| の大きさ）
3. 重要度が低い重み（閾値以下）を0に設定
4. 残った重みでファインチューニング
5. 必要に応じて2～4を反復（反復的プルーニング）
```

**削除基準（マグニチュードベース）**:
- 重みの絶対値 $|w|$ が小さいものを削除
- 閾値設定: $|w| < \theta$ なら $w = 0$

**利点**:
- 高い圧縮率（90%以上のスパース性も可能）
- 実装が簡単
- 精度低下が少ない

**欠点**:
- 不規則なスパースパターン
- 通常のハードウェアで高速化困難（専用ライブラリ必要）
- メモリアクセスが非効率

**例**:
```
元の重み行列:
[0.8  0.1  -0.3]
[-0.05 0.9  0.2]
[0.4  -0.02 0.7]

閾値 0.1 でプルーニング後:
[0.8  0    -0.3]
[0    0.9  0.2]
[0.4  0    0.7]

スパース性: 4/9 ≈ 44%
```

---

**2. 構造化プルーニング（Structured Pruning）**

**手法**: ニューロン・フィルタ・チャネル単位で削除

**種類**:
- **ニューロンプルーニング**: 全結合層のニューロン削除
- **フィルタプルーニング**: CNNの畳み込みフィルタ削除
- **チャネルプルーニング**: 特徴マップのチャネル削除
- **層プルーニング**: 層全体を削除

**アルゴリズム（フィルタプルーニング）**:
```
1. 各フィルタの重要度を評価
   - L1ノルム: Σ|w| の合計
   - L2ノルム: √(Σw²)
   - 出力の活性化値の統計
2. 重要度の低いフィルタを削除
3. ネットワーク構造を調整（削除後のサイズに合わせる）
4. ファインチューニング
```

**利点**:
- **ハードウェア高速化**: 通常のライブラリでそのまま高速化
- **メモリ効率**: 実際にメモリ使用量が減る
- **実装が容易**: 既存フレームワークで扱いやすい

**欠点**:
- 圧縮率が低い（典型的には50～70%削減）
- 精度低下がやや大きい

**例（CNNのフィルタプルーニング）**:
```
元: 64フィルタ（3×3×3）の畳み込み層
↓ 重要度の低い32フィルタを削除
後: 32フィルタの畳み込み層

計算量: 約50%削減
パラメータ数: 約50%削減
```

---

#### プルーニングのタイミング

**1. 学習後プルーニング（Post-training Pruning）**
- 最も一般的
- 学習完了後にプルーニング → ファインチューニング

**2. 学習中プルーニング（During-training Pruning）**
- 学習中に段階的にプルーニング
- Dynamic Network Surgery等

**3. 学習前プルーニング**
- Lottery Ticket Hypothesis（宝くじ仮説）
- 初期化時点で既に「勝ちサブネットワーク」が存在

---

#### 実例と効果

**LeNet（手書き数字認識）**:
- 元: 431KB
- プルーニング後: 36KB（92%削減）
- 精度: ほぼ維持

**AlexNet（画像分類）**:
- 元: 240MB
- プルーニング後: 27MB（89%削減）
- Top-1精度: 57.2% → 57.0%（0.2%低下のみ）

**VGGNet**:
- 構造化プルーニングで計算量を最大5倍削減
- 精度低下: 1%未満

**MobileNet（エッジ向け）**:
- 元々軽量だが、さらにプルーニングで30～50%削減可能

---

#### プルーニングの実践的手順

**典型的なワークフロー**:
```
1. 【学習】
   通常通りモデルを訓練
   
2. 【プルーニング】
   - 重要度評価（マグニチュード等）
   - 低重要度パラメータを削除
   - スパース性: 50% → 70% → 90% と段階的に
   
3. 【ファインチューニング】
   - 残ったパラメータで再訓練
   - エポック数: 元の10～30%程度
   
4. 【評価】
   - 精度・速度・サイズを測定
   - 目標未達なら2に戻る
   
5. 【デプロイ】
   - 軽量化されたモデルを実環境へ
```

**反復的プルーニング**:
```
初期: 100%パラメータ、精度90%
↓ プルーニング（20%削除）+ ファインチューニング
第1回: 80%パラメータ、精度89.5%
↓ プルーニング（20%削除）+ ファインチューニング
第2回: 64%パラメータ、精度89%
↓ プルーニング（20%削除）+ ファインチューニング
第3回: 51%パラメータ、精度88%
```

---

#### 他のモデル圧縮技術との比較

| 手法 | 圧縮率 | 精度維持 | 推論高速化 | 実装難易度 |
|------|-------|---------|-----------|-----------|
| **プルーニング** | 高（50～90%） | 良 | 構造化で高速 | 中 |
| **量子化** | 中（50～75%） | 非常に良 | 高速 | 低 |
| **知識蒸留** | 高（任意） | 良 | 高速 | 高 |
| **低ランク分解** | 中（30～50%） | 良 | 中速 | 高 |

**併用が効果的**:
- プルーニング + 量子化: 最大95%以上の圧縮
- プルーニング + 知識蒸留: 精度とサイズの両立

---

### 試験での問われ方

#### 典型的な出題パターン

**パターン1：プルーニングの説明として最も適切な選択肢を選べ**

✅ **適切な選択肢（正しい説明）**：
- **「不要な重みやニューロンを削除してモデルを軽量化する手法」** → ✅ **最も適切**
- **「学習済みモデルから重要度の低いパラメータを削除する」** → ✅ 適切
- **「精度を維持しながらモデルサイズを削減できる」** → ✅ 適切
- **「推論速度の向上とメモリ使用量の削減を実現する」** → ✅ 適切
- **「構造化プルーニングと非構造化プルーニングがある」** → ✅ 適切
- **「削除後はファインチューニングで精度を回復させる」** → ✅ 適切
- **「生物の神経系のシナプス刈り込みに着想を得ている」** → ✅ 適切
- **「エッジデバイスへのデプロイに有効」** → ✅ 適切

❌ **不適切な選択肢（誤った説明、試験で狙われる）**：
- **「データ拡張の一種である」** → ❌ 不適切（モデル圧縮手法、データ拡張とは無関係）
- **「過学習を防ぐために訓練データを削減する手法」** → ❌ 不適切（パラメータを削減、データは削減しない）
- **「学習率を調整する最適化手法である」** → ❌ 不適切（重み削除手法、最適化とは別）
- **「Dropoutと同じ手法である」** → ❌ 不適切（類似だが別物、Dropoutは訓練時のランダム無効化）
- **「量子化と同じ手法である」** → ❌ 不適切（別のモデル圧縮手法、精度削減 vs パラメータ削除）
- **「学習前に適用する必要がある」** → ❌ 不適切（通常は学習後に適用）
- **「必ず精度が向上する」** → ❌ 不適切（軽量化が目的、精度はやや低下する場合がある）

---

**パターン2：構造化プルーニングと非構造化プルーニングの比較**

> 「構造化プルーニングの特徴として、最も適切な選択肢を1つ選べ。」

✅ **適切な選択肢**：
- **「ニューロンやフィルタ単位で削除し、ハードウェア高速化しやすい」** → ✅ **最も適切**
- **「通常のライブラリでそのまま高速化できる」** → ✅ 適切
- **「実際にメモリ使用量が減る」** → ✅ 適切

❌ **不適切な選択肢**：
- **「非構造化プルーニングより高い圧縮率を達成できる」** → ❌ 不適切（逆、非構造化の方が高圧縮）
- **「個別の重み単位で削除する」** → ❌ 不適切（非構造化プルーニングの特徴）
- **「専用ライブラリが必要」** → ❌ 不適切（非構造化プルーニングの欠点）

---

**パターン3：プルーニングと他の手法の混同**

| 手法 | 何を削減・変更するか | 主な効果 | 試験での区別ポイント |
|------|-------------------|---------|-------------------|
| **プルーニング** | パラメータ数を削減 | モデルサイズ・計算量削減 | 「重み削除」「枝刈り」 |
| **量子化** | パラメータの精度を削減 | メモリ1/4（FP32→INT8） | 「精度削減」「8bit」 |
| **Dropout** | 訓練時にランダム無効化 | 過学習防止 | 「訓練時のみ」「ランダム」 |
| **データ拡張** | 訓練データを増やす | 過学習防止・精度向上 | 「データ操作」「回転・反転」 |
| **知識蒸留** | 大モデル→小モデルに知識転移 | 小モデルで高精度 | 「教師・生徒」「ソフトターゲット」 |

**ひっかけポイント**：
- ❌ 「プルーニング = Dropout」→ 別物（プルーニングは永久削除、Dropoutは一時的）
- ❌ 「プルーニング = 量子化」→ 別物（削除 vs 精度削減）
- ❌ 「プルーニング = データ削減」→ 誤り（パラメータ削減）

---

**パターン4：プルーニングの適用タイミング**

✅ **正しい理解**：
- 一般的には**学習後**にプルーニング
- プルーニング後は**ファインチューニング**で精度回復
- **反復的プルーニング**：段階的に削除率を上げる

❌ **誤った理解**：
- 学習前に適用が必須 → 誤り（学習後が一般的）
- ファインチューニング不要 → 誤り（通常必要）
- 一度に90%削除 → 非推奨（段階的が良い）

---

**パターン5：頻出の数値・具体例**

**覚えておくべき数値**：
- **AlexNet**: 89%削減（240MB → 27MB）
- **典型的なスパース性**: 50～90%
- **構造化プルーニングの圧縮率**: 50～70%
- **非構造化プルーニングの圧縮率**: 70～90%以上

---

**補足：実務的観点**
- スマホ・IoTデバイスへのデプロイで必須技術
- TensorFlow Lite、PyTorch Mobileが対応
- 構造化プルーニングが実用的（ハードウェア高速化容易）
- 量子化と併用で95%以上の圧縮も可能

---

## 量子化（Quantization）★試験頻出

### 要点
量子化は、ニューラルネットワークのパラメータや演算の精度を削減してモデルを圧縮する技術。32bit浮動小数点から8bit整数への変換が一般的で、メモリ使用量を1/4に削減し、推論を高速化。精度低下が小さく実装が容易なため、エッジデバイスへのデプロイで広く利用される。

### 定義
**量子化（Quantization）**とは、ニューラルネットワークの重みや活性化値を低精度（低ビット幅）の数値表現に変換し、モデルサイズと計算量を削減する手法。通常は32bit浮動小数点（FP32）から8bit整数（INT8）への変換が行われる。

**数値例**:
```
【量子化前】FP32（32bit）
重み: 0.123456789... （高精度）
メモリ: 4 bytes/パラメータ

↓ 量子化

【量子化後】INT8（8bit）
重み: 31 （-128～127の整数）
メモリ: 1 byte/パラメータ

→ メモリ使用量が1/4に削減
```

### 重要キーワード
- **量子化（Quantization）**: パラメータの精度削減でモデル圧縮
- **FP32（32bit浮動小数点）**: 標準的な学習・推論の数値精度
- **INT8（8bit整数）**: 量子化後の代表的な精度、-128～127の整数
- **FP16（16bit浮動小数点）**: 中間的な精度、混合精度学習で使用
- **量子化誤差**: 精度削減による表現力の低下
- **量子化認識学習（QAT）**: 学習時に量子化を考慮して誤差を軽減
- **学習後量子化（PTQ）**: 学習済みモデルを直接量子化
- **スケールファクター**: 浮動小数点と整数を対応づける係数
- **ゼロポイント**: 量子化時のオフセット値

### 詳細

#### 量子化の種類

**1. 学習後量子化（PTQ: Post-Training Quantization）**

**特徴**:
- 学習済みモデルを直接量子化
- 再学習不要、数分～数時間で完了
- 実装が簡単

**手順**:
```
1. 学習済みFP32モデルを用意
2. 重みの範囲を計算（最小値・最大値）
3. INT8に線形マッピング
4. キャリブレーション（代表データで活性化値の範囲を測定）
5. 量子化完了
```

**利点**:
- ✅ 追加学習不要
- ✅ 高速実装
- ✅ フレームワークのサポート充実

**欠点**:
- ❌ 精度低下が比較的大きい（1～5%程度）
- ❌ 複雑なモデルでは性能劣化

**2. 量子化認識学習（QAT: Quantization-Aware Training）**

**特徴**:
- 学習時に量子化をシミュレート
- 量子化誤差を学習で補正
- 精度低下が最小

**手順**:
```
1. 順伝播時に量子化をシミュレート
   重み → 量子化 → 逆量子化 → 演算
2. 逆伝播は浮動小数点で実施
3. 重みを更新（量子化誤差を考慮）
4. 学習完了後、完全にINT8化
```

**利点**:
- ✅ 精度低下が非常に小さい（<1%）
- ✅ 複雑なモデルでも高精度

**欠点**:
- ❌ 学習時間が長い
- ❌ 実装が複雑

#### 量子化の仕組み

**線形量子化（Affine Quantization）**:
$$Q = \text{round}\left(\frac{x}{\text{scale}}\right) + \text{zero\_point}$$

- $x$: 元の浮動小数点値
- $\text{scale}$: スケールファクター（$\frac{x_{\max} - x_{\min}}{255}$）
- $\text{zero\_point}$: ゼロポイント（0に対応する整数値）
- $Q$: 量子化後の整数値（-128～127）

**逆量子化（Dequantization）**:
$$x \approx \text{scale} \times (Q - \text{zero\_point})$$

**具体例**:
```
重みの範囲: -1.0 ～ 1.0
scale = (1.0 - (-1.0)) / 255 ≈ 0.00784
zero_point = 0

元の重み: 0.5
量子化: round(0.5 / 0.00784) + 0 = 64
逆量子化: 0.00784 × 64 ≈ 0.502（誤差: 0.002）
```

#### 量子化の効果

**1. メモリ削減**

| 精度 | ビット幅 | メモリ比 | 例（100万パラメータ） |
|------|---------|---------|-------------------|
| FP32 | 32bit | 1.0x | 4 MB |
| FP16 | 16bit | 0.5x | 2 MB |
| INT8 | 8bit | **0.25x** | **1 MB** |
| INT4 | 4bit | 0.125x | 0.5 MB |

**2. 推論高速化**

```
【INT8演算の利点】
- CPU/GPU: INT8演算はFP32の2～4倍高速
- 専用ハードウェア（TPU、Neural Engine）: さらに高速
- 消費電力: 大幅削減（バッテリー駆動デバイスに有利）
```

**3. 精度への影響**

| モデル | タスク | FP32精度 | INT8精度 | 精度低下 |
|--------|--------|---------|---------|---------|
| ResNet-50 | ImageNet | 76.1% | 75.8% | -0.3% |
| MobileNet | ImageNet | 71.0% | 70.5% | -0.5% |
| BERT | NLP | 90.5% | 89.8% | -0.7% |

→ 精度低下は通常**1%未満**（QAT使用時）

#### 量子化の対象

**1. 重み量子化**:
- 最も基本的、効果大
- メモリ削減が主目的

**2. 活性化値量子化**:
- 中間層の出力も量子化
- 推論高速化が主目的

**3. 全体量子化（Weight + Activation）**:
- 重みと活性化値の両方を量子化
- 最大の効果、実装が複雑

#### 実例

**1. スマホアプリでの画像認識**

```
【量子化前】
モデル: ResNet-50 (FP32)
サイズ: 98 MB
推論時間: 200ms
精度: 76.1%

【量子化後】
モデル: ResNet-50 (INT8)
サイズ: 25 MB（1/4削減）
推論時間: 50ms（4倍高速）
精度: 75.8%（-0.3%）
```

**2. IoTデバイスでの音声認識**

```
【量子化前】
モデル: Wav2Vec (FP32)
サイズ: 360 MB → メモリ不足
デプロイ不可

【量子化後】
モデル: Wav2Vec (INT8)
サイズ: 90 MB → デプロイ可能
推論時間: リアルタイム処理可能
```

---

## 知識蒸留（Knowledge Distillation）★試験頻出

### 要点
知識蒸留は、大きなモデル（教師モデル）から小さなモデル（生徒モデル）へ知識を転移する技術。教師の出力確率分布（ソフトターゲット）を利用して生徒を学習させることで、小さいモデルでも高精度を実現。モデル圧縮の中で精度低下が最も小さい手法の一つ。

### 定義
**知識蒸留（Knowledge Distillation）**とは、高精度な大規模モデル（教師モデル/Teacher Model）の知識を、軽量な小規模モデル（生徒モデル/Student Model）に転移する学習手法。教師モデルの出力する「ソフトな確率分布」を教師信号として、生徒モデルを学習させる。

**イメージ**:
```
【教師モデル】大規模（例: ResNet-152）
高精度だがサイズ大、推論遅い
        ↓ 知識を転移
【生徒モデル】小規模（例: MobileNet）
軽量で高速、教師に近い精度
```

### 重要キーワード
- **知識蒸留（Knowledge Distillation）**: 教師から生徒へ知識転移
- **教師モデル（Teacher Model）**: 大規模で高精度なモデル
- **生徒モデル（Student Model）**: 小規模で軽量なモデル
- **ソフトターゲット（Soft Target）**: 教師モデルの出力確率分布
- **ハードターゲット（Hard Target）**: 正解ラベル（one-hot）
- **温度パラメータ（Temperature）**: 確率分布の滑らかさを調整
- **蒸留損失（Distillation Loss）**: 教師の出力との一致度
- **暗黙知の転移**: クラス間の関係性等の暗黙的知識

### 詳細

#### 知識蒸留の仕組み

**標準的な学習（ハードターゲット）**:
```
入力画像: 犬の写真
正解ラベル: [0, 0, 1, 0]（one-hot、犬=1）
             猫 鳥 犬 馬
→ 犬=100%、他=0%の厳しい教師信号
```

**知識蒸留（ソフトターゲット）**:
```
教師モデルの出力:
  犬: 0.85
  猫: 0.10  ← ソフトな確率（犬と猫は似ている）
  鳥: 0.03
  馬: 0.02

→ クラス間の関係性も学習可能
```

**損失関数**:
$$L = \alpha \cdot L_{\text{distill}} + (1-\alpha) \cdot L_{\text{hard}}$$

- $L_{\text{distill}}$: 教師の出力との一致度（KLダイバージェンス）
- $L_{\text{hard}}$: 正解ラベルとの一致度（クロスエントロピー）
- $\alpha$: 蒸留損失の重み（通常0.5～0.9）

**温度パラメータ（Temperature）**:
$$p_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}$$

- $T=1$: 通常のSoftmax
- $T>1$: 確率分布が滑らか（小さい確率も学習）
- $T$が大きいほど暗黙知が伝わりやすい

**温度の効果**:
```
【T=1（通常）】
犬: 0.95, 猫: 0.03, 鳥: 0.01, 馬: 0.01
→ 尖った分布

【T=5（蒸留用）】
犬: 0.70, 猫: 0.20, 鳥: 0.05, 馬: 0.05
→ 滑らかな分布、クラス間関係を学習可能
```

#### 知識蒸留の手順

```
【ステップ1】教師モデルの準備
大規模モデルを高精度に学習
例: ResNet-152（精度80%）

【ステップ2】生徒モデルの設計
小規模アーキテクチャを選択
例: MobileNet-V2（パラメータ1/10）

【ステップ3】蒸留学習
温度T=5で教師・生徒の出力を計算
蒸留損失 + ハードターゲット損失で学習

【ステップ4】推論時
T=1に戻して生徒モデルのみ使用
軽量・高速・高精度を実現
```

#### 知識蒸留の効果

**精度比較**:
| モデル | パラメータ数 | 精度 | 推論時間 |
|--------|------------|------|---------|
| ResNet-152（教師） | 60M | 80.0% | 200ms |
| MobileNet（スクラッチ） | 4M | 71.0% | 20ms |
| MobileNet（蒸留） | 4M | **76.5%** | 20ms |

→ 蒸留により**5.5%の精度向上**（教師に近づく）

**転移される知識**:
1. **クラス間の関係性**: 犬と狼は似ている等
2. **決定境界の形状**: より滑らかな境界
3. **特徴の表現方法**: 効率的な中間表現
4. **汎化性能**: 未知データへの対応力

#### 知識蒸留の応用

**1. モデル圧縮**
```
BERT（340M params）→ DistilBERT（66M params）
- サイズ: 1/5削減
- 速度: 2倍高速
- 精度: -3%（97% → 94%の言語理解タスク）
```

**2. アンサンブル蒸留**
```
複数の教師モデル → 1つの生徒モデル
例: ResNet + DenseNet + EfficientNet → MobileNet
→ アンサンブルの高精度を単一モデルで実現
```

**3. クロスモダリティ蒸留**
```
教師: 画像 + テキストのマルチモーダルモデル
生徒: 画像のみのモデル
→ テキスト情報の恩恵を画像モデルへ転移
```

#### 実例

**1. スマホアプリの音声認識**

```
【教師】大規模音声認識モデル（クラウド）
サイズ: 500 MB
精度: 95%
レイテンシ: 通信遅延あり

↓ 知識蒸留

【生徒】軽量音声認識モデル（デバイス）
サイズ: 50 MB（1/10）
精度: 92%（-3%）
レイテンシ: 即座に応答、オフライン動作可
```

**2. リアルタイム物体検出**

```
【教師】Faster R-CNN（精度90%、20FPS）
↓ 知識蒸留
【生徒】YOLO-Tiny（精度85%、100FPS）
→ 自動運転、ドローン等で利用
```

---

### プルーニング・量子化・知識蒸留の比較★試験重要

| 手法 | 原理 | 圧縮率 | 精度維持 | 実装難易度 | 適用場面 |
|------|------|--------|---------|-----------|---------|
| **プルーニング** | 重み削除 | 高（50～90%） | 良 | 中 | 構造化で高速化 |
| **量子化** | 精度削減 | 中（75%） | **非常に良** | **低** | 汎用的、最も手軽 |
| **知識蒸留** | 知識転移 | 高（任意） | 良 | 高 | 新モデル設計 |

**使い分け**:
- **最も手軽**: 量子化（学習後量子化なら数分）
- **最高圧縮**: プルーニング（90%削減可能）
- **最高精度**: 知識蒸留（新しいアーキテクチャ設計可）
- **最強の組み合わせ**: プルーニング + 量子化 + 蒸留

**併用例**:
```
1. 大規模教師モデルを学習
2. 知識蒸留で小さい生徒モデルを作成
3. 生徒モデルをプルーニング（50%削減）
4. プルーニング後のモデルを量子化（INT8）
→ 合計で95%以上の圧縮、精度低下5%以内
```

---

### 試験での問われ方

#### 典型設問
- **「プルーニングの説明として最も適切」→ 不要な重み・ニューロンを削除してモデルを軽量化**
- **「プルーニングの目的」→ モデルサイズ削減、推論高速化**
- **「構造化プルーニング vs 非構造化プルーニング」→ ハードウェア高速化 vs 高圧縮率**
- プルーニングとドロップアウトの違い
- プルーニングの効果（精度・サイズ・速度）

#### ひっかけポイントと違いの整理

**混同注意**:
- **プルーニング vs ドロップアウト**:
  - プルーニング: 学習後に永久削除、モデル圧縮目的
  - ドロップアウト: 学習中に一時的に無効化、過学習対策
  
- **プルーニング vs 量子化**:
  - プルーニング: パラメータ削除（0にする）
  - 量子化: パラメータの精度削減（32bit → 8bit等）

- **構造化 vs 非構造化**:
  - 構造化: ニューロン・フィルタ単位、ハードウェア高速化可
  - 非構造化: 個別の重み単位、高圧縮率

**出題パターン**:
- 「不要なパラメータを削除」→ **プルーニング**
- 「モデルを軽量化してエッジデバイスにデプロイ」→ **プルーニング、量子化**
- 「学習中にニューロンを一時的に無効化」→ **ドロップアウト**（プルーニングではない）
- 「重みの大きさ（マグニチュード）で削除判定」→ **プルーニングの典型的手法**

**選択肢で出やすい記述**:
| 記述 | プルーニング | その他の技術 |
|------|------------|------------|
| 不要な重み・ニューロンを削除 | ◎ 正解 | - |
| モデルサイズを削減 | ◎ 正解 | 量子化、知識蒸留も |
| 学習中にランダムに無効化 | ✗ 不正解 | ドロップアウト |
| 重みの精度を下げる | ✗ 不正解 | 量子化 |
| 小さいモデルに知識を転移 | ✗ 不正解 | 知識蒸留 |
| 生物の枝刈りに着想 | ○ 補足説明 | - |

---

### 補足

#### 実務観点

**適用場面**:
- **エッジデバイス**: スマホ、IoT、組み込みシステム
- **リアルタイム処理**: 自動運転、ロボット制御
- **コスト削減**: クラウド推論コストの削減
- **プライバシー**: オンデバイス処理（データを外部送信しない）

**ツール・ライブラリ**:
- **TensorFlow Model Optimization**: TensorFlowの公式ツール
- **PyTorch Pruning**: torch.nn.utils.prune
- **NVIDIA TensorRT**: 推論最適化（プルーニング+量子化）
- **OpenVINO**: Intelの推論エンジン

**実装例（PyTorch）**:
```python
import torch.nn.utils.prune as prune

# 非構造化プルーニング（L1ノルムベース）
prune.l1_unstructured(module, name='weight', amount=0.5)

# 構造化プルーニング（フィルタ単位）
prune.ln_structured(module, name='weight', amount=0.3, n=2, dim=0)

# グローバルプルーニング（全層まとめて）
parameters_to_prune = [
    (model.conv1, 'weight'),
    (model.conv2, 'weight'),
]
prune.global_unstructured(
    parameters_to_prune,
    pruning_method=prune.L1Unstructured,
    amount=0.5,
)
```

**課題と対策**:
- **精度低下**: 反復的プルーニング、ファインチューニング期間延長
- **層ごとの最適化**: 各層で異なるプルーニング率を設定
- **自動化**: AutoML、Neural Architecture Search（NAS）との組み合わせ

#### 最新動向

**Lottery Ticket Hypothesis（2019）**:
- ランダム初期化された大規模ネットワーク内に、単独で高精度を達成できる小さなサブネットワーク（宝くじ）が存在
- 適切な初期化で小さいモデルも大きいモデル並みの性能

**動的プルーニング**:
- 推論時に入力に応じて動的にパラメータを選択
- 計算量を適応的に調整

**プルーニング + 量子化**:
- 両技術の併用で95%以上の圧縮
- TinyML（極小デバイスでのML）の実現

#### 関連トピック
- [ドロップアウト](../05_machine_learning/overfitting_underfitting.md) - 過学習対策との違い
- [CNN](cnn.md) - フィルタプルーニングの対象
- [モデル最適化](neural_network_basics.md) - 量子化、知識蒸留等

---

## スケーリング法則（Scaling Laws）

### 要点
- **スケーリング法則**: モデルサイズ（パラメーター数）、データセット規模、計算量の増加に伴い、AIモデルの性能が向上する法則。
- **根拠**: OpenAI（Kaplan et al., 2020）が大規模言語モデルで実証。パラメーター数と性能が**べき乗則**に従う。
- **実務的含意**: 計算資源を増やせば増やすほど性能向上が期待できるが、収穫逓減もある。大規模モデル（GPT、BERT等）の設計指針。

### 定義
**スケーリング法則（Scaling Laws）**は、ニューラルネットワークのパラメーター数 $N$、データセット規模 $D$、計算量（FLOPs）$C$ を増やすと、モデルの性能（損失の減少、精度の向上）が予測可能な形で向上することを示す経験則。

Kaplanら（2020）の発見:
$$
L(N) \propto N^{-\alpha}
$$
- $L$: 損失（Loss）
- $N$: パラメーター数
- $\alpha$: スケーリング指数（言語モデルで約0.076）

**キーポイント**:
- パラメーター数を10倍にすると、損失が約20-30%減少
- データ量、計算量も同様にべき乗則に従う

### 重要キーワード
- **スケーリング法則（Scaling Laws）**: モデル規模と性能の関係を示す法則
- **パラメーター数（Parameters）**: モデルの重みの総数（N）
- **計算量（Compute, FLOPs）**: 訓練に使用される浮動小数点演算の総数（C）
- **データセット規模（Dataset Size）**: 訓練データのトークン数・サンプル数（D）
- **べき乗則（Power Law）**: $y \propto x^{-\alpha}$ の形で表される関係
- **スケーリング指数（Scaling Exponent）**: べき乗の指数 $\alpha$
- **収穫逓減（Diminishing Returns）**: 投資を増やしても効果が徐々に小さくなる現象
- **最適配分（Optimal Allocation）**: 計算資源をモデルサイズとデータ量にどう配分するか

### 詳細

#### 背景：大規模化の時代

**歴史的経緯**:
- 2010年代後半から、深層学習モデルの大規模化が進行
- GPT-2（15億）→ GPT-3（1750億）→ GPT-4（推定1兆超）
- 「大きいほど良い」という経験則が実証的に確認される

**Kaplanらの研究（2020）**:
- OpenAIが言語モデルで系統的実験
- パラメーター数：1000〜160億の範囲で検証
- 性能とモデルサイズの関係が**べき乗則**に従うことを発見

#### スケーリング法則の3つの軸

**1. モデルサイズ（パラメーター数 N）**:
$$
L(N) \propto N^{-0.076}
$$
- パラメーター数を10倍 → 損失が約20%減少
- 100億パラメーターより1000億パラメーターの方が高性能

**2. データセット規模（D）**:
$$
L(D) \propto D^{-0.095}
$$
- データ量を10倍 → 損失が約25%減少
- より多くのデータで訓練するほど性能向上

**3. 計算量（C）**:
$$
L(C) \propto C^{-0.050}
$$
- 計算量を10倍 → 損失が約15%減少
- 長時間訓練（Epoch数増加）で性能向上

#### 最適配分の問題

**Chinchillaの法則（2022, DeepMind）**:
- Kaplanの法則を修正・拡張
- 「最適なモデルサイズとデータ量の比率」を提案
- **結論**: 従来のモデルはパラメーター数過多、データ量不足
- **最適比率**: $N \propto C^{0.5}$、$D \propto C^{0.5}$
- 例：1000億パラメーターのモデルには約2兆トークンが最適

**実例**:
| モデル | パラメーター数 | 訓練データ量 | 評価 |
|--------|--------------|------------|------|
| GPT-3 | 1750億 | 3000億トークン | データ不足 |
| Chinchilla | 700億 | 1.4兆トークン | **最適配分** |
| LLaMA | 700億 | 1.4兆トークン | Chinchilla準拠 |

#### 図解：スケーリング法則

```
損失（Loss）
  ↑
高 |     ╲
  |      ╲
  |       ╲        ← べき乗則に従って減少
  |        ╲
  |         ╲
  |          ╲_____ ← 収穫逓減（効果が小さくなる）
低 |_______________╲______
     小           大
       パラメーター数（N）
```

#### 実例：GPTシリーズの進化

| モデル | パラメーター数 | 訓練データ | 計算量 | 性能 |
|--------|--------------|----------|--------|------|
| GPT-1 | 1.2億 | 5GB | 小 | 基礎 |
| GPT-2 | 15億 | 40GB | 中 | 向上 |
| GPT-3 | 1750億 | 570GB | 大 | 大幅向上 |
| GPT-4 | 推定1兆超 | 推定10TB超 | 超大 | さらに向上 |

**観察**:
- パラメーター数が約10倍増加ごとに性能が段階的に向上
- 訓練コストも指数的に増加（GPT-3は推定1000万ドル超）

#### スケーリング法則の限界と課題

**1. 収穫逓減**:
- モデルサイズを2倍にしても性能は2倍にならない
- べき乗則の指数が小さい（-0.076）ため、改善幅は徐々に縮小

**2. コストの爆発**:
- 計算資源、電力、時間、費用が指数的に増加
- GPT-4の訓練コストは推定1億ドル超

**3. データの枯渇**:
- インターネット上の高品質データには限界
- Chinchilla法則を満たすデータ量の確保が困難

**4. 環境への影響**:
- 大規模訓練のCO2排出量が問題視
- GPT-3の訓練：約500トンのCO2排出（車の生涯排出量相当）

**5. アーキテクチャの重要性**:
- スケーリングだけでなく、効率的な設計（Transformer、MoE等）も重要
- Mixture of Experts（MoE）で計算効率を改善

### 試験での問われ方

#### 典型的な穴埋め問題

**問題例**: AIモデルのパラメーター数やデータセットのサイズ、トレーニングに使用される計算量が増えるにつれて、パフォーマンスが向上することを示す法則を（　）という。

**答え**: **スケーリング法則（Scaling Laws）**

**理由**:
- ✅ **「パラメーター数の増加」**: モデルサイズの拡大
- ✅ **「データセットのサイズ増加」**: 訓練データの増量
- ✅ **「計算量の増加」**: 訓練に使用する計算資源の増加
- ✅ **「パフォーマンスが向上」**: 損失減少、精度向上

この3つの要素（モデルサイズ、データ量、計算量）と性能の関係を示すのがスケーリング法則。

#### 関連する典型問題

**問**: スケーリング法則を最初に系統的に実証したのは？
- ✅ **正解**: OpenAI（Kaplanら、2020）

**問**: スケーリング法則で性能とモデルサイズの関係は？
- ✅ **正解**: べき乗則（Power Law）に従う
- ❌ 誤答: 線形関係
- ❌ 誤答: 指数関数的
- ❌ 誤答: 対数関数的

**問**: Chinchillaの法則が示したのは？
- ✅ **正解**: 最適なモデルサイズとデータ量の配分比率
- ❌ 誤答: パラメーター数のみが重要
- ❌ 誤答: データ量は無関係

**問**: スケーリング法則の限界として正しいのは？
- ✅ **正解**: 収穫逓減（効果が徐々に小さくなる）
- ✅ **正解**: 訓練コストの爆発的増加
- ✅ **正解**: データの枯渇問題

#### G検定での判定フロー

```
問題文を読む
    ↓
「パラメーター数」「データセット規模」「計算量」の3要素がある？
    ↓ YES → スケーリング関連
    ↓
「増加」→「性能向上」の関係を説明？
    ↓ YES → スケーリング法則（正解）
    ↓
「最適配分」「データ量とモデルサイズの比率」とある？
    ↓ YES → Chinchillaの法則
    ↓
「収穫逓減」「コスト増大」とある？
    ↓ YES → スケーリング法則の限界
```

#### 混同しやすい概念との対比

| 概念 | 内容 | 試験での問われ方 |
|------|------|----------------|
| **スケーリング法則** | モデル規模と性能の関係 | **「パラメーター数増加→性能向上」** |
| **ノーフリーランチ定理** | 万能アルゴリズムは存在しない | 「問題に応じた選択が必要」 |
| **過学習** | 訓練データに過剰適合 | 「汎化性能低下」 |
| **転移学習** | 事前学習の知識を活用 | 「少量データで高精度」 |
| **万能近似定理** | NNは任意の関数を近似可能 | 「理論上の能力」 |
| **ダブルディセント** | モデルサイズで性能が再向上 | 「補間閾値」 |

**重要**: スケーリング法則は「規模と性能の関係」を示す経験則、ノーフリーランチ定理は「問題依存性」を示す理論的結果。

### 補足：実務での活用

#### モデル開発の意思決定

**1. 計算資源の配分**:
- パラメーター数とデータ量のバランスを最適化
- Chinchilla法則に従い、データ収集にも投資

**2. ベンチマーク予測**:
- スケーリング法則から将来の性能を予測
- 開発コストと性能向上のトレードオフを評価

**3. 効率的な設計**:
- Mixture of Experts（MoE）で計算効率向上
- Sparse Attention等のアーキテクチャ工夫

#### 最新動向（2024-2026年）

**Emergent Abilities（創発能力）**:
- 一定規模を超えると新しい能力が突然出現
- 例：GPT-3で数学的推論、コード生成能力が急激に向上

**モデル圧縮との併用**:
- 大規模モデルを訓練後、プルーニング・量子化で圧縮
- 「Train Large, Deploy Small」戦略

**マルチモーダル化**:
- テキスト+画像+音声でのスケーリング
- より多様なデータでの性能向上

### 関連トピック
- [ニューラルネットワーク基礎](neural_network_basics.md)（本ファイル）
- [Transformer](transformer.md) - 大規模言語モデルの基盤
- [過学習と汎化](../05_machine_learning/overfitting_underfitting.md) - ダブルディセント
- [ノーフリーランチ定理](../05_machine_learning/supervised_learning.md) - 対比理解