# 誤差逆伝播法（Backpropagation）

## 要点
- ニューラルネットワークの学習アルゴリズム。出力層の誤差を入力層に向かって逆方向に伝播させ、各重みの勾配を計算。連鎖律を利用して効率的に計算。
- クレジット割り当て問題（どのパラメータが誤差に寄与しているか）を解決する手法。
- 1986年にRumelhartらが提案し、多層ニューラルネットワークの学習を実用化。深層学習の基盤技術。

## 定義
誤差逆伝播法（Backpropagation、バックプロパゲーション）とは、ニューラルネットワークにおいて、出力層で計算された誤差を入力層に向かって逆向きに伝播させることで、各層の重みに対する損失関数の勾配を効率的に計算する学習アルゴリズム。連鎖律（Chain Rule）を用いて実装される。

## 重要キーワード
- **誤差逆伝播**: 出力層から入力層へ誤差を伝える処理
- **連鎖律（Chain Rule）**: 合成関数の微分法則、$\frac{d}{dx}f(g(x)) = f'(g(x)) \cdot g'(x)$
- **勾配（Gradient）**: 各パラメータに対する損失関数の偏微分
- **勾配降下法**: 勾配に基づいて重みを更新する最適化手法
- **デルタ（δ）**: 各層の誤差項
- **順伝播（Forward Propagation）**: 入力から出力への計算
- **クレジット割り当て問題**: どのパラメータが誤差に寄与しているかを決定する問題
- **勾配消失問題**: 深い層で勾配が極端に小さくなる現象

---

## クレジット割り当て問題（Credit Assignment Problem）

### 要点
クレジット割り当て問題とは、ニューラルネットワークの出力誤差に対して、各パラメータ（重み・バイアス）がどの程度寄与しているかを決定する問題。誤差逆伝播法（バックプロパゲーション）により解決され、各層の重みに適切に誤差を割り当てることで学習が可能になる。

### 定義
クレジット割り当て問題（Credit Assignment Problem）とは、ニューラルネットワークにおいて、出力層で生じた誤差に対して、ネットワーク内のどのパラメータ（重み、バイアス）がどの程度責任を持つか（寄与しているか）を明らかにする問題。この問題を解決することで、各パラメータを適切に更新できる。

### 重要キーワード
- **クレジット割り当て**: 誤差の責任をパラメータに適切に配分すること
- **構造的クレジット割り当て**: どの重みが影響したかを決定（通常のNN）
- **時間的クレジット割り当て**: 時系列データでどの時点の入力が影響したかを決定（RNN等）
- **寄与度**: 各パラメータが誤差にどの程度関与しているか
- **BPTT（Backpropagation Through Time）**: 時間的クレジット割り当てを解決する手法

### 詳細

#### 背景
初期のパーセプトロンやニューラルネットワークでは、出力層の誤差をどのように隠れ層の重みに反映させるかが不明確でした。これがクレジット割り当て問題と呼ばれ、深層学習の大きな課題でした。1986年にRumelhartらが誤差逆伝播法を提案し、この問題を実用的に解決。

#### クレジット割り当て問題の本質

**問題の具体例**:
```
入力層 → 隠れ層1 → 隠れ層2 → 出力層
x     →  w1,h1  →  w2,h2  →  y（誤差発生）

問い: この誤差に対して
- w1（入力層→隠れ層1の重み）はどの程度責任があるか？
- w2（隠れ層1→隠れ層2の重み）はどの程度責任があるか？
- 各重みをどの方向にどれだけ更新すべきか？
```

**困難さ**:
- **複数層**: 多数の重みが複雑に絡み合っている
- **非線形**: 活性化関数により線形的な分析が困難
- **組み合わせ爆発**: すべての重みの組み合わせを試すのは不可能

#### 誤差逆伝播法による解決

**基本アイデア**:
1. **順伝播**: 入力から出力まで計算し、誤差を求める
2. **逆伝播**: 出力層から入力層に向かって誤差を伝播
3. **勾配計算**: 連鎖律を使い、各重みに対する誤差の勾配を計算
4. **重み更新**: 勾配に基づいて各重みを更新

#### 図解（クレジット割り当て）
```
順伝播（Forward）:
入力 → [w1] → 隠れ層 → [w2] → 出力 → 誤差計算
x              h              y       L = (y - t)²

逆伝播（Backward）でクレジット割り当て:
       ← ∂L/∂w1 ← ∂L/∂h ← ∂L/∂y ←
       
各重みへの寄与度（勾配）を計算:
- w1の寄与: ∂L/∂w1 = ... （連鎖律で計算）
- w2の寄与: ∂L/∂w2 = ... （連鎖律で計算）

更新:
w1 ← w1 - η・∂L/∂w1
w2 ← w2 - η・∂L/∂w2
```

#### クレジット割り当ての種類

**1. 構造的クレジット割り当て（Structural Credit Assignment）**:
- **問題**: どの重み・どの層が誤差に寄与しているか
- **解決**: 誤差逆伝播法
- **対象**: 通常のフィードフォワードニューラルネットワーク

**2. 時間的クレジット割り当て（Temporal Credit Assignment）**:
- **問題**: 時系列データで、過去のどの時点の入力が現在の出力に影響しているか
- **解決**: BPTT（Backpropagation Through Time）
- **対象**: RNN、LSTM等
- **課題**: 勾配消失・爆発問題

---

## 誤差逆伝播法の詳細

### 数学的仕組み

損失関数 $L$ の重み $w_{ij}^{(l)}$ に対する偏微分を連鎖律で計算：

$$\frac{\partial L}{\partial w_{ij}^{(l)}} = \frac{\partial L}{\partial z_j^{(l)}} \cdot \frac{\partial z_j^{(l)}}{\partial w_{ij}^{(l)}}$$

ここで：
- $z_j^{(l)}$: 第 $l$ 層の第 $j$ ユニットへの入力
- $\delta_j^{(l)} = \frac{\partial L}{\partial z_j^{(l)}}$: 誤差項（デルタ）

誤差項の逆伝播：
$$\delta_i^{(l-1)} = \sum_j w_{ij}^{(l)} \delta_j^{(l)} \cdot f'(z_i^{(l-1)})$$

重みの更新：
$$w_{ij}^{(l)} \leftarrow w_{ij}^{(l)} - \eta \frac{\partial L}{\partial w_{ij}^{(l)}}$$

### アルゴリズムの流れ

```
1. 初期化: 重みをランダムに設定
2. 順伝播（Forward Pass）:
   - 入力から出力まで順に計算
   - 各層の出力を保存（逆伝播で使用）
3. 損失計算: L = (y - t)²（二乗誤差の例）
4. 逆伝播（Backward Pass）:
   - 出力層の誤差項を計算: δ_out = ∂L/∂y
   - 各層を逆向きに辿り、誤差項を計算
   - 各重みの勾配を計算: ∂L/∂w
5. 重み更新: w ← w - η・∂L/∂w
6. 収束まで2-5を繰り返し
```

### 図解（アルゴリズム全体）
```
訓練データ (x, t)
     ↓
┌─────────────────┐
│  順伝播         │
│  x → h → y      │ 各層の出力を保存
└─────────────────┘
     ↓
  誤差計算: L = (y - t)²
     ↓
┌─────────────────┐
│  逆伝播         │
│  ∂L/∂w ← ← ←   │ 連鎖律で勾配計算
└─────────────────┘
     ↓
  重み更新: w ← w - η・∂L/∂w
     ↓
   繰り返し
```

---

## 試験での問われ方

### 典型設問
- **「どのパラメータが誤差を生じさせているかを明らかにする問題」→クレジット割り当て問題**
- **「クレジット割り当て問題を解決する手法」→誤差逆伝播法**
- 誤差逆伝播法のアルゴリズム
- 連鎖律の役割
- 勾配消失問題との関係

### ひっかけポイントと違いの整理

| 概念 | 説明 | 関係 |
|------|------|------|
| **クレジット割り当て問題** | どのパラメータが誤差に寄与しているか | 問題の定義 |
| **誤差逆伝播法** | クレジット割り当てを解決する手法 | 解決方法 |
| **勾配降下法** | パラメータを更新する最適化手法 | 更新アルゴリズム |
| **連鎖律** | 合成関数の微分法則 | 数学的基礎 |

**混同注意**:
- **クレジット割り当て問題 vs 誤差逆伝播法**: 前者は問題、後者は解決手法
- **順伝播 vs 逆伝播**: 順伝播は予測計算、逆伝播は勾配計算
- **勾配消失問題**: クレジット割り当てが困難になる現象（深い層で勾配が消える）
- **勾配爆発問題**: クレジット割り当て時に勾配が極端に大きくなる現象

**出題パターン**:
- 「どのパラメータが誤差に影響しているかを明らかにする問題」→**クレジット割り当て問題**
- 「クレジット割り当て問題を解決する手法」→**誤差逆伝播法（Backpropagation）**
- 「時系列データでの過去の影響を決定」→**時間的クレジット割り当て問題**
- 「連鎖律を用いて勾配を計算」→**誤差逆伝播法**

---

## 補足

### 実務観点
- **学習の本質**: クレジット割り当てが適切にできることが学習成功の鍵
- **深層学習の課題**: 層が深いほどクレジット割り当てが困難（勾配消失）
- **計算効率**: 順伝播と逆伝播の計算量はほぼ同等
- **自動微分**: PyTorch、TensorFlow等は自動的に誤差逆伝播を実装

### 解決策（勾配消失対策）
- **Batch Normalization**: 各層の入力を正規化
- **Residual Connection（ResNet）**: スキップ結合で勾配を直接伝播
- **LSTM/GRU**: ゲート機構で時間的クレジット割り当てを改善
- **ReLU**: シグモイドより勾配消失が少ない

### 歴史的重要性
- **1986年**: Rumelhart, Hinton, Williamsが誤差逆伝播法を提案
- **インパクト**: クレジット割り当て問題の実用的解決により、多層ニューラルネットワークの学習が可能に
- **第3次AIブーム**: 深層学習の基礎技術として不可欠

### 関連する問題
- **勾配消失問題**: 深い層でクレジット割り当てが困難
- **勾配爆発問題**: 勾配が指数的に増大
- **長期依存性問題**: RNNでの時間的クレジット割り当ての困難

### 関連トピック
- [ニューラルネットワーク基礎](neural_network_basics.md) - 基本構造
- [LSTM/GRU](lstm_gru.md) - 時間的クレジット割り当ての改善
- [最適化](../10_math_statistics/optimization.md) - 勾配降下法
- [CNN](cnn.md) - 畳み込み層での誤差逆伝播
