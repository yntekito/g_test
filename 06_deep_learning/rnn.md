# RNN（Recurrent Neural Network / 再帰型ニューラルネットワーク）

## 要点
- 系列データ（時系列・テキスト）を処理する深層学習モデル。前の時刻の情報を保持しながら逐次処理
- **長所**: 可変長入力対応、時間的依存関係を学習。**短所**: 長期依存性の問題（勾配消失）
- **発展形**: LSTM・GRUで長期依存性を改善。Encoder-Decoderで系列変換（機械翻訳等）を実現

## 定義
RNN（Recurrent Neural Network）は、系列データを処理するためのニューラルネットワーク。各時刻で前の隠れ状態を入力として受け取り、時間方向に情報を伝播させる。時系列予測、自然言語処理、音声認識等に応用される。

---

## RNNの基本構造

### 基本原理
通常のニューラルネットワークは時刻間で独立だが、RNNは**隠れ状態（Hidden State）**を持ち、前の時刻の情報を次の時刻に渡す。

### 基本式
時刻 $t$ における処理：

$$h_t = f(W_h h_{t-1} + W_x x_t + b)$$
$$y_t = g(W_y h_t + b_y)$$

- $x_t$: 時刻 $t$ の入力
- $h_t$: 時刻 $t$ の隠れ状態（前の時刻 $h_{t-1}$ と現在の入力 $x_t$ から計算）
- $y_t$: 時刻 $t$ の出力
- $W_h, W_x, W_y$: 重み行列
- $f, g$: 活性化関数（通常 $f$ はtanh、$g$ はsoftmax等）

### 図解（展開図）
```
時刻:   t=0      t=1      t=2      t=3
入力:   x₀  →   x₁  →   x₂  →   x₃
        ↓       ↓       ↓       ↓
隠れ:   h₀ →   h₁ →   h₂ →   h₃
        ↓       ↓       ↓       ↓
出力:   y₀      y₁      y₂      y₃
```

各時刻で**同じ重み $W_h, W_x, W_y$ を共有**（パラメータ共有）。

---

## RNNの長所と短所

### 長所
1. **可変長入力に対応**: 任意の長さの系列を処理可能
2. **時間的依存関係の学習**: 前の情報を保持しながら処理
3. **パラメータ共有**: 時刻数に関わらず重みは固定（過学習抑制）
4. **逐次処理**: オンライン学習・リアルタイム処理が可能

### 短所（勾配消失・爆発問題）
1. **長期依存性の問題**: 時刻が離れた情報の学習が困難
   - 勾配消失：誤差逆伝播で勾配が減衰し、遠い過去の影響が学習できない
   - 勾配爆発：勾配が指数的に増大し、学習が不安定
2. **並列化困難**: 逐次処理のため、時刻方向の並列化ができない（訓練が遅い）
3. **長い系列での性能低下**: 数百時刻を超えると精度が大幅に低下

---

## RNNの発展形

### LSTM（Long Short-Term Memory）
- **目的**: 勾配消失問題の解決
- **機構**: ゲート機構（入力・忘却・出力ゲート）とセルステートで長期記憶を保持
- **効果**: 数百〜数千時刻の長期依存性を学習可能
- 詳細は [lstm_gru.md](lstm_gru.md) を参照

### GRU（Gated Recurrent Unit）
- **目的**: LSTMの簡略版（パラメータ削減）
- **機構**: リセット・更新ゲートの2つ（LSTMより少ない）
- **効果**: LSTMに近い性能で高速
- 詳細は [lstm_gru.md](lstm_gru.md) を参照

### 双方向RNN（Bidirectional RNN）
- **構造**: 順方向と逆方向の2つのRNNを並列に配置
- **効果**: 過去と未来の両方の文脈を利用（文章分類・固有表現抽出等）

---

## RNN Encoder-Decoder（Seq2Seq）

### 概要
**Seq2Seq（Sequence-to-Sequence）モデル**または**Encoder-Decoderモデル**は、**可変長入力を可変長出力に変換**する系列変換アーキテクチャ。**機械翻訳などの系列変換タスク**に最も適している。2つのRNN（エンコーダ/デコーダ）を利用します。

### 構造と役割（★試験重要）

**2つのRNNの役割**：

1. **Encoder（エンコーダ/符号化器）の役割**：
   - 入力系列を順次読み込み、**固定長のベクトル（コンテキストベクトル）に圧縮**
   - 入力の意味情報を凝縮した表現を作成
   - 最終隠れ状態がコンテキストベクトルとなる

2. **Decoder（デコーダ/復号化器）の役割**：
   - コンテキストベクトルを受け取り、**出力系列を1単語ずつ生成**
   - Encoderが圧縮した情報を展開して出力を生成
   - 前の時刻の出力を次の入力として使用（自己回帰）

**空欄埋め問題の典型例**：
> 「（A）は機械翻訳などに用いられるモデルであり、2つのRNNを利用する。これらはエンコーダ/デコーダと呼ばれ、それぞれ（B）という役割を果たす。」

- **(A) = Seq2Seq（Sequence-to-Sequence）** または **Encoder-Decoder**
- **(B) = 入力系列を固定長ベクトルに圧縮 / 固定長ベクトルから出力系列を生成** または **符号化 / 復号化**

### 図解
```
入力: "I love AI"  →  Encoder  →  [コンテキストベクトル]  →  Decoder  →  出力: "私はAIが好きです"
      (英語系列)       (RNN)       (固定長の意味表現)        (RNN)         (日本語系列)
                        ↓                                      ↓
                   入力を圧縮                             出力を生成
```

### 詳細な処理フロー

**Encoderの処理**：
```
入力系列: x_1, x_2, ..., x_T
隠れ状態: h_1, h_2, ..., h_T

h_t = f(h_{t-1}, x_t)  # 各時刻で状態を更新
context = h_T          # 最終隠れ状態 = コンテキストベクトル
```

**Decoderの処理**：
```
初期状態: s_0 = context（Encoderの最終状態）
出力系列: y_1, y_2, ..., y_T'

s_t = f(s_{t-1}, y_{t-1})  # 前の出力を次の入力に
y_t = softmax(W * s_t)     # 単語確率分布を出力
```

### 役割の比較表

| 項目 | Encoder（エンコーダ） | Decoder（デコーダ） |
|------|-------------------|------------------|
| **入力** | 元言語の系列 | コンテキストベクトル + 前時刻の出力 |
| **出力** | コンテキストベクトル（固定長） | 目標言語の系列（可変長） |
| **役割** | **圧縮・符号化** | **生成・復号化** |
| **処理方向** | 入力系列を順次読込 | 出力系列を順次生成 |
| **時刻数** | 入力系列の長さ | 出力系列の長さ（可変） |
| **別名** | 符号化器 | 復号化器 |

### アルゴリズム
```
【Encoder】
for t in 入力系列:
    h_t = RNN(h_{t-1}, x_t)
context_vector = h_T  # 最終隠れ状態

【Decoder】
h_0 = context_vector  # Encoderの最終状態を初期状態に
for t in 出力系列:
    h_t = RNN(h_{t-1}, y_{t-1})  # 前の出力を次の入力に
    y_t = softmax(W * h_t)      # 単語分布を出力
```

### 最も適している応用（試験頻出）

| 応用分野 | 入力 | 出力 | 適合度 |
|---------|------|------|--------|
| **機械翻訳** | 英語文 | 日本語文 | **最適**（最頻出） |
| **文章要約** | 長文 | 要約文 | 最適 |
| **対話システム** | 質問文 | 応答文 | 最適 |
| **画像キャプション** | 画像（CNN） | 説明文 | 適合（Encoderは通常CNN） |
| **音声認識** | 音響特徴 | テキスト | 適合 |

### 典型的な問題形式
**問：RNN Encoder-Decoderは（　）に最も適している手法である。**

✅ **正解の選択肢**：
- **「機械翻訳」** → ✅ **最も適切**（最頻出、入力言語→出力言語）
- **「系列変換タスク」** → ✅ 適切（一般的表現）
- **「Seq2Seq（系列-系列変換）」** → ✅ 適切（技術用語）
- 「文章要約」→ ✅ 適切（長文→短文）
- 「対話システム」→ ✅ 適切（質問→応答）

❌ **不適切な選択肢**：
- 「画像分類」→ ❌ 系列→クラスラベル（Decoderが不要）
- 「時系列予測」→ ❌ 単純なRNNで十分（Encoder-Decoder不要）
- 「異常検知」→ ❌ 通常は教師なし学習
- 「回帰問題」→ ❌ 系列→単一値（Decoderが不要）

### Attention機構との組み合わせ
Encoder-Decoderの問題点：
- **情報ボトルネック**: 長い入力を固定長ベクトル1つに圧縮すると情報が失われる

**Attention機構**（注意機構）の導入：
- Decoderが各時刻で、Encoderの全時刻の隠れ状態から**重要な部分に注目**
- 長い文章でも精度向上
- Transformerへの発展（RNNを排除し、Attentionのみで構成）
- 詳細は [transformer.md](transformer.md) を参照

---

## RNNの応用分野

### 自然言語処理
- **機械翻訳**: Encoder-Decoder（現在はTransformer主流）
- **文章分類**: 感情分析、スパム検出（双方向RNN + 全結合層）
- **固有表現抽出**: 人名・地名等の識別（双方向RNN）
- **言語モデル**: 次単語予測（現在はTransformer主流）

### 時系列データ処理
- **株価予測**: 過去の価格から未来を予測
- **異常検知**: 正常パターンからの逸脱を検出
- **センサーデータ解析**: IoT、工場設備の状態監視

### 音声処理
- **音声認識**: 音響特徴→テキスト（現在はTransformer主流）
- **音声合成**: テキスト→音声波形

### その他
- **動画解析**: フレーム系列の処理
- **ジェスチャー認識**: 時系列の動作パターン認識

---

## 重要キーワード
- **RNN（Recurrent Neural Network）**: 系列データ処理のための再帰型ネットワーク
- **隠れ状態（Hidden State）**: 前の時刻の情報を保持するベクトル
- **勾配消失問題**: 誤差逆伝播で勾配が減衰し、長期依存性の学習が困難
- **勾配爆発問題**: 勾配が指数的に増大し、学習が不安定
- **LSTM（Long Short-Term Memory）**: ゲート機構で長期依存性を改善
- **GRU（Gated Recurrent Unit）**: LSTMの簡略版
- **双方向RNN（Bidirectional RNN）**: 順・逆方向の両方向から処理
- **Encoder-Decoder**: 系列変換アーキテクチャ、機械翻訳に最適
- **Seq2Seq（Sequence-to-Sequence）**: 系列-系列変換タスク
- **コンテキストベクトル**: Encoderが生成する固定長の意味表現
- **Attention機構**: Decoderが入力の重要部分に注目する仕組み

---

## 試験での問われ方

### 典型的な出題パターン
1. **RNNの特徴**: 「系列データを処理し、前の時刻の情報を保持」→ ✅
2. **RNNの問題点**: 「勾配消失により長期依存性の学習が困難」→ ✅
3. **LSTM/GRUの目的**: 「勾配消失問題の解決」→ ✅
4. **Encoder-Decoderの用途**: 「機械翻訳」→ ✅ **最頻出**
5. **Encoder-Decoderの別名**: 「Seq2Seq（Sequence-to-Sequence）」→ ✅
6. **エンコーダの役割**: 「入力系列を固定長ベクトルに圧縮（符号化）」→ ✅
7. **デコーダの役割**: 「固定長ベクトルから出力系列を生成（復号化）」→ ✅
8. **双方向RNNの利点**: 「過去と未来の両方の文脈を利用」→ ✅
9. **穴埋め問題**: 「RNN Encoder-Decoderは（機械翻訳）に最も適している」→ ✅
10. **穴埋め問題**: 「（Seq2Seq）は機械翻訳などに用いられるモデルであり、2つのRNNを利用する」→ ✅

### 空欄補充問題の頻出パターン

**問題例1：系列変換モデルの名称と構成**
> 「系列変換タスクに用いられる（　）モデルは、2つのRNNで構成される。入力側と出力側のネットワークは（　）と呼ばれる。」

✅ **正解**：
- **Seq2Seq（Sequence-to-Sequence）** または **Encoder-Decoder**
- **エンコーダ（Encoder）とデコーダ（Decoder）** または **符号化器と復号化器**

**問題例2：エンコーダ・デコーダの役割**
> 「Seq2Seqモデルにおいて、Encoderは入力系列を（　）に変換する役割を持ち、Decoderはそれを受け取って（　）する。」

✅ **正解**：
- **固定長ベクトル（コンテキストベクトル）** または **固定長の意味表現**
- **出力系列を生成（復号化）** または **目標系列を1単語ずつ生成**

**問題例3：機械翻訳への適用**
> 「機械翻訳では、原言語の文を（　）が処理して圧縮し、目標言語の文を（　）が生成する。」

✅ **正解**：
- **Encoder（エンコーダ）**
- **Decoder（デコーダ）**

### 引っ掛けポイント
❌ 「RNNは画像認識に最適」→ CNN（畳み込み）が適切  
❌ 「RNNは長期依存性に強い」→ 逆、弱い（LSTM/GRUが必要）  
❌ 「Encoder-Decoderは画像分類に最適」→ 系列変換が本来の用途  
❌ 「RNNは並列処理が得意」→ 逐次処理のため並列化困難（Transformerは並列可）
❌ 「Encoderが出力系列を生成」→ 逆、Decoderが生成
❌ 「Decoderが入力を圧縮」→ 逆、Encoderが圧縮
✅ 「RNNは系列データの時間的依存関係を学習」→ **正解**  
✅ 「Encoder-Decoderは機械翻訳に最適」→ **正解**
✅ 「Encoderは符号化、Decoderは復号化」→ **正解**  
✅ 「LSTMは勾配消失を緩和」→ **正解**  
✅ 「Encoder-Decoderは機械翻訳に最適」→ **正解**

### 比較されやすい概念
- **RNN vs CNN**: 系列データ vs 画像データ
- **RNN vs LSTM/GRU**: 基本形 vs 長期依存性対応
- **RNN vs Transformer**: 逐次処理 vs 並列処理、長期依存性の性能
- **Encoder-Decoder vs 単純RNN**: 系列変換 vs 系列予測
- **双方向RNN vs 一方向RNN**: 両方向文脈 vs 過去のみ

---

---

## BPTT（Backpropagation Through Time / 時間方向の誤差逆伝播）

### 要点
- RNNを学習するための誤差逆伝播アルゴリズム。時系列データを**時間方向に展開**してから誤差を逆伝播
- 長い系列では**勾配消失・爆発**が発生しやすく、Truncated BPTT（短縮版）が実用的
- LSTM/GRUはBPTTの勾配消失問題を緩和するために開発された

### 定義
時刻 $t$ の出力誤差を、過去の時刻 $t-1, t-2, \ldots$ の重みパラメータに対して逆向きに伝播させる手法。RNNの再帰構造を時間軸に沿って**展開（Unroll）**し、通常の誤差逆伝播を適用する。

### 処理の流れ
1. **順伝播**: 時刻 $t=1$ から $T$ まで順に入力を処理し、隠れ状態を更新
2. **展開**: RNNを時間方向に展開し、各時刻を別の層とみなす
3. **誤差計算**: 最終時刻（または各時刻）の出力誤差を計算
4. **逆伝播**: 時刻 $T$ から $1$ へ誤差を逆向きに伝播、各時刻の勾配を計算
5. **重み更新**: 全時刻の勾配を合計して重みを更新

### 直感的な説明
通常のニューラルネットワークが「**層の深さ方向**」に誤差を伝えるのに対し、BPTTは「**時間の流れを逆向き**」に誤差を伝えます。

例：「私は犬が好き」という文を処理する場合
- **順伝播**: 「私」→「は」→「犬」→「が」→「好き」と順に処理
- **逆伝播**: 「好き」の誤差を「が」→「犬」→「は」→「私」へ逆向きに伝播

### 図解
```
【時間展開されたRNN】
時刻:  t=1    t=2    t=3    t=4
入力:  私  →  は  →  犬  →  が  →  好き
       ↓      ↓      ↓      ↓      ↓
隠れ:  h₁ →  h₂ →  h₃ →  h₄ →  h₅
       ↓      ↓      ↓      ↓      ↓
出力:  y₁     y₂     y₃     y₄     y₅
                                    ↑
                            誤差: E (正解と比較)

【BPTTの誤差伝播】
誤差の流れ: E → ∂E/∂h₅ → ∂E/∂h₄ → ∂E/∂h₃ → ∂E/∂h₂ → ∂E/∂h₁
           (逆向きに伝播)
```

### 勾配消失・爆発の問題
時刻を $t$ ステップ遡ると、勾配は $(\frac{\partial h_t}{\partial h_{t-1}})^t$ の累積になる：
- **勾配消失**: $|\frac{\partial h_t}{\partial h_{t-1}}| < 1$ の場合、$t$ が大きいと勾配が指数的に減衰
- **勾配爆発**: $|\frac{\partial h_t}{\partial h_{t-1}}| > 1$ の場合、$t$ が大きいと勾配が指数的に増大

### Truncated BPTT（短縮版）
完全なBPTTは計算コストが高く、長い系列では勾配消失が深刻化するため、**一定の時間ステップ数だけ遡る**手法。

| 項目 | BPTT（完全版） | Truncated BPTT |
|------|---------------|---------------|
| 遡る範囲 | 系列の最初まで | 固定ステップ数（例：20ステップ）のみ |
| 計算コスト | 大（系列長に比例） | 小（固定） |
| 精度 | 理論上は最適 | 近似的 |
| 実用性 | 長い系列では困難 | 実用的 |

**アルゴリズム**:
```
系列長が1000ステップ、Truncated長が20の場合：
1. 0-19ステップ: 順伝播
2. 19→0ステップ: 逆伝播（20ステップのみ）
3. 20-39ステップ: 順伝播（隠れ状態は19から継続）
4. 39→20ステップ: 逆伝播（20ステップのみ）
   ...繰り返し
```

### 試験での問われ方

**典型的な出題パターン**:
1. **定義の確認**: 「BPTTは何を逆伝播させるか」→ **誤差を時間方向に逆向きに伝播**
2. **適用対象**: 「どのモデルに使用されるか」→ **RNN、LSTM、GRU等の時系列モデル**
3. **課題**: 「BPTTの問題点は何か」→ **勾配消失・勾配爆発**
4. **解決策**: 「長期依存関係を学習しやすい手法は」→ **LSTM、GRU、Attention機構**

**ひっかけポイント（混同しやすい概念）**:

| 手法 | 説明 | 誤解しやすいポイント |
|------|------|---------------------|
| **BPTT** | RNNの誤差逆伝播（時間方向） | ❌ 通常のBP（層方向）と混同 |
| **Backpropagation** | 通常のNNの誤差逆伝播（層方向） | ❌ BPTTとの違いは「時間展開」の有無 |
| **Truncated BPTT** | 一部時刻のみ逆伝播 | ❌ 完全なBPTTとの違いは計算範囲 |
| **Real-Time RNN (RTRL)** | 順伝播と同時に勾配計算 | ❌ BPTTは逆方向、RTRLは順方向 |

**選択肢で出やすい誤り**:
- ❌ 「畳み込み層の学習に使う」→ CNNはBPTTを使わない
- ❌ 「未来の時刻から過去へ予測する」→ 予測でなく誤差伝播
- ❌ 「並列計算で高速化できる」→ 時間依存性があり並列化困難
- ✅ 「時系列の各ステップで共有される重みを更新する」→ **正解**
- ✅ 「時間方向に展開してから誤差逆伝播を適用する」→ **正解**

### 関連する重要概念
- **勾配クリッピング（Gradient Clipping）**: 勾配爆発対策。勾配のノルムが閾値を超えたら切り詰める
- **勾配の正規化**: 勾配を正規化して安定化
- **LSTM/GRUのゲート機構**: 勾配消失を緩和し、長期依存性を学習可能に

---

## 補足
- **実務での現状**: 自然言語処理では**Transformerが主流**（2017年以降）。RNNは並列化困難で訓練が遅い
- **RNNが残る領域**: リアルタイム処理（オンライン学習）、メモリ制約が厳しい環境
- **BPTT実装の注意点**: 長い系列（例：1000ステップ以上）ではTruncated BPTTが必須、勾配クリッピングで勾配爆発を防止
- **Teacher Forcing**: Decoder訓練時に正解データを次の入力に使う（収束高速化）
- **歴史的意義**: RNNは深層学習の系列処理への道を開いたが、Transformerの登場で主役交代
