# RNN（Recurrent Neural Network / 再帰型ニューラルネットワーク）

## 要点
- 系列データ（時系列・テキスト）を処理する深層学習モデル。前の時刻の情報を保持しながら逐次処理
- **長所**: 可変長入力対応、時間的依存関係を学習。**短所**: 長期依存性の問題（勾配消失）
- **発展形**: LSTM・GRUで長期依存性を改善。Encoder-Decoderで系列変換（機械翻訳等）を実現

## 定義
RNN（Recurrent Neural Network）は、系列データを処理するためのニューラルネットワーク。各時刻で前の隠れ状態を入力として受け取り、時間方向に情報を伝播させる。時系列予測、自然言語処理、音声認識等に応用される。

---

## RNNの基本構造

### 基本原理
通常のニューラルネットワークは時刻間で独立だが、RNNは**隠れ状態（Hidden State）**を持ち、前の時刻の情報を次の時刻に渡す。

### 基本式
時刻 $t$ における処理：

$$h_t = f(W_h h_{t-1} + W_x x_t + b)$$
$$y_t = g(W_y h_t + b_y)$$

- $x_t$: 時刻 $t$ の入力
- $h_t$: 時刻 $t$ の隠れ状態（前の時刻 $h_{t-1}$ と現在の入力 $x_t$ から計算）
- $y_t$: 時刻 $t$ の出力
- $W_h, W_x, W_y$: 重み行列
- $f, g$: 活性化関数（通常 $f$ はtanh、$g$ はsoftmax等）

### 図解（展開図）
```
時刻:   t=0      t=1      t=2      t=3
入力:   x₀  →   x₁  →   x₂  →   x₃
        ↓       ↓       ↓       ↓
隠れ:   h₀ →   h₁ →   h₂ →   h₃
        ↓       ↓       ↓       ↓
出力:   y₀      y₁      y₂      y₃
```

各時刻で**同じ重み $W_h, W_x, W_y$ を共有**（パラメータ共有）。

---

## RNNの長所と短所

### 長所
1. **可変長入力に対応**: 任意の長さの系列を処理可能
2. **時間的依存関係の学習**: 前の情報を保持しながら処理
3. **パラメータ共有**: 時刻数に関わらず重みは固定（過学習抑制）
4. **逐次処理**: オンライン学習・リアルタイム処理が可能

### 短所（勾配消失・爆発問題）
1. **長期依存性の問題**: 時刻が離れた情報の学習が困難
   - 勾配消失：誤差逆伝播で勾配が減衰し、遠い過去の影響が学習できない
   - 勾配爆発：勾配が指数的に増大し、学習が不安定
2. **並列化困難**: 逐次処理のため、時刻方向の並列化ができない（訓練が遅い）
3. **長い系列での性能低下**: 数百時刻を超えると精度が大幅に低下

---

## RNNの発展形

### LSTM（Long Short-Term Memory）
- **目的**: 勾配消失問題の解決
- **機構**: ゲート機構（入力・忘却・出力ゲート）とセルステートで長期記憶を保持
- **効果**: 数百〜数千時刻の長期依存性を学習可能
- 詳細は [lstm_gru.md](lstm_gru.md) を参照

### GRU（Gated Recurrent Unit）
- **目的**: LSTMの簡略版（パラメータ削減）
- **機構**: リセット・更新ゲートの2つ（LSTMより少ない）
- **効果**: LSTMに近い性能で高速
- 詳細は [lstm_gru.md](lstm_gru.md) を参照

### 双方向RNN（Bidirectional RNN）
- **構造**: 順方向と逆方向の2つのRNNを並列に配置
- **効果**: 過去と未来の両方の文脈を利用（文章分類・固有表現抽出等）

---

## RNN Encoder-Decoder（Seq2Seq）

### 概要
**可変長入力を可変長出力に変換**する系列変換（Sequence-to-Sequence）アーキテクチャ。**機械翻訳**に最も適している。

### 構造
1. **Encoder（符号化器）**: 入力系列を固定長のベクトル（コンテキストベクトル）に圧縮
2. **Decoder（復号化器）**: コンテキストベクトルから出力系列を生成

### 図解
```
入力: "I love AI"  →  Encoder  →  [固定長ベクトル]  →  Decoder  →  出力: "私はAIが好きです"
      (英語系列)       (RNN)       (意味表現)        (RNN)         (日本語系列)
```

### アルゴリズム
```
【Encoder】
for t in 入力系列:
    h_t = RNN(h_{t-1}, x_t)
context_vector = h_T  # 最終隠れ状態

【Decoder】
h_0 = context_vector  # Encoderの最終状態を初期状態に
for t in 出力系列:
    h_t = RNN(h_{t-1}, y_{t-1})  # 前の出力を次の入力に
    y_t = softmax(W * h_t)      # 単語分布を出力
```

### 最も適している応用（試験頻出）

| 応用分野 | 入力 | 出力 | 適合度 |
|---------|------|------|--------|
| **機械翻訳** | 英語文 | 日本語文 | **最適**（最頻出） |
| **文章要約** | 長文 | 要約文 | 最適 |
| **対話システム** | 質問文 | 応答文 | 最適 |
| **画像キャプション** | 画像（CNN） | 説明文 | 適合（Encoderは通常CNN） |
| **音声認識** | 音響特徴 | テキスト | 適合 |

### 典型的な問題形式
**問：RNN Encoder-Decoderは（　）に最も適している手法である。**

✅ **正解の選択肢**：
- **「機械翻訳」** → ✅ **最も適切**（最頻出、入力言語→出力言語）
- **「系列変換タスク」** → ✅ 適切（一般的表現）
- **「Seq2Seq（系列-系列変換）」** → ✅ 適切（技術用語）
- 「文章要約」→ ✅ 適切（長文→短文）
- 「対話システム」→ ✅ 適切（質問→応答）

❌ **不適切な選択肢**：
- 「画像分類」→ ❌ 系列→クラスラベル（Decoderが不要）
- 「時系列予測」→ ❌ 単純なRNNで十分（Encoder-Decoder不要）
- 「異常検知」→ ❌ 通常は教師なし学習
- 「回帰問題」→ ❌ 系列→単一値（Decoderが不要）

### Attention機構との組み合わせ
Encoder-Decoderの問題点：
- **情報ボトルネック**: 長い入力を固定長ベクトル1つに圧縮すると情報が失われる

**Attention機構**（注意機構）の導入：
- Decoderが各時刻で、Encoderの全時刻の隠れ状態から**重要な部分に注目**
- 長い文章でも精度向上
- Transformerへの発展（RNNを排除し、Attentionのみで構成）
- 詳細は [transformer.md](transformer.md) を参照

---

## RNNの応用分野

### 自然言語処理
- **機械翻訳**: Encoder-Decoder（現在はTransformer主流）
- **文章分類**: 感情分析、スパム検出（双方向RNN + 全結合層）
- **固有表現抽出**: 人名・地名等の識別（双方向RNN）
- **言語モデル**: 次単語予測（現在はTransformer主流）

### 時系列データ処理
- **株価予測**: 過去の価格から未来を予測
- **異常検知**: 正常パターンからの逸脱を検出
- **センサーデータ解析**: IoT、工場設備の状態監視

### 音声処理
- **音声認識**: 音響特徴→テキスト（現在はTransformer主流）
- **音声合成**: テキスト→音声波形

### その他
- **動画解析**: フレーム系列の処理
- **ジェスチャー認識**: 時系列の動作パターン認識

---

## 重要キーワード
- **RNN（Recurrent Neural Network）**: 系列データ処理のための再帰型ネットワーク
- **隠れ状態（Hidden State）**: 前の時刻の情報を保持するベクトル
- **勾配消失問題**: 誤差逆伝播で勾配が減衰し、長期依存性の学習が困難
- **勾配爆発問題**: 勾配が指数的に増大し、学習が不安定
- **LSTM（Long Short-Term Memory）**: ゲート機構で長期依存性を改善
- **GRU（Gated Recurrent Unit）**: LSTMの簡略版
- **双方向RNN（Bidirectional RNN）**: 順・逆方向の両方向から処理
- **Encoder-Decoder**: 系列変換アーキテクチャ、機械翻訳に最適
- **Seq2Seq（Sequence-to-Sequence）**: 系列-系列変換タスク
- **コンテキストベクトル**: Encoderが生成する固定長の意味表現
- **Attention機構**: Decoderが入力の重要部分に注目する仕組み

---

## 試験での問われ方

### 典型的な出題パターン
1. **RNNの特徴**: 「系列データを処理し、前の時刻の情報を保持」→ ✅
2. **RNNの問題点**: 「勾配消失により長期依存性の学習が困難」→ ✅
3. **LSTM/GRUの目的**: 「勾配消失問題の解決」→ ✅
4. **Encoder-Decoderの用途**: 「機械翻訳」→ ✅ **最頻出**
5. **双方向RNNの利点**: 「過去と未来の両方の文脈を利用」→ ✅
6. **穴埋め問題**: 「RNN Encoder-Decoderは（機械翻訳）に最も適している」→ ✅

### 引っ掛けポイント
❌ 「RNNは画像認識に最適」→ CNN（畳み込み）が適切  
❌ 「RNNは長期依存性に強い」→ 逆、弱い（LSTM/GRUが必要）  
❌ 「Encoder-Decoderは画像分類に最適」→ 系列変換が本来の用途  
❌ 「RNNは並列処理が得意」→ 逐次処理のため並列化困難（Transformerは並列可）  
✅ 「RNNは系列データの時間的依存関係を学習」→ **正解**  
✅ 「LSTMは勾配消失を緩和」→ **正解**  
✅ 「Encoder-Decoderは機械翻訳に最適」→ **正解**

### 比較されやすい概念
- **RNN vs CNN**: 系列データ vs 画像データ
- **RNN vs LSTM/GRU**: 基本形 vs 長期依存性対応
- **RNN vs Transformer**: 逐次処理 vs 並列処理、長期依存性の性能
- **Encoder-Decoder vs 単純RNN**: 系列変換 vs 系列予測
- **双方向RNN vs 一方向RNN**: 両方向文脈 vs 過去のみ

---

## 補足
- **実務での現状**: 自然言語処理では**Transformerが主流**（2017年以降）。RNNは並列化困難で訓練が遅い
- **RNNが残る領域**: リアルタイム処理（オンライン学習）、メモリ制約が厳しい環境
- **BPTT（Backpropagation Through Time）**: RNNの学習手法。時間方向に展開して誤差逆伝播
- **Truncated BPTT**: 長い系列を分割して勾配計算（計算コスト削減）
- **Teacher Forcing**: Decoder訓練時に正解データを次の入力に使う（収束高速化）
- **歴史的意義**: RNNは深層学習の系列処理への道を開いたが、Transformerの登場で主役交代
