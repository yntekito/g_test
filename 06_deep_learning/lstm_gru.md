# LSTM・GRU

## 要点
- LSTM（Long Short-Term Memory）は、通常のRNNの勾配消失・爆発問題を解決し、長期依存関係を学習できるRNNの改良版。
- LSTMブロックは、**入力ゲート・出力ゲート・忘却ゲート**の3つのゲート機構と、**セルステート（Cell State）**と呼ばれる記憶素子で構成される。
- GRU（Gated Recurrent Unit）はLSTMを簡略化した構造で、2つのゲート（リセット・更新）のみ。計算効率が高く、短い系列で高性能。

## 定義
**LSTM（Long Short-Term Memory）**は、セルステートと呼ばれる長期記憶機構とゲート機構を持つRNNの一種。時系列データの長期依存関係を効果的に学習できる。

**GRU（Gated Recurrent Unit）**は、LSTMを簡略化したRNN構造。セルステートを持たず、隠れ状態のみでリセット・更新ゲートにより情報を制御。

## 重要キーワード
- **LSTM（Long Short-Term Memory）**: 長期記憶を保持できるRNN、1997年にHochreiter & Schmidhuberが提案
- **セルステート（Cell State）**: LSTMの核心、長期記憶を保持する記憶素子（Memory Cell）
- **入力ゲート（Input Gate）**: 新しい情報をどれだけセルステートに追加するかを制御
- **忘却ゲート（Forget Gate）**: 過去の情報をどれだけ忘れるかを制御
- **出力ゲート（Output Gate）**: セルステートから隠れ状態への出力を制御
- **GRU（Gated Recurrent Unit）**: LSTMの簡略版、リセット・更新ゲートのみ
- **勾配消失問題（Vanishing Gradient）**: RNNで長期依存を学習できない原因、LSTMで解決
- **勾配爆発問題（Exploding Gradient）**: 勾配が指数的に増大する問題、勾配クリッピングで対処

---

## LSTM（Long Short-Term Memory）

### 要点
LSTMは通常のRNNの勾配消失問題を解決し、長期依存関係を学習できる。セルステート（記憶素子）と3つのゲート機構（入力・忘却・出力）により、必要な情報を保持し不要な情報を忘却する。自然言語処理（翻訳・音声認識）、時系列予測で広く使用。

### 定義
LSTM（Long Short-Term Memory）は、セルステート $C_t$ と3つのゲート機構を持つRNN。各時刻 $t$ で以下を計算：

1. **忘却ゲート**: $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
2. **入力ゲート**: $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
3. **セルステート候補**: $\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$
4. **セルステート更新**: $C_t = f_t \ast C_{t-1} + i_t \ast \tilde{C}_t$
5. **出力ゲート**: $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
6. **隠れ状態**: $h_t = o_t \ast \tanh(C_t)$

### 重要キーワード
- **セルステート（Cell State）**: $C_t$、長期記憶を保持する「コンベア」、情報が時間方向に流れる
- **忘却ゲート（Forget Gate）**: $f_t$、過去のセルステートをどれだけ保持するか（0=全忘却、1=全保持）
- **入力ゲート（Input Gate）**: $i_t$、新情報をどれだけ追加するか
- **出力ゲート（Output Gate）**: $o_t$、セルステートから隠れ状態への出力量を制御
- **ゲート機構**: シグモイド関数で0～1の値を出力、情報の流れを調整
- **要素積（Element-wise Product）**: $\ast$ 演算、ゲート値とデータの掛け算

### 詳細

#### 背景
通常のRNNは、長い時系列で勾配消失問題が発生し、遠い過去の情報を学習できない。LSTMは1997年にHochreiter & Schmidhuberが提案し、セルステートとゲート機構により長期依存を学習可能にした。

#### LSTMブロックの構造
LSTMブロックは以下で構成：
1. **セルステート $C_t$（記憶素子）**: 情報が時間方向に流れる「高速道路」、勾配が減衰しにくい
2. **忘却ゲート $f_t$**: 過去の記憶を選択的に忘却
3. **入力ゲート $i_t$**: 新しい情報を選択的に追加
4. **出力ゲート $o_t$**: 記憶から出力を選択的に取り出し

#### セルステートの役割（重要）
セルステートは、**加算と要素積のみで更新**されるため、勾配が時間方向に流れやすく、勾配消失を防ぐ。通常のRNNは掛け算の連鎖で勾配が指数的に減衰するが、LSTMはセルステートにより勾配を保持。

#### 図解（概念）
```
時刻 t-1                  時刻 t
C_{t-1} ─────────────────→ C_t  (セルステート、長期記憶)
  ↓              ×f_t   +  ↓
h_{t-1} → [忘却ゲート] ─────→ [加算] ← [入力ゲート] × [候補値]
  ↓              f_t         ↓     i_t      ×     tanh
x_t ──────────────────────────────────────────────→
  ↓                                           ↓
  └─────────→ [出力ゲート] ────→ h_t (隠れ状態)
                  o_t      × tanh(C_t)
```

#### 各ゲートの役割と動作例

**1. 忘却ゲート（Forget Gate）**
- **役割**: 過去の記憶を忘れるか保持するか
- **例**: 「彼は〜、彼女は〜」で主語が変わる際、前の主語情報を忘却

**2. 入力ゲート（Input Gate）**
- **役割**: 新しい情報を記憶に追加するか
- **例**: 新しい主語「彼女」を記憶に追加

**3. 出力ゲート（Output Gate）**
- **役割**: 記憶から何を出力するか
- **例**: 動詞の活用に必要な主語情報のみ取り出し

#### 勾配消失問題の解決原理
通常のRNN:
$$\frac{\partial h_t}{\partial h_0} = \prod_{i=1}^t \frac{\partial h_i}{\partial h_{i-1}} \rightarrow 0 \quad (t \text{が大きいとき})$$

LSTM:
$$\frac{\partial C_t}{\partial C_0} = \prod_{i=1}^t f_i \quad (\text{忘却ゲートが1に近いと勾配保持})$$

セルステートの更新が加算ベースなので、勾配が減衰しにくい。

### 試験での問われ方

#### 典型設問
- **「LSTMブロックは、入力・出力・忘却ゲート機構と、（セルステート）と呼ばれる記憶素子で構成」**
- LSTMがRNNの何の問題を解決したか→勾配消失問題
- LSTMの3つのゲート機構→入力・忘却・出力
- セルステートの役割→長期記憶の保持、勾配の保持

#### ひっかけポイントと違いの整理

| 項目 | RNN | LSTM | GRU |
|------|-----|------|-----|
| **ゲート数** | なし | 3（入力・忘却・出力） | 2（リセット・更新） |
| **記憶素子** | 隠れ状態のみ | セルステート + 隠れ状態 | 隠れ状態のみ |
| **長期依存** | 困難 | 可能 | 可能 |
| **パラメータ数** | 少 | 多 | 中 |
| **計算コスト** | 低 | 高 | 中 |
| **適用例** | 短期系列 | 長期系列（翻訳・音声） | 中期系列 |

**混同注意**:
- **RNN vs LSTM**: LSTMはゲート機構とセルステートで勾配消失を解決
- **LSTM vs GRU**: LSTMは3ゲート+セルステート、GRUは2ゲートで簡略化
- **セルステート vs 隠れ状態**: セルステートは内部記憶、隠れ状態は外部出力

**出題パターン**:
- 「セルステート（Cell State）と呼ばれる記憶素子」→**LSTM**
- 「忘却ゲートで過去の情報を制御」→**LSTM**
- 「勾配消失問題を解決」→**LSTM/GRU**
- 「3つのゲート機構」→**LSTM（入力・忘却・出力）**

### 補足

#### 実務観点
- **用途**: 機械翻訳（Seq2Seq）、音声認識、株価予測、異常検知
- **ライブラリ**: PyTorch（`nn.LSTM`）、TensorFlow/Keras（`LSTM`）
- **ハイパーパラメータ**: 隠れ層次元数、層数、ドロップアウト率
- **計算コスト**: 通常のRNNの約4倍（4つの層を計算するため）
- **変種**: Peephole LSTM（セルステートをゲート入力に追加）、双方向LSTM（Bidirectional LSTM）

#### 最新動向
- **Transformer登場**: 2017年以降、NLPではTransformerがLSTMを上回る
- **現在の位置付け**: 中規模データ・時系列予測では依然有効、長文翻訳はTransformer優位

---

## GRU（Gated Recurrent Unit）

### 要点
GRUはLSTMを簡略化したRNN構造。セルステートを持たず、隠れ状態のみで**リセットゲート**と**更新ゲート**の2つで情報を制御。パラメータ数が少なく計算効率が高い。短〜中期系列でLSTMと同等以上の性能を発揮することが多い。

### 定義
GRU（Gated Recurrent Unit）は、リセットゲート $r_t$ と更新ゲート $z_t$ を持つRNN。各時刻 $t$ で：

1. **更新ゲート**: $z_t = \sigma(W_z \cdot [h_{t-1}, x_t])$
2. **リセットゲート**: $r_t = \sigma(W_r \cdot [h_{t-1}, x_t])$
3. **隠れ状態候補**: $\tilde{h}_t = \tanh(W \cdot [r_t \ast h_{t-1}, x_t])$
4. **隠れ状態更新**: $h_t = (1 - z_t) \ast h_{t-1} + z_t \ast \tilde{h}_t$

### 重要キーワード
- **GRU（Gated Recurrent Unit）**: LSTMの簡略版、2014年にChoらが提案
- **更新ゲート（Update Gate）**: $z_t$、過去と現在の情報の混合比率を制御
- **リセットゲート（Reset Gate）**: $r_t$、過去の情報をどれだけ忘れるか制御
- **隠れ状態のみ**: セルステートを持たず、隠れ状態 $h_t$ のみで記憶

### 詳細

#### LSTMとの違い
- **ゲート数**: LSTM 3つ（入力・忘却・出力）→ GRU 2つ（リセット・更新）
- **記憶機構**: LSTM セルステート + 隠れ状態 → GRU 隠れ状態のみ
- **パラメータ数**: GRUはLSTMの約75%（層が少ない）
- **計算速度**: GRUが高速（行列演算が少ない）

#### 図解（概念）
```
時刻 t-1                  時刻 t
h_{t-1} ───────────────────→ h_t (隠れ状態)
  ↓        × (1-z_t)       ↑
  ↓                    +   ↑
  ↓                        ↑ × z_t
  └→ [リセットゲート] → [候補値] ──┘
        r_t          tanh(...)

更新ゲート z_t: 過去 vs 現在の混合比率
リセットゲート r_t: 過去情報のリセット度
```

#### 各ゲートの役割

**1. 更新ゲート（Update Gate）**
- **役割**: 過去の隠れ状態をどれだけ保持し、新情報をどれだけ追加するか
- **動作**: $z_t \approx 1$ → 新情報を多く取り込む、$z_t \approx 0$ → 過去を保持

**2. リセットゲート（Reset Gate）**
- **役割**: 過去の隠れ状態をどれだけ忘れて候補値を計算するか
- **動作**: $r_t \approx 0$ → 過去を忘却、$r_t \approx 1$ → 過去を利用

### 試験での問われ方

#### 典型設問
- GRUのゲート数→2つ（リセット・更新）
- LSTMとGRUの違い（パラメータ数、計算コスト、構造）
- GRUがLSTMより優位な場面→短期系列、計算資源制約

#### ひっかけポイント
- **GRUとLSTMの混同**: GRUはセルステートを持たない、ゲートが2つ
- **GRUのゲート名**: 忘却ゲートはLSTM、GRUはリセットゲート・更新ゲート
- **性能比較**: 長期依存はLSTM優位、短期依存はGRU優位またはGRU同等

### 補足

#### 実務での選択基準
- **LSTM選択**: 長い系列（機械翻訳、長文書要約）、長期依存が重要
- **GRU選択**: 短〜中期系列、計算資源制約、高速推論が必要
- **実験**: 両方試してデータセットごとに最適を選ぶ（一概にどちらが良いとは言えない）

#### パフォーマンス比較（経験則）
- **学習速度**: GRU > LSTM（パラメータ数が少ない）
- **精度**: 長期系列ではLSTM優位、短期系列ではGRU同等以上
- **メモリ**: GRU < LSTM（セルステート不要）

---

## 試験での問われ方（LSTM・GRU全般）

### 典型設問
- LSTMブロックの構成要素→入力・出力・忘却ゲート + セルステート
- LSTMが解決した問題→勾配消失問題
- セルステートの役割→長期記憶の保持
- LSTM vs GRU の違い→ゲート数（3 vs 2）、セルステートの有無

### 引っ掛けポイント
- **RNN・LSTM・GRUの区別**: 構造・ゲート数・記憶機構の違い
- **ゲート名の混同**: LSTMは入力・忘却・出力、GRUはリセット・更新
- **セルステート**: LSTMのみ、GRUにはない
- **Transformerとの違い**: LSTM/GRUは逐次処理、Transformerは並列処理（Self-Attention）

## 補足（LSTM・GRU全般）

### 実務的観点
- **前処理**: 系列長の正規化、パディング、数値の標準化が重要
- **双方向モデル**: Bidirectional LSTM/GRUで前後の文脈を学習
- **スタッキング**: 複数層を積み重ねて表現力向上
- **ドロップアウト**: 層間ドロップアウトで過学習防止
- **Attention機構**: LSTM/GRUとAttentionの組み合わせで長期依存を強化

### 関連トピック
- [RNN基礎](rnn.md) - 通常のRNNの構造と勾配消失問題
- [Transformer](transformer.md) - LSTMを置き換えたAttention機構
- [誤差逆伝播](backpropagation.md) - BPTT（Backpropagation Through Time）
- [自然言語処理](../07_ai_applications/natural_language_processing.md) - LSTM/GRUの応用例
